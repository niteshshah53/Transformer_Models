# Historical Document Segmentation - Advanced Deep Learning Models

This repository contains state-of-the-art deep learning models for historical document segmentation, featuring transformer-based architectures and hybrid CNN-transformer combinations optimized for manuscript analysis.

## ğŸ“ Directory Structure

```
Transformer_Models/
â”œâ”€â”€ models/                          # Model-specific implementations
â”‚   â”œâ”€â”€ sstrans/                     # Smart Swin Transformer
â”‚   â”‚   â”œâ”€â”€ train.py                 # SSTrans training script
â”‚   â”‚   â”œâ”€â”€ test.py                  # SSTrans testing script
â”‚   â”‚   â”œâ”€â”€ trainer.py               # SSTrans-specific trainer
â”‚   â”‚   â”œâ”€â”€ run.sh                   # SSTrans execution script
â”‚   â”‚   â”œâ”€â”€ Only_Smart.py            # Smart attention mechanism
â”‚   â”‚   â”œâ”€â”€ vision_transformer.py    # SSTrans model implementation
â”‚   â”‚   â”œâ”€â”€ modules.py               # SSTrans modules
â”‚   â”‚   â””â”€â”€ ...                      # Other SSTrans-specific files
â”‚   â”œâ”€â”€ swinunet/                    # Swin Transformer U-Net
â”‚   â”‚   â”œâ”€â”€ train.py                 # SwinUnet training script
â”‚   â”‚   â”œâ”€â”€ test.py                  # SwinUnet testing script
â”‚   â”‚   â”œâ”€â”€ trainer.py               # SwinUnet-specific trainer
â”‚   â”‚   â”œâ”€â”€ run.sh                   # SwinUnet execution script
â”‚   â”‚   â”œâ”€â”€ swin_transformer_unet_skip_expand_decoder_sys.py  # Main model
â”‚   â”‚   â”œâ”€â”€ vision_transformer.py    # Vision transformer components
â”‚   â”‚   â””â”€â”€ ...                      # Other SwinUnet-specific files
â”‚   â”œâ”€â”€ missformer/                  # MissFormer (Multi-scale Transformer)
â”‚   â”‚   â”œâ”€â”€ train.py                 # MissFormer training script
â”‚   â”‚   â”œâ”€â”€ test.py                  # MissFormer testing script
â”‚   â”‚   â”œâ”€â”€ trainer.py               # MissFormer-specific trainer
â”‚   â”‚   â”œâ”€â”€ run.sh                   # MissFormer execution script
â”‚   â”‚   â”œâ”€â”€ MISSFormer.py            # MissFormer model implementation
â”‚   â”‚   â”œâ”€â”€ segformer.py             # SegFormer backbone
â”‚   â”‚   â””â”€â”€ ...                      # Other MissFormer-specific files
â”‚   â”œâ”€â”€ hybrid/                      # Hybrid CNN-Transformer Models
â”‚   â”‚   â”œâ”€â”€ hybrid2/                 # Swin-EfficientNet Hybrid (Enhanced)
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py             # Main hybrid model
â”‚   â”‚   â”‚   â”œâ”€â”€ components.py        # Model components
â”‚   â”‚   â”‚   â””â”€â”€ Hybrid2_Architecture_Description.tex  # Architecture docs
â”‚   â”‚   â”œâ”€â”€ train.py                 # Unified training script
â”‚   â”‚   â”œâ”€â”€ test.py                  # Unified testing script
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Hybrid-specific trainer
â”‚   â”‚   â”œâ”€â”€ aggregate_results.py     # Results aggregation script
â”‚   â”‚   â”œâ”€â”€ run.sh                   # Hybrid execution script
â”‚   â”‚   â”œâ”€â”€ run1.sh, run2.sh, run3.sh  # Additional run scripts
â”‚   â”‚   â””â”€â”€ Result/                  # Training results
â”‚   â””â”€â”€ network/                     # CNN-Transformer Network Models
â”‚       â”œâ”€â”€ train.py                 # Network training script
â”‚       â”œâ”€â”€ test.py                  # Network testing script
â”‚       â”œâ”€â”€ trainer.py               # Network-specific trainer
â”‚       â”œâ”€â”€ cnn_transformer.py      # CNN-Transformer model
â”‚       â”œâ”€â”€ vision_transformer_cnn.py  # Vision Transformer CNN model
â”‚       â”œâ”€â”€ components.py            # Network components
â”‚       â”œâ”€â”€ visualize_gradcam.py     # GradCAM visualization
â”‚       â”œâ”€â”€ run.sh                   # Network execution script
â”‚       â”œâ”€â”€ run1.sh, run2.sh, run3.sh  # Additional run scripts
â”‚       â””â”€â”€ Result/                  # Training results
â”œâ”€â”€ common/                          # Shared components
â”‚   â”œâ”€â”€ datasets/                    # Dataset implementations
â”‚   â”‚   â”œâ”€â”€ dataset_udiadsbib.py     # U-DIADS-Bib dataset loader (legacy)
â”‚   â”‚   â”œâ”€â”€ dataset_udiadsbib_2.py   # U-DIADS-Bib dataset loader (enhanced with class-aware augmentation)
â”‚   â”‚   â”œâ”€â”€ dataset_divahisdb.py     # DivaHisDB dataset loader
â”‚   â”‚   â”œâ”€â”€ sstrans_transforms.py    # SSTrans-specific transforms
â”‚   â”‚   â””â”€â”€ README.md                # Dataset documentation
â”‚   â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”‚   â””â”€â”€ utils.py                 # Common utilities (losses, metrics)
â”‚   â””â”€â”€ configs/                     # Configuration files
â”‚       â”œâ”€â”€ config.py                # Configuration management
â”‚       â””â”€â”€ swin_tiny_patch4_window7_224_lite.yaml  # SSTrans config
â”œâ”€â”€ pretrained_ckpt/                 # Pre-trained checkpoints
â”œâ”€â”€ logs/                            # Training logs
â”œâ”€â”€ U-DIADS-Bib-MS/                  # U-DIADS-Bib-MS dataset (original)
â”œâ”€â”€ U-DIADS-Bib-MS_patched/          # U-DIADS-Bib-MS dataset (patched)
â”œâ”€â”€ U-DIADS-Bib-FS/                  # U-DIADS-Bib-FS dataset (original)
â”œâ”€â”€ U-DIADS-Bib-FS_patched/          # U-DIADS-Bib-FS dataset (patched)
â”œâ”€â”€ DivaHisDB/                       # DivaHisDB dataset (original)
â”œâ”€â”€ DivaHisDB_patched/               # DivaHisDB dataset (patched)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install dependencies
pip install -r requirements.txt

# Load Python environment (if using module system)
module load python/pytorch2.6py3.12
module load cuda/11.8
module load cudnn

# Activate conda environment
conda activate pytorch2.6-py3.12
```

### Run Individual Models

Each model has its own execution script:

```bash
# SSTrans (Smart Swin Transformer with attention mechanisms)
cd models/sstrans
./run.sh

# SwinUnet (Standard Swin Transformer U-Net)
cd models/swinunet
./run.sh

# MissFormer (Multi-scale Transformer with SegFormer backbone)
cd models/missformer
./run.sh

# Hybrid2 (SwinUnet encoder + Enhanced EfficientNet decoder)
cd models/hybrid
./run.sh

# Network (CNN-Transformer models)
cd models/network
./run.sh
```

### Custom Training

```bash
# Train Hybrid2 with custom parameters
cd models/hybrid
python3 train.py \
    --use_baseline \
    --decoder EfficientNet-B4 \
    --use_smart_skip \
    --dataset UDIADS_BIB \
    --udiadsbib_root "../../U-DIADS-Bib-MS_patched" \
    --manuscript Latin2 \
    --use_patched_data \
    --batch_size 16 \
    --max_epochs 300 \
    --base_lr 0.0001 \
    --patience 150 \
    --output_dir "./Result/a1/Latin2"
```

## ğŸ”§ Model Architectures & Configurations

### SSTrans (Smart Swin Transformer)
- **Architecture**: Enhanced Swin Transformer with smart attention mechanisms
- **Key Features**: 
  - Smart attention masks for improved focus
  - Heavy data augmentation pipeline
  - Advanced normalization strategies
- **Training**: Standardized with validation and early stopping
- **Loss Function**: Combined CE, Focal, and Dice losses
- **Optimizer**: AdamW with weight_decay=0.01
- **Validation**: Sliding window on full images
- **Config File**: Requires `--cfg` parameter pointing to config YAML

### SwinUnet (Swin Transformer U-Net)
- **Architecture**: Standard Swin Transformer with U-Net decoder
- **Key Features**:
  - Skip connections between encoder and decoder
  - Patch merging and expanding operations
  - Window-based self-attention
  - ImageNet-pretrained weights for better initialization
- **Training**: Advanced with class imbalance handling
  - **Class Weighting**: Effective Number of Samples (ENS) method with aggressive rare class boosting
  - **Balanced Sampling**: Optional oversampling of rare classes
  - **Class-Aware Augmentation**: Stronger augmentation for rare classes (Title, Paratext, Decoration, Chapter Headings)
  - **Loss Function**: Combined CE (15%), Focal (55%, Î³=5), and Dice (30%) losses
  - **Early Stopping**: Based on mean foreground Dice score (better reflects rare class performance)
  - **Learning Rate Warmup**: Gradual LR increase over first N epochs
  - **Scheduler**: CosineAnnealingWarmRestarts (T_0=50, T_mult=2) optimized for imbalanced data
- **Optimizer**: AdamW with weight_decay=0.01
- **Validation**: Sliding window on full images with per-class Dice tracking
- **Inference**: 
  - **Test-Time Augmentation (TTA)**: 8 augmentations (original, flips, rotations, combinations)
  - **Multi-Scale Testing**: 3 scales (0.75x, 1.0x, 1.25x) with probability averaging
  - **Batch Processing**: Efficient patch batching for faster inference
  - **Probability Accumulation**: Correct overlap handling by accumulating probabilities, not class indices

### MissFormer (Multi-scale Transformer)
- **Architecture**: SegFormer backbone with multi-scale feature fusion
- **Key Features**:
  - Efficient self-attention mechanisms
  - Multi-scale feature aggregation
  - Bridge layers for feature fusion
- **Training**: Advanced with class weights and sliding window validation
- **Loss Function**: Combined CE and Dice losses (with class weights)
- **Optimizer**: AdamW with weight_decay=1e-4
- **Validation**: Advanced sliding window with mask conversion

### Hybrid2 (Swin-EfficientNet Enhanced)
- **Architecture**: SwinUnet encoder + Enhanced EfficientNet decoder
- **Key Features**:
  - **Swin Transformer Encoder**: 4 stages (96â†’192â†’384â†’768 dim)
  - **Bottleneck**: 2 Swin Transformer blocks (768 dim, 24 heads)
  - **EfficientNet-B4 Decoder**: MBConv blocks with channel progression
  - **Smart Skip Connections**: Attention-based feature fusion
  - **Positional Embeddings**: 2D learnable positional embeddings
  - **Deep Supervision**: Optional multi-resolution auxiliary outputs
  - **CBAM Attention**: Optional channel and spatial attention
  - **Feature Refinement**: Gradual channel reduction with residual connections
- **Training**: Advanced with balanced sampling and class-aware augmentation
- **Loss Function**: Combined CE, Focal (Î³=2.0), and Dice losses
- **Optimizer**: AdamW with differential learning rates
- **Variants**: B0 (lightweight), B4 (balanced), B5 (heavy)
- **Decoder Options**: EfficientNet-B4, ResNet50, or simple CNN decoder

### Network (CNN-Transformer)
- **Architecture**: CNN-Transformer hybrid models
- **Key Features**:
  - Vision Transformer CNN integration
  - GradCAM visualization support
  - Flexible architecture combinations
- **Training**: Standardized training pipeline
- **Visualization**: Includes GradCAM for model interpretability

## ğŸ“Š Supported Datasets

### U-DIADS-Bib
- **Description**: Historical manuscript segmentation dataset
- **Variants**: 
  - **MS (Full Dataset)**: Full Datasets
  - **FS (Few-Shot)**: Few Images or Few-Shot Dataset
- **Classes**: 6 classes (5 for Syriaque341 manuscripts)
  - Background, Paratext, Decoration, Main Text, Title, Chapter Headings
- **Note**: Syriaque341 manuscripts don't have Chapter Headings (5 classes)
- **Format**: RGB color-coded masks
- **Usage**: `--dataset UDIADS_BIB --use_patched_data --udiadsbib_root "../../U-DIADS-Bib-MS_patched"`

### DIVAHISDB
- **Description**: Historical document analysis dataset
- **Classes**: 4 classes
  - Background, Comment, Decoration, Main Text
- **Format**: Bitmask-encoded masks
- **Usage**: `--dataset DIVAHISDB --use_patched_data`


## ğŸ”§ Key Benefits of Repository Structure

1. **Modularity**: Each model is self-contained with its own implementation
2. **Flexibility**: Easy to experiment with different architectures
3. **Maintainability**: Clear separation between model-specific and shared code
4. **Extensibility**: Simple to add new models or modify existing ones
5. **Reproducibility**: Consistent training and evaluation pipelines
6. **SLURM Integration**: All run scripts include SLURM job submission headers

## ğŸ“ Adding New Models

To add a new model:

1. Create a new folder in `models/`
2. Copy the structure from an existing model (recommend starting with `hybrid/`)
3. Implement your model architecture
4. Modify the training/testing scripts
5. Update the common trainer if needed
6. Create run scripts with SLURM headers if needed

## ğŸ› Troubleshooting

### Common Issues

#### Import Errors
- Ensure you're running scripts from the correct directory
- Check that the common directory is in the Python path
- Verify Python environment is properly loaded (`module load python/pytorch2.6py3.12`)
- Add user site-packages to PYTHONPATH: `export PYTHONPATH="${HOME}/.local/lib/python3.12/site-packages:${PYTHONPATH}"`

#### Model-Specific Issues
- **SSTrans**: Requires config file (`--cfg` parameter pointing to YAML config)
- **Hybrid Models**: Check `--decoder` parameter (EfficientNet-B4, ResNet50, or baseline)
- **MissFormer**: Verify SegFormer dependencies
- Check individual `trainer.py` files for model-specific logic

#### Training Issues
- **CUDA Memory**: Reduce batch size if encountering OOM errors
  - Set `export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` for memory optimization
- **Data Loading**: Ensure dataset paths are correct and accessible
- **Checkpoints**: Verify checkpoint paths and model compatibility
- **SLURM Jobs**: Check job output files in Result directories for detailed error messages

#### Performance Issues
- **Slow Training**: Consider reducing image size or using mixed precision (`--amp_opt_level O1`)
- **Poor Convergence**: Adjust learning rate or try different optimizers
- **Overfitting**: Increase regularization or use more data augmentation
- **Class Imbalance**: Use `--use_balanced_sampler` and `--use_class_aware_aug` flags

## ğŸ“ˆ Model Performance Comparison

| Model | Architecture | Parameters | Memory | Speed | Best For |
|-------|-------------|-----------|--------|-------|----------|
| SSTrans | Smart Swin Transformer | ~28M | Moderate | Fast | Attention-focused tasks |
| SwinUnet | Standard Swin U-Net | ~27M | Moderate | Fast | General segmentation |
| MissFormer | Multi-scale Transformer | ~30M | High | Moderate | Multi-scale features |
| Hybrid2 | Swin-EfficientNet | ~45M | Moderate | Moderate | Enhanced feature extraction |
| Network | CNN-Transformer | Variable | Variable | Variable | Flexible architectures |

## ğŸ”„ Recent Updates & Improvements

### Hybrid2 Enhancements (Latest)
- **Smart Skip Connections**: Attention-based feature fusion instead of simple concatenation
- **Deep Supervision**: Optional multi-resolution auxiliary outputs for better training
- **Positional Embeddings**: 2D learnable positional embeddings for better spatial understanding
- **Balanced Sampling**: Oversampling rare classes to handle class imbalance
- **Class-Aware Augmentation**: Stronger augmentation for rare classes
- **Class-Balanced Loss**: CB Loss with beta=0.9999 for extreme imbalance handling
- **Differential Learning Rates**: Different LR for encoder, bottleneck, and decoder
- **Multiple Decoder Options**: EfficientNet-B4, ResNet50, or simple CNN decoder

### Training Standardization
- **All Models**: AdamW optimizer with various schedulers (CosineAnnealingWarmRestarts, ReduceLROnPlateau)
- **Early Stopping**: Configurable patience across all models
- **Validation**: Proper validation during training with sliding window for transformer models
- **Logging**: Improved TensorBoard logging and progress tracking
- **Checkpointing**: Automatic best model saving and cleanup
- **Results Aggregation**: Scripts to aggregate results across multiple manuscripts

### Key Technical Improvements
1. **Attention Mechanisms**: CBAM and smart attention for better feature focus
2. **Skip Connections**: Intelligent fusion instead of simple concatenation
3. **Residual Learning**: Better gradient flow and training stability
4. **Multi-scale Processing**: Enhanced feature extraction at different scales
5. **Advanced Augmentation**: MixUp, CutMix, and sophisticated transforms
6. **Memory Optimization**: CUDA memory fragmentation reduction for large models
7. **Mixed Precision**: AMP support for faster training

### Future Enhancements
- **Model Ensembling**: Combining multiple models for improved performance
- **Efficient Variants**: Lightweight versions for deployment
- **Cross-dataset Training**: Multi-dataset learning capabilities
- **Advanced Visualization**: More interpretability tools

## ğŸ“‹ SLURM Job Submission

All run scripts include SLURM headers for cluster execution. Key parameters:
- **Job Name**: Model-specific job names
- **GPU**: Typically 1 GPU (rtx3080 or generic)
- **Time Limit**: 22-24 hours
- **CPUs**: 8 CPUs per task
- **Output**: Results saved in `Result/` directories

Example SLURM submission:
```bash
sbatch models/hybrid/run.sh
```

## ğŸ“š Additional Resources

- Dataset documentation: `common/datasets/README.md`
- Hybrid2 architecture: `models/hybrid/hybrid2/Hybrid2_Architecture_Description.tex`
- Results aggregation: Use `aggregate_results.py` in hybrid models

This repository provides a comprehensive framework for historical document segmentation with state-of-the-art models and best practices.

