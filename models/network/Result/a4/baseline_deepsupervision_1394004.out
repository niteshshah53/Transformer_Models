### Starting TaskPrologue of job 1394004 on tg066 at Tue Nov 18 04:27:54 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Tue Nov 18 16:27:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:AF:00.0 Off |                  N/A |
| 27%   26C    P8              3W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple (baseline skip connections)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)
  ‚úì Loss: CB Loss (Class-Balanced, beta=0.9999) + Focal (Œ≥=2.0) + Dice
  ‚úì Deep Supervision: ENABLED (MSAGHNet-style multi-resolution)
    - Simple OutConv heads (single Conv2d)
    - Outputs at native resolutions (H/16, H/8, H/4)
    - Multi-resolution loss computation
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Smart Skip Connections: disabled
  ‚úó Fourier Feature Fusion: disabled

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a4/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a4/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,890,802
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a4/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a4/Latin2/best_model_latest.pth
   File exists: True

üìÇ Found checkpoint: ./Result/a4/Latin2/best_model_latest.pth
   Attempting to resume training...
   ‚úì Loaded model state
   ‚úì Loaded optimizer state
   ‚úì Loaded scheduler state
   ‚úì Loaded scaler state (AMP)
   ‚úì Successfully loaded checkpoint from epoch 268
   ‚úì Best validation loss: 0.4607
   ‚úì Resuming from epoch 269

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 269
================================================================================


EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0911
  ‚Ä¢ Validation Loss: 0.4200
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4200
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0544
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4248, best: 0.4200)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0773
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4231, best: 0.4200)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0821
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4218, best: 0.4200)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0770
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4197
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0515
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4199, best: 0.4197)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0505
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4228, best: 0.4197)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0381
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4243, best: 0.4197)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0522
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4213, best: 0.4197)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0745
  ‚Ä¢ Validation Loss: 0.4187
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4187
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0768
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1414
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4185, best: 0.4177)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1007
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4177)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0742
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4163
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1008
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4170, best: 0.4163)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0250
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4171, best: 0.4163)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0788
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4184, best: 0.4163)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1280
  ‚Ä¢ Validation Loss: 0.4188
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4188, best: 0.4163)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0810
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4161
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1159
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4176, best: 0.4161)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0907
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4212, best: 0.4161)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1148
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4189, best: 0.4161)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0543
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4161)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0505
  ‚Ä¢ Validation Loss: 0.4175
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4175, best: 0.4161)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1504
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4211, best: 0.4161)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0641
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4213, best: 0.4161)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0519
  ‚Ä¢ Validation Loss: 0.4210
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4210, best: 0.4161)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0522
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4193, best: 0.4161)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1307
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4161
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0501
  ‚Ä¢ Validation Loss: 0.4174
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4174, best: 0.4161)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0939
  ‚Ä¢ Validation Loss: 0.4165
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4165, best: 0.4161)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4161
Total Epochs:   300
Models Saved:   ./Result/a4/Latin2
TensorBoard:    ./Result/a4/Latin2/tensorboard_logs
================================================================================

[16:34:40] Training completed. Best val loss: 0.4161

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9892, Recall=0.9917, F1=0.9904, IoU=0.9811
Paratext            : Precision=0.7544, Recall=0.6959, F1=0.7240, IoU=0.5673
Decoration          : Precision=0.8658, Recall=0.9027, F1=0.8839, IoU=0.7919
Main Text           : Precision=0.8388, Recall=0.8432, F1=0.8410, IoU=0.7256
Title               : Precision=0.8560, Recall=0.7805, F1=0.8165, IoU=0.6899
Chapter Headings    : Precision=0.7197, Recall=0.3769, F1=0.4948, IoU=0.3287

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8373
Mean Recall:    0.7651
Mean F1-Score:  0.7917
Mean IoU:       0.6807
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a4/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a4/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,890,802
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a4/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a4/Latin14396/best_model_latest.pth
   File exists: True

üìÇ Found checkpoint: ./Result/a4/Latin14396/best_model_latest.pth
   Attempting to resume training...
   ‚úì Loaded model state
   ‚úì Loaded optimizer state
   ‚úì Loaded scheduler state
   ‚úì Loaded scaler state (AMP)
   ‚úì Successfully loaded checkpoint from epoch 213
   ‚úì Best validation loss: 0.4684
   ‚úì Resuming from epoch 214

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 214
================================================================================


EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0675
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4422
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1059
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4472, best: 0.4422)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0287
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4467, best: 0.4422)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0558
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4414
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0536
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4397
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0271
  ‚Ä¢ Validation Loss: 0.4400
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4400, best: 0.4397)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0521
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4386
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0857
  ‚Ä¢ Validation Loss: 0.4435
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4435, best: 0.4386)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0548
  ‚Ä¢ Validation Loss: 0.4438
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4438, best: 0.4386)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0712
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4422, best: 0.4386)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0680
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4397, best: 0.4386)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1420
  ‚Ä¢ Validation Loss: 0.4398
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4398, best: 0.4386)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0894
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4354
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0928
  ‚Ä¢ Validation Loss: 0.4366
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4366, best: 0.4354)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1310
  ‚Ä¢ Validation Loss: 0.4381
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4381, best: 0.4354)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0417
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4365, best: 0.4354)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0788
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4374, best: 0.4354)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0518
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4376, best: 0.4354)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0656
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4374, best: 0.4354)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0969
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4390, best: 0.4354)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1039
  ‚Ä¢ Validation Loss: 0.4400
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4400, best: 0.4354)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0527
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4427, best: 0.4354)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0864
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4359, best: 0.4354)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0897
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4370, best: 0.4354)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0793
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4355, best: 0.4354)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0902
  ‚Ä¢ Validation Loss: 0.4377
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4377, best: 0.4354)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1071
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4399, best: 0.4354)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1075
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4368, best: 0.4354)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1034
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4357, best: 0.4354)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1035
  ‚Ä¢ Validation Loss: 0.4395
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4395, best: 0.4354)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1132
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4347
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0908
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4348, best: 0.4347)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4411
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4411, best: 0.4347)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0394
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4403, best: 0.4347)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1153
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4346
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1417
  ‚Ä¢ Validation Loss: 0.4378
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4378, best: 0.4346)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1229
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4373, best: 0.4346)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4365, best: 0.4346)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1552
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4358, best: 0.4346)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0405
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4359, best: 0.4346)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0663
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4410, best: 0.4346)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4393
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4393, best: 0.4346)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1017
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0892
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4339
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0938
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4384, best: 0.4339)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1032
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4343, best: 0.4339)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0615
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4330
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1324
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4386, best: 0.4330)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1124
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4353, best: 0.4330)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1058
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4343, best: 0.4330)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0887
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4343, best: 0.4330)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1212
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4410, best: 0.4330)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0670
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4403, best: 0.4330)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0814
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4340, best: 0.4330)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0524
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4348, best: 0.4330)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1264
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4352, best: 0.4330)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0595
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4376, best: 0.4330)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1129
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4369, best: 0.4330)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1161
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4351, best: 0.4330)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4339, best: 0.4330)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0678
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4322
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1470
  ‚Ä¢ Validation Loss: 0.4336
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4336, best: 0.4322)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4368, best: 0.4322)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1050
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4328, best: 0.4322)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1304
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4337, best: 0.4322)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0659
  ‚Ä¢ Validation Loss: 0.4324
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4324, best: 0.4322)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1411
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4330, best: 0.4322)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1451
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4328, best: 0.4322)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1288
  ‚Ä¢ Validation Loss: 0.4334
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4334, best: 0.4322)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1053
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4341, best: 0.4322)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1029
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4340, best: 0.4322)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0628
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4326, best: 0.4322)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0926
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4313
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4318, best: 0.4313)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1567
  ‚Ä¢ Validation Loss: 0.4316
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4316, best: 0.4313)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0901
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4308
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0938
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4319, best: 0.4308)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4328, best: 0.4308)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0667
  ‚Ä¢ Validation Loss: 0.4329
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4329, best: 0.4308)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4356, best: 0.4308)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1266
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4330, best: 0.4308)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1383
  ‚Ä¢ Validation Loss: 0.4336
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4336, best: 0.4308)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1270
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4338, best: 0.4308)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1682
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4341, best: 0.4308)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0875
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4339, best: 0.4308)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4332, best: 0.4308)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4308
Total Epochs:   300
Models Saved:   ./Result/a4/Latin14396
TensorBoard:    ./Result/a4/Latin14396/tensorboard_logs
================================================================================

[16:54:24] Training completed. Best val loss: 0.4308

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9877, Recall=0.9915, F1=0.9896, IoU=0.9794
Paratext            : Precision=0.6533, Recall=0.5436, F1=0.5934, IoU=0.4219
Decoration          : Precision=0.9474, Recall=0.9452, F1=0.9463, IoU=0.8980
Main Text           : Precision=0.8988, Recall=0.8784, F1=0.8885, IoU=0.7994
Title               : Precision=0.8781, Recall=0.8044, F1=0.8396, IoU=0.7236
Chapter Headings    : Precision=0.8709, Recall=0.7041, F1=0.7787, IoU=0.6376

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8727
Mean Recall:    0.8112
Mean F1-Score:  0.8393
Mean IoU:       0.7433
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a4/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a4/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,890,802
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a4/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a4/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 2.5635
  ‚Ä¢ Validation Loss: 0.8983
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8983
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.9215
  ‚Ä¢ Validation Loss: 0.7555
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7555
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8255
  ‚Ä¢ Validation Loss: 0.6835
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6835
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7754
  ‚Ä¢ Validation Loss: 0.6614
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6614
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7556
  ‚Ä¢ Validation Loss: 0.6427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7197
  ‚Ä¢ Validation Loss: 0.6338
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7001
  ‚Ä¢ Validation Loss: 0.6190
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6190
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6697
  ‚Ä¢ Validation Loss: 0.6039
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6039
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6495
  ‚Ä¢ Validation Loss: 0.6034
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6034
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6263
  ‚Ä¢ Validation Loss: 0.5744
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5744
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6092
  ‚Ä¢ Validation Loss: 0.5706
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5706
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6059
  ‚Ä¢ Validation Loss: 0.5633
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5633
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5941
  ‚Ä¢ Validation Loss: 0.5561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5561
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5949
  ‚Ä¢ Validation Loss: 0.5613
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5613, best: 0.5561)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5774
  ‚Ä¢ Validation Loss: 0.5501
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5501
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5678
  ‚Ä¢ Validation Loss: 0.5467
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5467
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5599
  ‚Ä¢ Validation Loss: 0.5387
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5387
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5596
  ‚Ä¢ Validation Loss: 0.5363
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5363
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5414
  ‚Ä¢ Validation Loss: 0.5336
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5336
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5361
  ‚Ä¢ Validation Loss: 0.5294
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5294
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5251
  ‚Ä¢ Validation Loss: 0.5259
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5259
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5138
  ‚Ä¢ Validation Loss: 0.5284
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5284, best: 0.5259)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5122
  ‚Ä¢ Validation Loss: 0.5163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5163
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5166
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5142
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5141
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5076
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5047
  ‚Ä¢ Validation Loss: 0.5044
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5044
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5013
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5078, best: 0.5044)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4927
  ‚Ä¢ Validation Loss: 0.5045
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5045, best: 0.5044)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4940
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5010
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4890
  ‚Ä¢ Validation Loss: 0.5018
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5018, best: 0.5010)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4849
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5032, best: 0.5010)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4849
  ‚Ä¢ Validation Loss: 0.5021
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5021, best: 0.5010)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4784
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4946
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4780
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4945
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4728
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4935
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4654
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4940, best: 0.4935)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4700
  ‚Ä¢ Validation Loss: 0.4959
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4959, best: 0.4935)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4711
  ‚Ä¢ Validation Loss: 0.4920
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4920
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4634
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4906
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4679
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4918, best: 0.4906)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4694
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4889
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4653
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4909, best: 0.4889)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4669
  ‚Ä¢ Validation Loss: 0.4893
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4893, best: 0.4889)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4637
  ‚Ä¢ Validation Loss: 0.4885
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4885
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4670
  ‚Ä¢ Validation Loss: 0.4896
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4896, best: 0.4885)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4583
  ‚Ä¢ Validation Loss: 0.4898
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4898, best: 0.4885)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4639
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4886, best: 0.4885)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4677
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4890, best: 0.4885)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4673
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4894, best: 0.4885)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4640
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4901, best: 0.4885)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4760
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4964, best: 0.4885)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4679
  ‚Ä¢ Validation Loss: 0.5065
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5065, best: 0.4885)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4780
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4990, best: 0.4885)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4469
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4907, best: 0.4885)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4817
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4906, best: 0.4885)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4649
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4835
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4576
  ‚Ä¢ Validation Loss: 0.4897
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4897, best: 0.4835)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4393
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4836, best: 0.4835)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4493
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4772
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4383
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4430
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4833, best: 0.4741)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4426
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4706
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4358
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4763, best: 0.4706)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4309
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4722, best: 0.4706)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4275
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4682
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4280
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4318
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4680)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4196
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4131
  ‚Ä¢ Validation Loss: 0.4619
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4619, best: 0.4605)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4160
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4605)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4051
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4592
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4059
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4575
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4035
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4748, best: 0.4575)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3878
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4621, best: 0.4575)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3912
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4602, best: 0.4575)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4087
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4574
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4143
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4581, best: 0.4574)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3880
  ‚Ä¢ Validation Loss: 0.4533
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4533
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3833
  ‚Ä¢ Validation Loss: 0.4501
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4501
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3879
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4504, best: 0.4501)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3814
  ‚Ä¢ Validation Loss: 0.4556
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4556, best: 0.4501)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3904
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4587, best: 0.4501)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3795
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4491
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3766
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4523, best: 0.4491)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4472
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3700
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4441
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3734
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4454, best: 0.4441)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3781
  ‚Ä¢ Validation Loss: 0.4435
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4435
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3489
  ‚Ä¢ Validation Loss: 0.4446
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4446, best: 0.4435)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3625
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4414
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3443
  ‚Ä¢ Validation Loss: 0.4432
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4432, best: 0.4414)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4430, best: 0.4414)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3571
  ‚Ä¢ Validation Loss: 0.4415
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4415, best: 0.4414)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3380
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4396
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3525
  ‚Ä¢ Validation Loss: 0.4415
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4415, best: 0.4396)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3621
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4442, best: 0.4396)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3552
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4389
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3637
  ‚Ä¢ Validation Loss: 0.4398
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4398, best: 0.4389)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3405
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4356
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3577
  ‚Ä¢ Validation Loss: 0.4406
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4406, best: 0.4356)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3595
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4371, best: 0.4356)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3554
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4389, best: 0.4356)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3340
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4441, best: 0.4356)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 104/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3666
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4408, best: 0.4356)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3543
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4397, best: 0.4356)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3436
  ‚Ä¢ Validation Loss: 0.4364
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4364, best: 0.4356)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 107/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3521
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4328
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3619
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4339, best: 0.4328)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3502
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4343, best: 0.4328)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3568
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4385, best: 0.4328)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3175
  ‚Ä¢ Validation Loss: 0.4366
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4366, best: 0.4328)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3530
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4332, best: 0.4328)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3344
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4319
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3357
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4338, best: 0.4319)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3573
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4379, best: 0.4319)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3352
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4379, best: 0.4319)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3271
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4343, best: 0.4319)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 118/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3485
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4323, best: 0.4319)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 119/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3462
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4309
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3408
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4325, best: 0.4309)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3296
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4308
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3314
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4321, best: 0.4308)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3294
  ‚Ä¢ Validation Loss: 0.4305
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4305
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3236
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4322, best: 0.4305)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3318
  ‚Ä¢ Validation Loss: 0.4324
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4324, best: 0.4305)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3125
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4302
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 127/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3419
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4302
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3225
  ‚Ä¢ Validation Loss: 0.4303
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4303, best: 0.4302)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3332
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4289
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3449
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4294, best: 0.4289)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 131/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3451
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4280
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 132/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3419
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4276
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3340
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4291, best: 0.4276)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3474
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4284, best: 0.4276)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 135/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3387
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4286, best: 0.4276)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3073
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4283, best: 0.4276)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3262
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4270
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3226
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4291, best: 0.4270)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3306
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4283, best: 0.4270)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 140/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3378
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4271, best: 0.4270)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 141/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3412
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4289, best: 0.4270)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3193
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4284, best: 0.4270)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 143/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3477
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4276, best: 0.4270)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 144/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3471
  ‚Ä¢ Validation Loss: 0.4281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4281, best: 0.4270)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3206
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4267
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 146/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3381
  ‚Ä¢ Validation Loss: 0.4275
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4275, best: 0.4267)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 147/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3474
  ‚Ä¢ Validation Loss: 0.4278
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4278, best: 0.4267)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3110
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4267
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 149/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3404
  ‚Ä¢ Validation Loss: 0.4273
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4273, best: 0.4267)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3223
  ‚Ä¢ Validation Loss: 0.4287
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4287, best: 0.4267)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3230
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4291, best: 0.4267)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3377
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4385, best: 0.4267)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 153/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3561
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4403, best: 0.4267)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3392
  ‚Ä¢ Validation Loss: 0.4349
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4349, best: 0.4267)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 155/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3522
  ‚Ä¢ Validation Loss: 0.4297
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4297, best: 0.4267)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3327
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4269, best: 0.4267)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3036
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4369, best: 0.4267)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 158/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3664
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4358, best: 0.4267)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3396
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4325, best: 0.4267)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 160/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3491
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4320, best: 0.4267)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3075
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4327, best: 0.4267)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 162/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3482
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4252
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3293
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4345, best: 0.4252)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 164/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3511
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4263, best: 0.4252)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 165/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3432
  ‚Ä¢ Validation Loss: 0.4278
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4278, best: 0.4252)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 166/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3381
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4249
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2993
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4295, best: 0.4249)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3160
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4274, best: 0.4249)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 169/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3385
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4234
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 170/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3251
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4291, best: 0.4234)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 171/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3415
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4217
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 172/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3367
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4222, best: 0.4217)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3123
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4234, best: 0.4217)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3097
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4310, best: 0.4217)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3013
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4293, best: 0.4217)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3241
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4226, best: 0.4217)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2916
  ‚Ä¢ Validation Loss: 0.4297
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4297, best: 0.4217)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2405
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4296, best: 0.4217)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1899
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4233, best: 0.4217)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1905
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4229, best: 0.4217)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2421
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4252, best: 0.4217)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1957
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4219, best: 0.4217)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2530
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4197
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2500
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4215, best: 0.4197)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2454
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4214, best: 0.4197)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2451
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4233, best: 0.4197)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4158
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2631
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4218, best: 0.4158)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2505
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4245, best: 0.4158)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2234
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4250, best: 0.4158)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1964
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4191, best: 0.4158)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.4200
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4200, best: 0.4158)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1925
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4239, best: 0.4158)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2585
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4202, best: 0.4158)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2597
  ‚Ä¢ Validation Loss: 0.4203
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4203, best: 0.4158)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2464
  ‚Ä¢ Validation Loss: 0.4157
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4157
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2149
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4156
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2425
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4170, best: 0.4156)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2590
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4160, best: 0.4156)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2172
  ‚Ä¢ Validation Loss: 0.4157
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4157, best: 0.4156)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1902
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4148
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2682
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4225, best: 0.4148)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1854
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4177, best: 0.4148)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2398
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4184, best: 0.4148)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2795
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2280
  ‚Ä¢ Validation Loss: 0.4137
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4137
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1700
  ‚Ä¢ Validation Loss: 0.4150
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4150, best: 0.4137)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2162
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4153, best: 0.4137)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2593
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4170, best: 0.4137)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2106
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4168, best: 0.4137)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2281
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4141, best: 0.4137)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2300
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4147, best: 0.4137)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2411
  ‚Ä¢ Validation Loss: 0.4179
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4179, best: 0.4137)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1324
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4112
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2130
  ‚Ä¢ Validation Loss: 0.4124
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4124, best: 0.4112)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2603
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4185, best: 0.4112)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1636
  ‚Ä¢ Validation Loss: 0.4128
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4128, best: 0.4112)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2531
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4133, best: 0.4112)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2541
  ‚Ä¢ Validation Loss: 0.4145
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4145, best: 0.4112)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2731
  ‚Ä¢ Validation Loss: 0.4129
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4129, best: 0.4112)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2414
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4122, best: 0.4112)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2723
  ‚Ä¢ Validation Loss: 0.4131
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4131, best: 0.4112)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2799
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4169, best: 0.4112)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2091
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4117, best: 0.4112)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2206
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4094
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.4144
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4144, best: 0.4094)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2307
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4095, best: 0.4094)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2056
  ‚Ä¢ Validation Loss: 0.4093
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4093
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4104
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4104, best: 0.4093)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2353
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4109, best: 0.4093)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2172
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4071
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2399
  ‚Ä¢ Validation Loss: 0.4106
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4106, best: 0.4071)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2503
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4116, best: 0.4071)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2360
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4132, best: 0.4071)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2608
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4109, best: 0.4071)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4109, best: 0.4071)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2221
  ‚Ä¢ Validation Loss: 0.4120
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4120, best: 0.4071)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2829
  ‚Ä¢ Validation Loss: 0.4083
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4083, best: 0.4071)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2305
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4084, best: 0.4071)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2425
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4105, best: 0.4071)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2664
  ‚Ä¢ Validation Loss: 0.4103
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4103, best: 0.4071)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2842
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4072, best: 0.4071)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1907
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4079, best: 0.4071)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2604
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4072, best: 0.4071)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2611
  ‚Ä¢ Validation Loss: 0.4136
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4136, best: 0.4071)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2185
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4113, best: 0.4071)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2701
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4109, best: 0.4071)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2205
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4084, best: 0.4071)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2498
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4094, best: 0.4071)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2404
  ‚Ä¢ Validation Loss: 0.4096
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4096, best: 0.4071)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2558
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4091, best: 0.4071)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2264
  ‚Ä¢ Validation Loss: 0.4089
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4089, best: 0.4071)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2245
  ‚Ä¢ Validation Loss: 0.4087
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4087, best: 0.4071)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2322
  ‚Ä¢ Validation Loss: 0.4104
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4104, best: 0.4071)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2342
  ‚Ä¢ Validation Loss: 0.4118
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4118, best: 0.4071)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2294
  ‚Ä¢ Validation Loss: 0.4124
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4124, best: 0.4071)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2397
  ‚Ä¢ Validation Loss: 0.4110
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4110, best: 0.4071)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2247
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4113, best: 0.4071)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2443
  ‚Ä¢ Validation Loss: 0.4085
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4085, best: 0.4071)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2608
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4065
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2223
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4055
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1996
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4050
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2640
  ‚Ä¢ Validation Loss: 0.4062
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4062, best: 0.4050)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2660
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4067, best: 0.4050)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2345
  ‚Ä¢ Validation Loss: 0.4054
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4054, best: 0.4050)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2164
  ‚Ä¢ Validation Loss: 0.4053
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4053, best: 0.4050)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2141
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4045
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2671
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4059, best: 0.4045)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2198
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4067, best: 0.4045)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2264
  ‚Ä¢ Validation Loss: 0.4074
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4074, best: 0.4045)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2508
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4050, best: 0.4045)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2510
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4047, best: 0.4045)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2333
  ‚Ä¢ Validation Loss: 0.4037
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4037
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2048
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4061, best: 0.4037)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2287
  ‚Ä¢ Validation Loss: 0.4060
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4060, best: 0.4037)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4048, best: 0.4037)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2261
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4058, best: 0.4037)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2544
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4041, best: 0.4037)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2522
  ‚Ä¢ Validation Loss: 0.4031
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4031
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2210
  ‚Ä¢ Validation Loss: 0.4039
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4039, best: 0.4031)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2391
  ‚Ä¢ Validation Loss: 0.4070
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4070, best: 0.4031)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4050, best: 0.4031)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2489
  ‚Ä¢ Validation Loss: 0.4060
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4060, best: 0.4031)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2712
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4066, best: 0.4031)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2395
  ‚Ä¢ Validation Loss: 0.4031
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4031, best: 0.4031)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2726
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4036, best: 0.4031)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4063, best: 0.4031)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2097
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4052, best: 0.4031)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1851
  ‚Ä¢ Validation Loss: 0.4062
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4062, best: 0.4031)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2171
  ‚Ä¢ Validation Loss: 0.4043
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4043, best: 0.4031)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1511
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4063, best: 0.4031)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1986
  ‚Ä¢ Validation Loss: 0.4042
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4042, best: 0.4031)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1396
  ‚Ä¢ Validation Loss: 0.4064
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4064, best: 0.4031)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1229
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4061, best: 0.4031)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1590
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4041, best: 0.4031)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1719
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4055, best: 0.4031)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0991
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4050, best: 0.4031)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1746
  ‚Ä¢ Validation Loss: 0.4039
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4039, best: 0.4031)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1556
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4038, best: 0.4031)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1214
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4036, best: 0.4031)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4031
Total Epochs:   300
Models Saved:   ./Result/a4/Latin16746
TensorBoard:    ./Result/a4/Latin16746/tensorboard_logs
================================================================================

[18:00:37] Training completed. Best val loss: 0.4031

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9864, Recall=0.9919, F1=0.9891, IoU=0.9784
Paratext            : Precision=0.7617, Recall=0.8184, F1=0.7890, IoU=0.6515
Decoration          : Precision=0.9696, Recall=0.9318, F1=0.9503, IoU=0.9053
Main Text           : Precision=0.9235, Recall=0.8911, F1=0.9070, IoU=0.8298
Title               : Precision=0.8003, Recall=0.7601, F1=0.7797, IoU=0.6389
Chapter Headings    : Precision=0.9051, Recall=0.7899, F1=0.8436, IoU=0.7295

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8911
Mean Recall:    0.8639
Mean F1-Score:  0.8764
Mean IoU:       0.7889
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a4/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a4/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,884,686
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a4/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a4/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.4358
  ‚Ä¢ Validation Loss: 0.8469
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8469
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8722
  ‚Ä¢ Validation Loss: 0.7820
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7714
  ‚Ä¢ Validation Loss: 0.6510
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6510
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7130
  ‚Ä¢ Validation Loss: 0.6359
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6359
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6799
  ‚Ä¢ Validation Loss: 0.6052
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6052
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6156
  ‚Ä¢ Validation Loss: 0.5985
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5985
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6355
  ‚Ä¢ Validation Loss: 0.5850
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5850
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6147
  ‚Ä¢ Validation Loss: 0.5910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5910, best: 0.5850)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6018
  ‚Ä¢ Validation Loss: 0.5830
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5830
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5780
  ‚Ä¢ Validation Loss: 0.5849
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5849, best: 0.5830)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4545
  ‚Ä¢ Validation Loss: 0.5809
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5809
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5416
  ‚Ä¢ Validation Loss: 0.5674
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5674
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5571
  ‚Ä¢ Validation Loss: 0.5747
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5747, best: 0.5674)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5593
  ‚Ä¢ Validation Loss: 0.5786
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5786, best: 0.5674)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4863
  ‚Ä¢ Validation Loss: 0.5634
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5634
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4998
  ‚Ä¢ Validation Loss: 0.5627
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5627
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4421
  ‚Ä¢ Validation Loss: 0.5636
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5636, best: 0.5627)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5214
  ‚Ä¢ Validation Loss: 0.5587
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5587
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4523
  ‚Ä¢ Validation Loss: 0.5590
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5590, best: 0.5587)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4937
  ‚Ä¢ Validation Loss: 0.5559
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5559
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4546
  ‚Ä¢ Validation Loss: 0.5478
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5478
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4820
  ‚Ä¢ Validation Loss: 0.5477
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5477
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3923
  ‚Ä¢ Validation Loss: 0.5524
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5524, best: 0.5477)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4963
  ‚Ä¢ Validation Loss: 0.5505
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5505, best: 0.5477)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4573
  ‚Ä¢ Validation Loss: 0.5662
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5662, best: 0.5477)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4769
  ‚Ä¢ Validation Loss: 0.5517
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5517, best: 0.5477)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4471
  ‚Ä¢ Validation Loss: 0.5509
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5509, best: 0.5477)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4190
  ‚Ä¢ Validation Loss: 0.5506
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5506, best: 0.5477)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4328
  ‚Ä¢ Validation Loss: 0.5476
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5476
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4724
  ‚Ä¢ Validation Loss: 0.5438
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5438
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4176
  ‚Ä¢ Validation Loss: 0.5427
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4764
  ‚Ä¢ Validation Loss: 0.5435
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5435, best: 0.5427)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4895
  ‚Ä¢ Validation Loss: 0.5404
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5404
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4126
  ‚Ä¢ Validation Loss: 0.5407
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5407, best: 0.5404)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4600
  ‚Ä¢ Validation Loss: 0.5387
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5387
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4890
  ‚Ä¢ Validation Loss: 0.5421
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5421, best: 0.5387)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4371
  ‚Ä¢ Validation Loss: 0.5419
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5419, best: 0.5387)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4558
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4822
  ‚Ä¢ Validation Loss: 0.5411
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5411, best: 0.5373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4583
  ‚Ä¢ Validation Loss: 0.5390
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5390, best: 0.5373)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4569
  ‚Ä¢ Validation Loss: 0.5381
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5381, best: 0.5373)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4034
  ‚Ä¢ Validation Loss: 0.5395
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5395, best: 0.5373)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4147
  ‚Ä¢ Validation Loss: 0.5358
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5358
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4786
  ‚Ä¢ Validation Loss: 0.5378
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5378, best: 0.5358)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4349
  ‚Ä¢ Validation Loss: 0.5365
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5365, best: 0.5358)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3715
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5364, best: 0.5358)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4530
  ‚Ä¢ Validation Loss: 0.5394
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5394, best: 0.5358)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4178
  ‚Ä¢ Validation Loss: 0.5371
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5371, best: 0.5358)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3780
  ‚Ä¢ Validation Loss: 0.5380
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5380, best: 0.5358)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4146
  ‚Ä¢ Validation Loss: 0.5383
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5383, best: 0.5358)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4496
  ‚Ä¢ Validation Loss: 0.5366
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5366, best: 0.5358)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4787
  ‚Ä¢ Validation Loss: 0.5467
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5467, best: 0.5358)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4649
  ‚Ä¢ Validation Loss: 0.5421
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5421, best: 0.5358)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4468
  ‚Ä¢ Validation Loss: 0.5367
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5367, best: 0.5358)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4351
  ‚Ä¢ Validation Loss: 0.5347
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5347
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4013
  ‚Ä¢ Validation Loss: 0.5313
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5313
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3901
  ‚Ä¢ Validation Loss: 0.5521
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5521, best: 0.5313)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4346
  ‚Ä¢ Validation Loss: 0.5335
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5335, best: 0.5313)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4098
  ‚Ä¢ Validation Loss: 0.5404
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5404, best: 0.5313)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3593
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5283
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4460
  ‚Ä¢ Validation Loss: 0.5301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5301, best: 0.5283)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4378
  ‚Ä¢ Validation Loss: 0.5274
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5274
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4611
  ‚Ä¢ Validation Loss: 0.5353
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5353, best: 0.5274)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4268
  ‚Ä¢ Validation Loss: 0.5292
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5292, best: 0.5274)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.5267
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5267
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4193
  ‚Ä¢ Validation Loss: 0.5323
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5323, best: 0.5267)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4220
  ‚Ä¢ Validation Loss: 0.5287
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5287, best: 0.5267)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4440
  ‚Ä¢ Validation Loss: 0.5202
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4076
  ‚Ä¢ Validation Loss: 0.5215
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5215, best: 0.5202)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4213
  ‚Ä¢ Validation Loss: 0.5217
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5217, best: 0.5202)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.5231
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5231, best: 0.5202)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.5185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5185
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4043
  ‚Ä¢ Validation Loss: 0.5170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5170
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3662
  ‚Ä¢ Validation Loss: 0.5189
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5189, best: 0.5170)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3987
  ‚Ä¢ Validation Loss: 0.5170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5170, best: 0.5170)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3844
  ‚Ä¢ Validation Loss: 0.5198
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5198, best: 0.5170)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4095
  ‚Ä¢ Validation Loss: 0.5177
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5177, best: 0.5170)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4163
  ‚Ä¢ Validation Loss: 0.5118
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3667
  ‚Ä¢ Validation Loss: 0.5145
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5145, best: 0.5118)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3562
  ‚Ä¢ Validation Loss: 0.5174
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5174, best: 0.5118)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3822
  ‚Ä¢ Validation Loss: 0.5128
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5128, best: 0.5118)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4003
  ‚Ä¢ Validation Loss: 0.5141
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5141, best: 0.5118)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4033
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3774
  ‚Ä¢ Validation Loss: 0.5214
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5214, best: 0.5078)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3889
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5098, best: 0.5078)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4160
  ‚Ä¢ Validation Loss: 0.5130
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5130, best: 0.5078)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3962
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5152, best: 0.5078)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3543
  ‚Ä¢ Validation Loss: 0.5072
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5072
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3518
  ‚Ä¢ Validation Loss: 0.5091
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5091, best: 0.5072)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3694
  ‚Ä¢ Validation Loss: 0.5111
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5111, best: 0.5072)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3966
  ‚Ä¢ Validation Loss: 0.5064
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5064
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3844
  ‚Ä¢ Validation Loss: 0.5104
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5104, best: 0.5064)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3214
  ‚Ä¢ Validation Loss: 0.5111
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5111, best: 0.5064)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3649
  ‚Ä¢ Validation Loss: 0.5049
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5049
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3094
  ‚Ä¢ Validation Loss: 0.5040
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5040
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2805
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5020
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3138
  ‚Ä¢ Validation Loss: 0.5054
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5054, best: 0.5020)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2767
  ‚Ä¢ Validation Loss: 0.5037
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5037, best: 0.5020)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3084
  ‚Ä¢ Validation Loss: 0.5024
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5024, best: 0.5020)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2910
  ‚Ä¢ Validation Loss: 0.5022
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.5022, best: 0.5020)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3278
  ‚Ä¢ Validation Loss: 0.5029
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5029, best: 0.5020)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.5052
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5052, best: 0.5020)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2927
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5056, best: 0.5020)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2503
  ‚Ä¢ Validation Loss: 0.5042
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5042, best: 0.5020)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2775
  ‚Ä¢ Validation Loss: 0.5046
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5046, best: 0.5020)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3324
  ‚Ä¢ Validation Loss: 0.5016
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5016
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3185
  ‚Ä¢ Validation Loss: 0.4998
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4998
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2498
  ‚Ä¢ Validation Loss: 0.4983
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4983
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3499
  ‚Ä¢ Validation Loss: 0.5033
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5033, best: 0.4983)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3244
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3038
  ‚Ä¢ Validation Loss: 0.4963
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4963
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3084
  ‚Ä¢ Validation Loss: 0.5026
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5026, best: 0.4963)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.4979
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4979, best: 0.4963)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3050
  ‚Ä¢ Validation Loss: 0.5030
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5030, best: 0.4963)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2609
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4969, best: 0.4963)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2314
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4954
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.4944
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4944
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2807
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4972, best: 0.4944)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3474
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4973, best: 0.4944)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2444
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4975, best: 0.4944)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2977
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2628
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4967, best: 0.4942)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3330
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4938
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2630
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4938, best: 0.4938)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2365
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4960, best: 0.4938)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2469
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4942, best: 0.4938)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2832
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4922
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2597
  ‚Ä¢ Validation Loss: 0.4932
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4932, best: 0.4922)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3250
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4933, best: 0.4922)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3186
  ‚Ä¢ Validation Loss: 0.4930
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4930, best: 0.4922)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2431
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4953, best: 0.4922)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3328
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4933, best: 0.4922)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2961
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4928, best: 0.4922)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3442
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4953, best: 0.4922)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2918
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4938, best: 0.4922)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2772
  ‚Ä¢ Validation Loss: 0.4943
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4943, best: 0.4922)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2272
  ‚Ä¢ Validation Loss: 0.4949
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4949, best: 0.4922)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2797
  ‚Ä¢ Validation Loss: 0.4929
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4929, best: 0.4922)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2880
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4956, best: 0.4922)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4947, best: 0.4922)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4954, best: 0.4922)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2775
  ‚Ä¢ Validation Loss: 0.4937
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4937, best: 0.4922)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2761
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4934, best: 0.4922)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2995
  ‚Ä¢ Validation Loss: 0.4929
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4929, best: 0.4922)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3040
  ‚Ä¢ Validation Loss: 0.4941
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4941, best: 0.4922)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2575
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4927, best: 0.4922)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2883
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4925, best: 0.4922)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2514
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4924, best: 0.4922)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3230
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4916
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2897
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4916)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2554
  ‚Ä¢ Validation Loss: 0.5092
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5092, best: 0.4916)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2409
  ‚Ä¢ Validation Loss: 0.5127
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5127, best: 0.4916)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2589
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4972, best: 0.4916)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3001
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5010, best: 0.4916)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3224
  ‚Ä¢ Validation Loss: 0.4962
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4962, best: 0.4916)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3122
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4972, best: 0.4916)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2161
  ‚Ä¢ Validation Loss: 0.4971
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4971, best: 0.4916)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3002
  ‚Ä¢ Validation Loss: 0.4985
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4985, best: 0.4916)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2999
  ‚Ä¢ Validation Loss: 0.5006
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5006, best: 0.4916)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3420
  ‚Ä¢ Validation Loss: 0.4989
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4989, best: 0.4916)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2951
  ‚Ä¢ Validation Loss: 0.4962
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4962, best: 0.4916)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3011
  ‚Ä¢ Validation Loss: 0.4962
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4962, best: 0.4916)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1995
  ‚Ä¢ Validation Loss: 0.4949
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4949, best: 0.4916)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1791
  ‚Ä¢ Validation Loss: 0.4958
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4958, best: 0.4916)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2466
  ‚Ä¢ Validation Loss: 0.5044
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5044, best: 0.4916)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2405
  ‚Ä¢ Validation Loss: 0.4971
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4971, best: 0.4916)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3552
  ‚Ä¢ Validation Loss: 0.4939
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4939, best: 0.4916)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2545
  ‚Ä¢ Validation Loss: 0.5024
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5024, best: 0.4916)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2995
  ‚Ä¢ Validation Loss: 0.4948
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4948, best: 0.4916)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2731
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2418
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4974, best: 0.4910)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3298
  ‚Ä¢ Validation Loss: 0.4847
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4847
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2797
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4906, best: 0.4847)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3162
  ‚Ä¢ Validation Loss: 0.4908
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4908, best: 0.4847)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3062
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4934, best: 0.4847)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2701
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4880, best: 0.4847)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2488
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4869, best: 0.4847)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2958
  ‚Ä¢ Validation Loss: 0.4888
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4888, best: 0.4847)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2606
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4990, best: 0.4847)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2658
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4852, best: 0.4847)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2407
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4883, best: 0.4847)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2486
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4924, best: 0.4847)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3070
  ‚Ä¢ Validation Loss: 0.4917
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4917, best: 0.4847)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 184/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3580
  ‚Ä¢ Validation Loss: 0.4899
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4899, best: 0.4847)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2491
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4894, best: 0.4847)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3194
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4909, best: 0.4847)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2859
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4878, best: 0.4847)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2189
  ‚Ä¢ Validation Loss: 0.4848
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4848, best: 0.4847)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3187
  ‚Ä¢ Validation Loss: 0.4877
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4877, best: 0.4847)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1859
  ‚Ä¢ Validation Loss: 0.4840
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4840
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2335
  ‚Ä¢ Validation Loss: 0.4798
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4798
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2876
  ‚Ä¢ Validation Loss: 0.4874
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4874, best: 0.4798)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2620
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4827, best: 0.4798)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2767
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4946, best: 0.4798)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2988
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4800, best: 0.4798)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3061
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4782
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1966
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4891, best: 0.4782)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2742
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4805, best: 0.4782)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2852
  ‚Ä¢ Validation Loss: 0.4840
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4840, best: 0.4782)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2972
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4813, best: 0.4782)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2789
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4957, best: 0.4782)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2912
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4826, best: 0.4782)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2842
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4789, best: 0.4782)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1934
  ‚Ä¢ Validation Loss: 0.4792
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4792, best: 0.4782)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2863
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4839, best: 0.4782)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2836
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4784, best: 0.4782)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2407
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4803, best: 0.4782)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2173
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4801, best: 0.4782)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2853
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4764
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2374
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4779, best: 0.4764)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2156
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4811, best: 0.4764)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.4762
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4762
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2058
  ‚Ä¢ Validation Loss: 0.4796
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4796, best: 0.4762)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2338
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4793, best: 0.4762)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.4806
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4806, best: 0.4762)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1971
  ‚Ä¢ Validation Loss: 0.4788
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4788, best: 0.4762)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2160
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4801, best: 0.4762)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1898
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4820, best: 0.4762)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1725
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4833, best: 0.4762)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2160
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4795, best: 0.4762)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1593
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4824, best: 0.4762)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1861
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4756
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2048
  ‚Ä¢ Validation Loss: 0.4857
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4857, best: 0.4756)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1981
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4784, best: 0.4756)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1741
  ‚Ä¢ Validation Loss: 0.4819
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4819, best: 0.4756)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2487
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4773, best: 0.4756)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1876
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4802, best: 0.4756)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2116
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4757, best: 0.4756)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1530
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4814, best: 0.4756)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1405
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4747
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1705
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4753, best: 0.4747)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1429
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4761, best: 0.4747)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1418
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4744
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1862
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4753, best: 0.4744)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1295
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4782, best: 0.4744)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1536
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4790, best: 0.4744)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1689
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4766, best: 0.4744)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2005
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4771, best: 0.4744)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1954
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4717
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1896
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4759, best: 0.4717)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1694
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4820, best: 0.4717)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2164
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4751, best: 0.4717)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2135
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4744, best: 0.4717)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1700
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4778, best: 0.4717)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1932
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4753, best: 0.4717)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1402
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4717)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1657
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2253
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4717, best: 0.4709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1835
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4709)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1367
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4743, best: 0.4709)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1864
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4737, best: 0.4709)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1244
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4709)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1788
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4709)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0970
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4736, best: 0.4709)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1480
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4738, best: 0.4709)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1534
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4709)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1647
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4739, best: 0.4709)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1534
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4737, best: 0.4709)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1480
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4725, best: 0.4709)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2091
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4752, best: 0.4709)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2361
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4733, best: 0.4709)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1976
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4709)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1642
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4724, best: 0.4709)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1803
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4768, best: 0.4709)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1763
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4686
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1786
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4739, best: 0.4686)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4756, best: 0.4686)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0831
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4754, best: 0.4686)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2301
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4686)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4742, best: 0.4686)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4728, best: 0.4686)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1528
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4758, best: 0.4686)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1374
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4718, best: 0.4686)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1214
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4722, best: 0.4686)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2082
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4686)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1546
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4716, best: 0.4686)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4686)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1835
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4686)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1974
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4708, best: 0.4686)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2318
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4733, best: 0.4686)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2329
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4733, best: 0.4686)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2376
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4696, best: 0.4686)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1632
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4738, best: 0.4686)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2347
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4740, best: 0.4686)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1223
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4701, best: 0.4686)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1917
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2016
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4696, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1468
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4695, best: 0.4684)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1731
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4691, best: 0.4684)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1469
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4725, best: 0.4684)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1882
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4690, best: 0.4684)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1285
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4717, best: 0.4684)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1865
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4712, best: 0.4684)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2261
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4725, best: 0.4684)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2132
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4684)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1831
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4710, best: 0.4684)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1535
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4725, best: 0.4684)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2294
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4702, best: 0.4684)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1423
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4701, best: 0.4684)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2084
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4713, best: 0.4684)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4684
Total Epochs:   300
Models Saved:   ./Result/a4/Syr341
TensorBoard:    ./Result/a4/Syr341/tensorboard_logs
================================================================================

[19:02:42] Training completed. Best val loss: 0.4684

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9643, Recall=0.9828, F1=0.9735, IoU=0.9483
Paratext            : Precision=0.4793, Recall=0.4561, F1=0.4674, IoU=0.3050
Decoration          : Precision=0.9704, Recall=0.6255, F1=0.7607, IoU=0.6137
Main Text           : Precision=0.8658, Recall=0.8137, F1=0.8389, IoU=0.7226
Title               : Precision=0.1974, Recall=0.1854, F1=0.1912, IoU=0.1057

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6954
Mean Recall:    0.6127
Mean F1-Score:  0.6463
Mean IoU:       0.5391
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8241
Mean Recall:    0.7632
Mean F1-Score:  0.7885
Mean IoU:       0.6880
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Results Location: ./Result/a4/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Tue Nov 18 07:05:48 PM CET 2025
= Job-ID             : 1394004 on tinygpu
= Job-Name           : 4th
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 02:37:55
= Total RAM usage    : 8.7 GiB of requested  GiB (%)   
= Node list          : tg066
= Subm/Elig/Start/End: 2025-11-18T16:27:19 / 2025-11-18T16:27:19 / 2025-11-18T16:27:27 / 2025-11-18T19:05:22
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              61.7G   104.9G   209.7G        N/A     238K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2869752, 54 %, 39 %, 8248 MiB, 386179 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2871040, 18 %, 8 %, 854 MiB, 177871 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2871172, 66 %, 47 %, 8248 MiB, 978595 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2874729, 18 %, 8 %, 854 MiB, 178389 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2874771, 60 %, 43 %, 8114 MiB, 3773806 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2886994, 19 %, 9 %, 854 MiB, 174435 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2887032, 64 %, 46 %, 8092 MiB, 3527697 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 2898261, 19 %, 9 %, 854 MiB, 175948 ms
