### Starting TaskPrologue of job 1392370 on tg069 at Tue Nov 18 02:39:57 AM CET 2025
Running on cores 0-1,8-9,17,19,24-25 with governor ondemand
Tue Nov 18 02:39:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:18:00.0 Off |                  N/A |
| 29%   26C    P8             14W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL (ResNet-50 Encoder)

Component Details:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple (concatenation)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)
  ‚úì Loss: CB Loss (Class-Balanced, beta=0.9999) + Focal (Œ≥=2.0) + Dice
  ‚úó Deep Supervision: disabled
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Fourier Feature Fusion: disabled (using simple fusion)
  ‚úó Smart Skip Connections: disabled (using simple fusion)

Training Parameters:
  - Batch Size: 12 (best result configuration)
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience

Configuration:
  ‚úì Balanced sampler: ENABLED
  ‚úì Class-aware augmentation: ENABLED
  ‚úì Focal gamma: 2.0
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
Output Directory: ./Result/a1/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0253
  ‚Ä¢ Validation Loss: 0.7087
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7087
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6704
  ‚Ä¢ Validation Loss: 0.6195
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6195
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6222
  ‚Ä¢ Validation Loss: 0.6029
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6029
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6029
  ‚Ä¢ Validation Loss: 0.5814
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5814
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5820
  ‚Ä¢ Validation Loss: 0.5596
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5596
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5531
  ‚Ä¢ Validation Loss: 0.5431
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5431
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5411
  ‚Ä¢ Validation Loss: 0.5278
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5278
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5162
  ‚Ä¢ Validation Loss: 0.5221
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5221
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5102
  ‚Ä¢ Validation Loss: 0.5148
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5148
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4706
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5058
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4714
  ‚Ä¢ Validation Loss: 0.4984
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4984
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4544
  ‚Ä¢ Validation Loss: 0.4892
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4892
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4457
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4890
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4413
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4800
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3892
  ‚Ä¢ Validation Loss: 0.4794
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4794
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4269
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4826, best: 0.4794)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4215
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4752
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3961
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4736
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3950
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4726
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4686
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4052
  ‚Ä¢ Validation Loss: 0.4619
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4619
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3901
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4646, best: 0.4619)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3890
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4665, best: 0.4619)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4065
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4646, best: 0.4619)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4638, best: 0.4619)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3721
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4575
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3819
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4587, best: 0.4575)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3878
  ‚Ä¢ Validation Loss: 0.4576
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4576, best: 0.4575)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3793
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4561
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3873
  ‚Ä¢ Validation Loss: 0.4553
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4553
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3799
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4565, best: 0.4553)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3774
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4530
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3807
  ‚Ä¢ Validation Loss: 0.4546
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4546, best: 0.4530)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3803
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4506
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3653
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4513, best: 0.4506)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3756
  ‚Ä¢ Validation Loss: 0.4498
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4498
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3728
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4491
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3678
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4480
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3762
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4492, best: 0.4480)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3522
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4478
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3577
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4474
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3662
  ‚Ä¢ Validation Loss: 0.4456
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4456
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3772
  ‚Ä¢ Validation Loss: 0.4470
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4470, best: 0.4456)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3683
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4463, best: 0.4456)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3631
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4460, best: 0.4456)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3200
  ‚Ä¢ Validation Loss: 0.4449
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4449
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3214
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4464, best: 0.4449)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3176
  ‚Ä¢ Validation Loss: 0.4452
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4452, best: 0.4449)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3083
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4461, best: 0.4449)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3257
  ‚Ä¢ Validation Loss: 0.4452
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4452, best: 0.4449)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3119
  ‚Ä¢ Validation Loss: 0.4546
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4546, best: 0.4449)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2556
  ‚Ä¢ Validation Loss: 0.4543
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4543, best: 0.4449)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2946
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4561, best: 0.4449)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3182
  ‚Ä¢ Validation Loss: 0.4449
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4449, best: 0.4449)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2863
  ‚Ä¢ Validation Loss: 0.4544
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4544, best: 0.4449)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3289
  ‚Ä¢ Validation Loss: 0.4482
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4482, best: 0.4449)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3201
  ‚Ä¢ Validation Loss: 0.4424
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4424
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3159
  ‚Ä¢ Validation Loss: 0.4413
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4413
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2626
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4466, best: 0.4413)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2537
  ‚Ä¢ Validation Loss: 0.4409
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4409
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2887
  ‚Ä¢ Validation Loss: 0.4438
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4438, best: 0.4409)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2793
  ‚Ä¢ Validation Loss: 0.4401
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4401
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2679
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4463, best: 0.4401)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2772
  ‚Ä¢ Validation Loss: 0.4428
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4428, best: 0.4401)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2792
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4419, best: 0.4401)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2834
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4458, best: 0.4401)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2406
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4356
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2725
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4437, best: 0.4356)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4440, best: 0.4356)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4399, best: 0.4356)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2882
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4396, best: 0.4356)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2423
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4352
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2534
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2748
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4367, best: 0.4338)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2246
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4344, best: 0.4338)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3155
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4362, best: 0.4338)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.4306
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4306
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2729
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4333, best: 0.4306)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2539
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4325, best: 0.4306)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2279
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4270
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2303
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4280, best: 0.4270)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2552
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4288, best: 0.4270)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2105
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4270, best: 0.4270)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2317
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4320, best: 0.4270)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2211
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4376, best: 0.4270)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4294, best: 0.4270)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2351
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4254
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2029
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4250
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2112
  ‚Ä¢ Validation Loss: 0.4306
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4306, best: 0.4250)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2726
  ‚Ä¢ Validation Loss: 0.4224
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4224
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2503
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4260, best: 0.4224)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.4251
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4251, best: 0.4224)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2448
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4260, best: 0.4224)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2561
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4204
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2316
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4271, best: 0.4204)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2383
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4216, best: 0.4204)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1850
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4219, best: 0.4204)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2377
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4261, best: 0.4204)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2471
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4217, best: 0.4204)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2072
  ‚Ä¢ Validation Loss: 0.4279
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4279, best: 0.4204)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2363
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4296, best: 0.4204)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2222
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4213, best: 0.4204)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4222, best: 0.4204)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1887
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4271, best: 0.4204)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0723
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4235, best: 0.4204)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0975
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4216, best: 0.4204)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0549
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4226, best: 0.4204)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1085
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4219, best: 0.4204)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1039
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4184
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1234
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4194, best: 0.4184)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0862
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4162
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0988
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4290, best: 0.4162)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1288
  ‚Ä¢ Validation Loss: 0.4205
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4205, best: 0.4162)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0514
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4219, best: 0.4162)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1045
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4192, best: 0.4162)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4243, best: 0.4162)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0620
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4192, best: 0.4162)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1141
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4169, best: 0.4162)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0467
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4206, best: 0.4162)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1089
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4149
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0827
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4173, best: 0.4149)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1063
  ‚Ä¢ Validation Loss: 0.4181
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4181, best: 0.4149)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0838
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4152, best: 0.4149)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0905
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4184, best: 0.4149)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0836
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4166, best: 0.4149)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0629
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4152, best: 0.4149)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1237
  ‚Ä¢ Validation Loss: 0.4151
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4151, best: 0.4149)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1007
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4171, best: 0.4149)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1093
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4152, best: 0.4149)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4173, best: 0.4149)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1183
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4168, best: 0.4149)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0580
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4171, best: 0.4149)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0670
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4160, best: 0.4149)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1040
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4171, best: 0.4149)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0983
  ‚Ä¢ Validation Loss: 0.4198
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4198, best: 0.4149)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1253
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4189, best: 0.4149)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1043
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4172, best: 0.4149)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1141
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4166, best: 0.4149)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1013
  ‚Ä¢ Validation Loss: 0.4167
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4167, best: 0.4149)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0658
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4191, best: 0.4149)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0508
  ‚Ä¢ Validation Loss: 0.4187
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4187, best: 0.4149)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0748
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4176, best: 0.4149)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0578
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4193, best: 0.4149)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.4181
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4181, best: 0.4149)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0567
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4170, best: 0.4149)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1015
  ‚Ä¢ Validation Loss: 0.4159
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4159, best: 0.4149)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1312
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4156, best: 0.4149)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0766
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4191, best: 0.4149)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1003
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4169, best: 0.4149)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0784
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4166, best: 0.4149)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0642
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4231, best: 0.4149)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0274
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4260, best: 0.4149)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0484
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4263, best: 0.4149)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0894
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4230, best: 0.4149)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4204, best: 0.4149)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4302, best: 0.4149)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0424
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4249, best: 0.4149)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0696
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4348, best: 0.4149)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4185, best: 0.4149)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4220, best: 0.4149)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0827
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4185, best: 0.4149)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0629
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4219, best: 0.4149)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0427
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4328, best: 0.4149)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0798
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4286, best: 0.4149)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0633
  ‚Ä¢ Validation Loss: 0.4401
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4401, best: 0.4149)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0631
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4158, best: 0.4149)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0537
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4237, best: 0.4149)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0946
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4211, best: 0.4149)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0916
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4207, best: 0.4149)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0460
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4263, best: 0.4149)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0384
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4264, best: 0.4149)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0461
  ‚Ä¢ Validation Loss: 0.4179
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4179, best: 0.4149)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0535
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4212, best: 0.4149)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0634
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4243, best: 0.4149)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0599
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4172, best: 0.4149)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0601
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4142
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0769
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4195, best: 0.4142)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0899
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4254, best: 0.4142)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1294
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4133
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4267, best: 0.4133)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0839
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4113
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0830
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4148, best: 0.4113)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0327
  ‚Ä¢ Validation Loss: 0.4111
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4111
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0548
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4248, best: 0.4111)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0607
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4226, best: 0.4111)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0699
  ‚Ä¢ Validation Loss: 0.4292
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4292, best: 0.4111)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0982
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4192, best: 0.4111)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0917
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4219, best: 0.4111)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0986
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4227, best: 0.4111)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0373
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4207, best: 0.4111)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0638
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4172, best: 0.4111)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1337
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4228, best: 0.4111)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4132, best: 0.4111)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0746
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4170, best: 0.4111)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0836
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4163, best: 0.4111)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0785
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4108
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0574
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4172, best: 0.4108)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0618
  ‚Ä¢ Validation Loss: 0.4178
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4178, best: 0.4108)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0821
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4135, best: 0.4108)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0861
  ‚Ä¢ Validation Loss: 0.4128
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4128, best: 0.4108)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1034
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4108, best: 0.4108)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0682
  ‚Ä¢ Validation Loss: 0.4180
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4180, best: 0.4108)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0279
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4274, best: 0.4108)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0575
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4147, best: 0.4108)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0748
  ‚Ä¢ Validation Loss: 0.4208
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4208, best: 0.4108)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4151
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4151, best: 0.4108)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1086
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4213, best: 0.4108)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0578
  ‚Ä¢ Validation Loss: 0.4103
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4103
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4102
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0468
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4285, best: 0.4102)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0428
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4216, best: 0.4102)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0837
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4243, best: 0.4102)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0841
  ‚Ä¢ Validation Loss: 0.4139
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4139, best: 0.4102)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0425
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4215, best: 0.4102)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0657
  ‚Ä¢ Validation Loss: 0.4106
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4106, best: 0.4102)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0877
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4237, best: 0.4102)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0513
  ‚Ä¢ Validation Loss: 0.4157
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4157, best: 0.4102)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0779
  ‚Ä¢ Validation Loss: 0.4305
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4305, best: 0.4102)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0757
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4105, best: 0.4102)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0938
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4191, best: 0.4102)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0459
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4197, best: 0.4102)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0727
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4109, best: 0.4102)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0834
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4122, best: 0.4102)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0881
  ‚Ä¢ Validation Loss: 0.4139
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4139, best: 0.4102)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0784
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4140, best: 0.4102)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0495
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4163, best: 0.4102)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0783
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4141, best: 0.4102)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0600
  ‚Ä¢ Validation Loss: 0.4165
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4165, best: 0.4102)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0927
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4126, best: 0.4102)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0754
  ‚Ä¢ Validation Loss: 0.4130
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4130, best: 0.4102)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0718
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4160, best: 0.4102)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0445
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4068
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0542
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4071, best: 0.4068)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0777
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4148, best: 0.4068)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0483
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4142, best: 0.4068)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0576
  ‚Ä¢ Validation Loss: 0.4076
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4076, best: 0.4068)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1074
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4135, best: 0.4068)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4073, best: 0.4068)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0733
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4091, best: 0.4068)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1076
  ‚Ä¢ Validation Loss: 0.4123
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4123, best: 0.4068)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4067
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0753
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4141, best: 0.4067)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0580
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4058
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1165
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4079, best: 0.4058)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0503
  ‚Ä¢ Validation Loss: 0.4174
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4174, best: 0.4058)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0740
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4113, best: 0.4058)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4065, best: 0.4058)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1299
  ‚Ä¢ Validation Loss: 0.4077
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4077, best: 0.4058)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0814
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4138, best: 0.4058)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0965
  ‚Ä¢ Validation Loss: 0.4155
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4155, best: 0.4058)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0526
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4149, best: 0.4058)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1027
  ‚Ä¢ Validation Loss: 0.4051
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4051
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0504
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4047
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0897
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4091, best: 0.4047)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1257
  ‚Ä¢ Validation Loss: 0.4076
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4076, best: 0.4047)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0496
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4140, best: 0.4047)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0303
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4133, best: 0.4047)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0820
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4109, best: 0.4047)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1210
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4091, best: 0.4047)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1034
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4094, best: 0.4047)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0917
  ‚Ä¢ Validation Loss: 0.4086
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4086, best: 0.4047)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0555
  ‚Ä¢ Validation Loss: 0.4054
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4054, best: 0.4047)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0893
  ‚Ä¢ Validation Loss: 0.4051
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4051, best: 0.4047)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0965
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4045
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0701
  ‚Ä¢ Validation Loss: 0.4083
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4083, best: 0.4045)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0817
  ‚Ä¢ Validation Loss: 0.4016
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4016
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0577
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4046, best: 0.4016)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0943
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4038, best: 0.4016)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0776
  ‚Ä¢ Validation Loss: 0.4070
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4070, best: 0.4016)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0975
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4072, best: 0.4016)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4059, best: 0.4016)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0790
  ‚Ä¢ Validation Loss: 0.4062
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4062, best: 0.4016)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0192
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4025, best: 0.4016)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0196
  ‚Ä¢ Validation Loss: 0.4014
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4014
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0176
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4028, best: 0.4014)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0351
  ‚Ä¢ Validation Loss: 0.4043
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4043, best: 0.4014)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0307
  ‚Ä¢ Validation Loss: 0.4039
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4039, best: 0.4014)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0291
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4066, best: 0.4014)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0233
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4041, best: 0.4014)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0252
  ‚Ä¢ Validation Loss: 0.4033
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4033, best: 0.4014)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0437
  ‚Ä¢ Validation Loss: 0.4034
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4034, best: 0.4014)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0109
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4036, best: 0.4014)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0223
  ‚Ä¢ Validation Loss: 0.4021
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4021, best: 0.4014)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0290
  ‚Ä¢ Validation Loss: 0.4060
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4060, best: 0.4014)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0436
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4028, best: 0.4014)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4028, best: 0.4014)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0325
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4041, best: 0.4014)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0312
  ‚Ä¢ Validation Loss: 0.4031
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4031, best: 0.4014)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0394
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4038, best: 0.4014)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0073
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4041, best: 0.4014)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0397
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4047, best: 0.4014)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0188
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4058, best: 0.4014)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0175
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4047, best: 0.4014)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0458
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4071, best: 0.4014)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0056
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4057, best: 0.4014)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0170
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4052, best: 0.4014)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0053
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4095, best: 0.4014)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0112
  ‚Ä¢ Validation Loss: 0.4054
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4054, best: 0.4014)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0342
  ‚Ä¢ Validation Loss: 0.4042
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4042, best: 0.4014)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0388
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4045, best: 0.4014)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4014
Total Epochs:   300
Models Saved:   ./Result/a1/Latin2
TensorBoard:    ./Result/a1/Latin2/tensorboard_logs
================================================================================

[03:46:28] Training completed. Best val loss: 0.4014

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9874, Recall=0.9928, F1=0.9901, IoU=0.9804
Paratext            : Precision=0.7379, Recall=0.6959, F1=0.7163, IoU=0.5580
Decoration          : Precision=0.8798, Recall=0.8929, F1=0.8863, IoU=0.7958
Main Text           : Precision=0.8527, Recall=0.8036, F1=0.8275, IoU=0.7057
Title               : Precision=0.8834, Recall=0.7808, F1=0.8290, IoU=0.7079
Chapter Headings    : Precision=0.6196, Recall=0.4179, F1=0.4991, IoU=0.3325

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8268
Mean Recall:    0.7640
Mean F1-Score:  0.7914
Mean IoU:       0.6801
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
Output Directory: ./Result/a1/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0829
  ‚Ä¢ Validation Loss: 0.6937
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6937
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7080
  ‚Ä¢ Validation Loss: 0.6005
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6005
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6845
  ‚Ä¢ Validation Loss: 0.5856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5856
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6349
  ‚Ä¢ Validation Loss: 0.5689
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5689
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6054
  ‚Ä¢ Validation Loss: 0.5464
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5464
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5933
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5610
  ‚Ä¢ Validation Loss: 0.5545
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5545, best: 0.5373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5450
  ‚Ä¢ Validation Loss: 0.5181
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5181
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5342
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5182, best: 0.5181)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5183
  ‚Ä¢ Validation Loss: 0.5085
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5085
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5041
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5001
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5025
  ‚Ä¢ Validation Loss: 0.4991
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4991
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4804
  ‚Ä¢ Validation Loss: 0.4929
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4929
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4762
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4872
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4654
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4814
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4738
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4838, best: 0.4814)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4628
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4815, best: 0.4814)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4448
  ‚Ä¢ Validation Loss: 0.4770
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4770
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4547
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4753
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4477
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4260
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4033
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4684, best: 0.4654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4198
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4652
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4206
  ‚Ä¢ Validation Loss: 0.4609
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4609
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4136
  ‚Ä¢ Validation Loss: 0.4645
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4645, best: 0.4609)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4102
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4639, best: 0.4609)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4106
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4634, best: 0.4609)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3989
  ‚Ä¢ Validation Loss: 0.4614
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4614, best: 0.4609)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3955
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4566
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4001
  ‚Ä¢ Validation Loss: 0.4637
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4637, best: 0.4566)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3773
  ‚Ä¢ Validation Loss: 0.4547
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3994
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4569, best: 0.4547)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3978
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4581, best: 0.4547)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3773
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4588, best: 0.4547)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3883
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4566, best: 0.4547)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3845
  ‚Ä¢ Validation Loss: 0.4539
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4539
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3433
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4537
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3668
  ‚Ä¢ Validation Loss: 0.4545
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4545, best: 0.4537)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3765
  ‚Ä¢ Validation Loss: 0.4542
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4542, best: 0.4537)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3739
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4530
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3857
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4532, best: 0.4530)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3700
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4527
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3672
  ‚Ä¢ Validation Loss: 0.4536
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4536, best: 0.4527)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3852
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4524
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3917
  ‚Ä¢ Validation Loss: 0.4529
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4529, best: 0.4524)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3328
  ‚Ä¢ Validation Loss: 0.4515
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4515
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2943
  ‚Ä¢ Validation Loss: 0.4528
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4528, best: 0.4515)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2881
  ‚Ä¢ Validation Loss: 0.4518
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4518, best: 0.4515)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3602
  ‚Ä¢ Validation Loss: 0.4522
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4522, best: 0.4515)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2860
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4527, best: 0.4515)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2899
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4594, best: 0.4515)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3371
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4527, best: 0.4515)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2512
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4575, best: 0.4515)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3233
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4601, best: 0.4515)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2742
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4554, best: 0.4515)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3213
  ‚Ä¢ Validation Loss: 0.4503
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4503
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3221
  ‚Ä¢ Validation Loss: 0.4539
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4539, best: 0.4503)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2936
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4590, best: 0.4503)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2399
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4471
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3129
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4490, best: 0.4471)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4485, best: 0.4471)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3042
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4469
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2748
  ‚Ä¢ Validation Loss: 0.4497
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4497, best: 0.4469)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2351
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4494, best: 0.4469)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2968
  ‚Ä¢ Validation Loss: 0.4509
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4509, best: 0.4469)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3025
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4496, best: 0.4469)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4516, best: 0.4469)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3009
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4469, best: 0.4469)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2912
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4419
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2689
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4491, best: 0.4419)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2117
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4479, best: 0.4419)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2341
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4464, best: 0.4419)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2901
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4416
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2260
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4426, best: 0.4416)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2715
  ‚Ä¢ Validation Loss: 0.4423
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4423, best: 0.4416)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2765
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4408
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2334
  ‚Ä¢ Validation Loss: 0.4497
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4497, best: 0.4408)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2873
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4346
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2340
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4408, best: 0.4346)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3081
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4371, best: 0.4346)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2118
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4352, best: 0.4346)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2516
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4442, best: 0.4346)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2652
  ‚Ä¢ Validation Loss: 0.4420
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4420, best: 0.4346)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2565
  ‚Ä¢ Validation Loss: 0.4388
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4388, best: 0.4346)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2380
  ‚Ä¢ Validation Loss: 0.4418
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4418, best: 0.4346)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2554
  ‚Ä¢ Validation Loss: 0.4336
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4336
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2833
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4337, best: 0.4336)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4344, best: 0.4336)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2494
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4311
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2762
  ‚Ä¢ Validation Loss: 0.4316
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4316, best: 0.4311)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2304
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4327, best: 0.4311)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2465
  ‚Ä¢ Validation Loss: 0.4314
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4314, best: 0.4311)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2222
  ‚Ä¢ Validation Loss: 0.4306
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4306
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2070
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4332, best: 0.4306)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2400
  ‚Ä¢ Validation Loss: 0.4317
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4317, best: 0.4306)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2803
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4325, best: 0.4306)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2756
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4331, best: 0.4306)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2262
  ‚Ä¢ Validation Loss: 0.4281
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4281
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2285
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4253
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2687
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4302, best: 0.4253)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3107
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4252
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2421
  ‚Ä¢ Validation Loss: 0.4262
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4262, best: 0.4252)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2138
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4265, best: 0.4252)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1938
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4266, best: 0.4252)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1796
  ‚Ä¢ Validation Loss: 0.4246
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4246
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0952
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4294, best: 0.4246)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1209
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4269, best: 0.4246)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0958
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4261, best: 0.4246)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0872
  ‚Ä¢ Validation Loss: 0.4306
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4306, best: 0.4246)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0910
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4245
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4266, best: 0.4245)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1255
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4289, best: 0.4245)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4253, best: 0.4245)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0912
  ‚Ä¢ Validation Loss: 0.4240
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4240
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1530
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4263, best: 0.4240)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1514
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4220
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1247
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4231, best: 0.4220)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1429
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4233, best: 0.4220)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1382
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4237, best: 0.4220)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1559
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4231, best: 0.4220)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0960
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4231, best: 0.4220)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1244
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4206
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1839
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4222, best: 0.4206)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4217, best: 0.4206)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1191
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4232, best: 0.4206)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1311
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4221, best: 0.4206)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1598
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4233, best: 0.4206)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1022
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4220, best: 0.4206)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4212, best: 0.4206)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1300
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4218, best: 0.4206)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1325
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4216, best: 0.4206)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1422
  ‚Ä¢ Validation Loss: 0.4224
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4224, best: 0.4206)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1812
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4212, best: 0.4206)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0966
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4220, best: 0.4206)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1160
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4206)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1347
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4233, best: 0.4206)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1255
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4196
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0936
  ‚Ä¢ Validation Loss: 0.4203
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4203, best: 0.4196)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0997
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4211, best: 0.4196)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1100
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4213, best: 0.4196)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1412
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4196)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1600
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4196)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1181
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4215, best: 0.4196)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0671
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4214, best: 0.4196)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1411
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4213, best: 0.4196)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1085
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4219, best: 0.4196)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1755
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4196)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1590
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4207, best: 0.4196)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4220, best: 0.4196)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1189
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4227, best: 0.4196)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1269
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4229, best: 0.4196)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0896
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4286, best: 0.4196)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1411
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4225, best: 0.4196)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0862
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4311, best: 0.4196)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1417
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4244, best: 0.4196)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0789
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4236, best: 0.4196)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1434
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4270, best: 0.4196)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1092
  ‚Ä¢ Validation Loss: 0.4257
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4257, best: 0.4196)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1132
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4222, best: 0.4196)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1222
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4245, best: 0.4196)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0812
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4271, best: 0.4196)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0789
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4321, best: 0.4196)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0879
  ‚Ä¢ Validation Loss: 0.4279
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4279, best: 0.4196)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1603
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4318, best: 0.4196)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1003
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4227, best: 0.4196)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4290, best: 0.4196)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0926
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4220, best: 0.4196)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1198
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4218, best: 0.4196)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4222, best: 0.4196)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1256
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4196)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0937
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4244, best: 0.4196)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4254, best: 0.4196)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0999
  ‚Ä¢ Validation Loss: 0.4292
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4292, best: 0.4196)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1009
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4222, best: 0.4196)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0881
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4230, best: 0.4196)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1015
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4232, best: 0.4196)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1211
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4295, best: 0.4196)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1795
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4206, best: 0.4196)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0840
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4315, best: 0.4196)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1177
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4204, best: 0.4196)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1170
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.4182
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4182, best: 0.4177)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1090
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4204, best: 0.4177)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1071
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4310, best: 0.4177)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1136
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4236, best: 0.4177)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1097
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4194, best: 0.4177)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0805
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4226, best: 0.4177)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0843
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4234, best: 0.4177)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4170
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1006
  ‚Ä¢ Validation Loss: 0.4258
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4258, best: 0.4170)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1142
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4191, best: 0.4170)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1338
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4183, best: 0.4170)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1367
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4249, best: 0.4170)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1461
  ‚Ä¢ Validation Loss: 0.4200
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4200, best: 0.4170)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1263
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4177, best: 0.4170)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1598
  ‚Ä¢ Validation Loss: 0.4187
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4187, best: 0.4170)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1204
  ‚Ä¢ Validation Loss: 0.4144
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4144
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4140
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1092
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4213, best: 0.4140)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1285
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4161, best: 0.4140)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1154
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4148, best: 0.4140)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1406
  ‚Ä¢ Validation Loss: 0.4164
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4164, best: 0.4140)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4181
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4181, best: 0.4140)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0901
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4192, best: 0.4140)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1280
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4166, best: 0.4140)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4153, best: 0.4140)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1145
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4226, best: 0.4140)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1212
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4113
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4204, best: 0.4113)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1457
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4162, best: 0.4113)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1551
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4185, best: 0.4113)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0794
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4168, best: 0.4113)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4131
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4131, best: 0.4113)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1366
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4138, best: 0.4113)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1026
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4135, best: 0.4113)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1587
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4170, best: 0.4113)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1176
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4161, best: 0.4113)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1447
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4162, best: 0.4113)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1327
  ‚Ä¢ Validation Loss: 0.4182
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4182, best: 0.4113)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1201
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4148, best: 0.4113)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0869
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4173, best: 0.4113)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1080
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4176, best: 0.4113)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0834
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4101
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0155
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4102, best: 0.4101)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0524
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4122, best: 0.4101)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0518
  ‚Ä¢ Validation Loss: 0.4118
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4118, best: 0.4101)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.4123
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4123, best: 0.4101)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0288
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4122, best: 0.4101)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0439
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4161, best: 0.4101)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0328
  ‚Ä¢ Validation Loss: 0.4145
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4145, best: 0.4101)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0419
  ‚Ä¢ Validation Loss: 0.4128
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4128, best: 0.4101)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0374
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4132, best: 0.4101)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0152
  ‚Ä¢ Validation Loss: 0.4136
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4136, best: 0.4101)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0391
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4105, best: 0.4101)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0462
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4091
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0401
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4105, best: 0.4091)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4102, best: 0.4091)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0235
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4102, best: 0.4091)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0397
  ‚Ä¢ Validation Loss: 0.4088
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4088
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0436
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4152, best: 0.4088)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0204
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4183, best: 0.4088)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0283
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4199, best: 0.4088)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0144
  ‚Ä¢ Validation Loss: 0.4208
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4208, best: 0.4088)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0309
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4177, best: 0.4088)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0289
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4147, best: 0.4088)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0442
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4160, best: 0.4088)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0415
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4121, best: 0.4088)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0715
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4156, best: 0.4088)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0564
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4168, best: 0.4088)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0254
  ‚Ä¢ Validation Loss: 0.4167
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4167, best: 0.4088)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0447
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4168, best: 0.4088)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0377
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4138, best: 0.4088)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0552
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4107, best: 0.4088)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0504
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4140, best: 0.4088)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0457
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4132, best: 0.4088)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0692
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4127, best: 0.4088)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0335
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4117, best: 0.4088)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0450
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4121, best: 0.4088)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0104
  ‚Ä¢ Validation Loss: 0.4128
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4128, best: 0.4088)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0226
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4133, best: 0.4088)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0132
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4135, best: 0.4088)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0282
  ‚Ä¢ Validation Loss: 0.4144
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4144, best: 0.4088)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0458
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4142, best: 0.4088)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0396
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4112, best: 0.4088)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0604
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4113, best: 0.4088)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0292
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4126, best: 0.4088)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0303
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4141, best: 0.4088)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0136
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4149, best: 0.4088)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0385
  ‚Ä¢ Validation Loss: 0.4118
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4118, best: 0.4088)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0482
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4121, best: 0.4088)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0478
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4121, best: 0.4088)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0369
  ‚Ä¢ Validation Loss: 0.4154
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4154, best: 0.4088)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0133
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4173, best: 0.4088)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0347
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4161, best: 0.4088)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0353
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4153, best: 0.4088)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4147, best: 0.4088)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0580
  ‚Ä¢ Validation Loss: 0.4093
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4093, best: 0.4088)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4107, best: 0.4088)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0338
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4116, best: 0.4088)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0206
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4133, best: 0.4088)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0196
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4138, best: 0.4088)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0252
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4141, best: 0.4088)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0191
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4126, best: 0.4088)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0497
  ‚Ä¢ Validation Loss: 0.4100
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4100, best: 0.4088)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0484
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4116, best: 0.4088)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0389
  ‚Ä¢ Validation Loss: 0.4128
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4128, best: 0.4088)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0435
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4122, best: 0.4088)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0454
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4108, best: 0.4088)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0541
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4113, best: 0.4088)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0539
  ‚Ä¢ Validation Loss: 0.4125
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4125, best: 0.4088)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0414
  ‚Ä¢ Validation Loss: 0.4114
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4114, best: 0.4088)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0514
  ‚Ä¢ Validation Loss: 0.4103
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4103, best: 0.4088)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0292
  ‚Ä¢ Validation Loss: 0.4104
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4104, best: 0.4088)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0182
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4095, best: 0.4088)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0309
  ‚Ä¢ Validation Loss: 0.4100
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4100, best: 0.4088)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0710
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4102, best: 0.4088)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0404
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4107, best: 0.4088)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0478
  ‚Ä¢ Validation Loss: 0.4099
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4099, best: 0.4088)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0433
  ‚Ä¢ Validation Loss: 0.4098
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4098, best: 0.4088)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0469
  ‚Ä¢ Validation Loss: 0.4114
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4114, best: 0.4088)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4088
Total Epochs:   300
Models Saved:   ./Result/a1/Latin14396
TensorBoard:    ./Result/a1/Latin14396/tensorboard_logs
================================================================================

[04:55:45] Training completed. Best val loss: 0.4088

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9887, Recall=0.9894, F1=0.9891, IoU=0.9784
Paratext            : Precision=0.5785, Recall=0.6012, F1=0.5897, IoU=0.4181
Decoration          : Precision=0.9051, Recall=0.9416, F1=0.9230, IoU=0.8569
Main Text           : Precision=0.8744, Recall=0.8858, F1=0.8801, IoU=0.7858
Title               : Precision=0.8352, Recall=0.7685, F1=0.8005, IoU=0.6674
Chapter Headings    : Precision=0.8557, Recall=0.5436, F1=0.6649, IoU=0.4980

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8396
Mean Recall:    0.7884
Mean F1-Score:  0.8079
Mean IoU:       0.7008
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
Output Directory: ./Result/a1/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.1464
  ‚Ä¢ Validation Loss: 0.6684
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7224
  ‚Ä¢ Validation Loss: 0.5992
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5992
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6792
  ‚Ä¢ Validation Loss: 0.5777
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5777
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6336
  ‚Ä¢ Validation Loss: 0.5673
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5673
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6117
  ‚Ä¢ Validation Loss: 0.5455
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5455
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5726
  ‚Ä¢ Validation Loss: 0.5376
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5376
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5525
  ‚Ä¢ Validation Loss: 0.5127
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5127
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5353
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4976
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5147
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4996, best: 0.4976)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4954
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4757
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4934, best: 0.4820)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4722
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4683
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4674
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4672
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4580
  ‚Ä¢ Validation Loss: 0.4637
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4637
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4451
  ‚Ä¢ Validation Loss: 0.4572
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4572
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4433
  ‚Ä¢ Validation Loss: 0.4583
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4583, best: 0.4572)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4321
  ‚Ä¢ Validation Loss: 0.4535
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4535
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4272
  ‚Ä¢ Validation Loss: 0.4558
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4558, best: 0.4535)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4168
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4574, best: 0.4535)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4103
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4463
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4106
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4490, best: 0.4463)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4051
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4513, best: 0.4463)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3958
  ‚Ä¢ Validation Loss: 0.4443
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4443
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4082
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4426
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3885
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4427, best: 0.4426)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4009
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4363
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3943
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4384, best: 0.4363)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3893
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4339
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3762
  ‚Ä¢ Validation Loss: 0.4317
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4317
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3792
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4300
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3832
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4332, best: 0.4300)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3801
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4294
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3704
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4310, best: 0.4294)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3749
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3693
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4326, best: 0.4284)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3593
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3664
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4299, best: 0.4284)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3621
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4253
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3585
  ‚Ä¢ Validation Loss: 0.4273
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4273, best: 0.4253)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3658
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4270, best: 0.4253)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3676
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4264, best: 0.4253)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3597
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4236
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3658
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4243, best: 0.4236)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3661
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4223
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3234
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4228, best: 0.4223)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3278
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4252, best: 0.4223)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3089
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4227, best: 0.4223)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3119
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4235, best: 0.4223)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3063
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4223, best: 0.4223)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3351
  ‚Ä¢ Validation Loss: 0.4241
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4241, best: 0.4223)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3108
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4339, best: 0.4223)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3438
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4264, best: 0.4223)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2999
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4319, best: 0.4223)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3101
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4348, best: 0.4223)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2908
  ‚Ä¢ Validation Loss: 0.4240
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4240, best: 0.4223)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4189
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2538
  ‚Ä¢ Validation Loss: 0.4317
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4317, best: 0.4189)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3277
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4250, best: 0.4189)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3014
  ‚Ä¢ Validation Loss: 0.4272
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4272, best: 0.4189)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2799
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4193, best: 0.4189)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2546
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4217, best: 0.4189)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2388
  ‚Ä¢ Validation Loss: 0.4188
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4188
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2929
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2844
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4156
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2575
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4121
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2616
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4141, best: 0.4121)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4199, best: 0.4121)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2449
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4160, best: 0.4121)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2520
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4158, best: 0.4121)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4127, best: 0.4121)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2446
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4147, best: 0.4121)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2253
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4113
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2531
  ‚Ä¢ Validation Loss: 0.4060
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4060
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2480
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4148, best: 0.4060)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.4143
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4143, best: 0.4060)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2274
  ‚Ä¢ Validation Loss: 0.4150
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4150, best: 0.4060)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2121
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4149, best: 0.4060)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2261
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4223, best: 0.4060)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2398
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4055
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1938
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4047
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2781
  ‚Ä¢ Validation Loss: 0.4040
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4040
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2250
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4068, best: 0.4040)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2014
  ‚Ä¢ Validation Loss: 0.4012
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4012
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1983
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4038, best: 0.4012)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1945
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3992
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2192
  ‚Ä¢ Validation Loss: 0.4020
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4020, best: 0.3992)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2099
  ‚Ä¢ Validation Loss: 0.4051
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4051, best: 0.3992)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1735
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4101, best: 0.3992)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2311
  ‚Ä¢ Validation Loss: 0.3998
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3998, best: 0.3992)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2078
  ‚Ä¢ Validation Loss: 0.3982
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3982
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2634
  ‚Ä¢ Validation Loss: 0.3972
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3972
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2288
  ‚Ä¢ Validation Loss: 0.3954
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3954
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2247
  ‚Ä¢ Validation Loss: 0.3996
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3996, best: 0.3954)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2170
  ‚Ä¢ Validation Loss: 0.3984
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3984, best: 0.3954)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.3973
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3973, best: 0.3954)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2146
  ‚Ä¢ Validation Loss: 0.3984
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3984, best: 0.3954)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2248
  ‚Ä¢ Validation Loss: 0.3979
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3979, best: 0.3954)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2357
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3980, best: 0.3954)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2204
  ‚Ä¢ Validation Loss: 0.3963
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.3963, best: 0.3954)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2451
  ‚Ä¢ Validation Loss: 0.3943
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3943
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1757
  ‚Ä¢ Validation Loss: 0.3989
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3989, best: 0.3943)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2007
  ‚Ä¢ Validation Loss: 0.3972
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3972, best: 0.3943)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1135
  ‚Ä¢ Validation Loss: 0.3956
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3956, best: 0.3943)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0951
  ‚Ä¢ Validation Loss: 0.3940
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3940
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1002
  ‚Ä¢ Validation Loss: 0.3953
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3953, best: 0.3940)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0863
  ‚Ä¢ Validation Loss: 0.3931
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3931
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1125
  ‚Ä¢ Validation Loss: 0.3975
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3975, best: 0.3931)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0848
  ‚Ä¢ Validation Loss: 0.3948
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3948, best: 0.3931)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0972
  ‚Ä¢ Validation Loss: 0.3951
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3951, best: 0.3931)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1025
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3958, best: 0.3931)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0456
  ‚Ä¢ Validation Loss: 0.4001
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4001, best: 0.3931)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0191
  ‚Ä¢ Validation Loss: 0.3986
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3986, best: 0.3931)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0526
  ‚Ä¢ Validation Loss: 0.3967
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3967, best: 0.3931)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1111
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3958, best: 0.3931)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1041
  ‚Ä¢ Validation Loss: 0.3926
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3926
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.3962
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3962, best: 0.3926)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1065
  ‚Ä¢ Validation Loss: 0.3954
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3954, best: 0.3926)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1357
  ‚Ä¢ Validation Loss: 0.3918
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3918
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1330
  ‚Ä¢ Validation Loss: 0.3920
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3920, best: 0.3918)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.3898
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3898
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0738
  ‚Ä¢ Validation Loss: 0.3920
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3920, best: 0.3898)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1133
  ‚Ä¢ Validation Loss: 0.3915
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3915, best: 0.3898)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1512
  ‚Ä¢ Validation Loss: 0.3900
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3900, best: 0.3898)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.3918
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3918, best: 0.3898)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0874
  ‚Ä¢ Validation Loss: 0.3933
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3933, best: 0.3898)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1238
  ‚Ä¢ Validation Loss: 0.3899
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3899, best: 0.3898)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0948
  ‚Ä¢ Validation Loss: 0.3932
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3932, best: 0.3898)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1130
  ‚Ä¢ Validation Loss: 0.3919
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3919, best: 0.3898)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1076
  ‚Ä¢ Validation Loss: 0.3917
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3917, best: 0.3898)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1212
  ‚Ä¢ Validation Loss: 0.3924
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3924, best: 0.3898)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1171
  ‚Ä¢ Validation Loss: 0.3891
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3891
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.3908
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3908, best: 0.3891)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1139
  ‚Ä¢ Validation Loss: 0.3913
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3913, best: 0.3891)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1685
  ‚Ä¢ Validation Loss: 0.3906
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3906, best: 0.3891)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0991
  ‚Ä¢ Validation Loss: 0.3904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3904, best: 0.3891)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.3907
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3907, best: 0.3891)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1391
  ‚Ä¢ Validation Loss: 0.3907
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3907, best: 0.3891)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1169
  ‚Ä¢ Validation Loss: 0.3899
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3899, best: 0.3891)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1065
  ‚Ä¢ Validation Loss: 0.3888
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3888
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.3901
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3901, best: 0.3888)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1121
  ‚Ä¢ Validation Loss: 0.3900
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3900, best: 0.3888)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0955
  ‚Ä¢ Validation Loss: 0.3904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3904, best: 0.3888)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1094
  ‚Ä¢ Validation Loss: 0.3894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3894, best: 0.3888)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1472
  ‚Ä¢ Validation Loss: 0.3890
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3890, best: 0.3888)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0833
  ‚Ä¢ Validation Loss: 0.3894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3894, best: 0.3888)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1578
  ‚Ä¢ Validation Loss: 0.3904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3904, best: 0.3888)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1485
  ‚Ä¢ Validation Loss: 0.3888
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3888
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1597
  ‚Ä¢ Validation Loss: 0.3903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3903, best: 0.3888)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1301
  ‚Ä¢ Validation Loss: 0.3910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3910, best: 0.3888)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3965, best: 0.3888)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0962
  ‚Ä¢ Validation Loss: 0.3969
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3969, best: 0.3888)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0928
  ‚Ä¢ Validation Loss: 0.3994
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3994, best: 0.3888)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0372
  ‚Ä¢ Validation Loss: 0.4003
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4003, best: 0.3888)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0723
  ‚Ä¢ Validation Loss: 0.3910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3910, best: 0.3888)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4028, best: 0.3888)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0394
  ‚Ä¢ Validation Loss: 0.4023
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4023, best: 0.3888)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0686
  ‚Ä¢ Validation Loss: 0.3978
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3978, best: 0.3888)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0577
  ‚Ä¢ Validation Loss: 0.3962
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3962, best: 0.3888)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0761
  ‚Ä¢ Validation Loss: 0.4027
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4027, best: 0.3888)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0139
  ‚Ä¢ Validation Loss: 0.4040
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4040, best: 0.3888)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0535
  ‚Ä¢ Validation Loss: 0.3967
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3967, best: 0.3888)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0670
  ‚Ä¢ Validation Loss: 0.3998
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3998, best: 0.3888)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0940
  ‚Ä¢ Validation Loss: 0.3953
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3953, best: 0.3888)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0242
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4084, best: 0.3888)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0343
  ‚Ä¢ Validation Loss: 0.3940
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3940, best: 0.3888)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0512
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4046, best: 0.3888)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0538
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4036, best: 0.3888)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0773
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4025, best: 0.3888)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0909
  ‚Ä¢ Validation Loss: 0.3933
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3933, best: 0.3888)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0977
  ‚Ä¢ Validation Loss: 0.3942
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3942, best: 0.3888)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0981
  ‚Ä¢ Validation Loss: 0.3941
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3941, best: 0.3888)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4008, best: 0.3888)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.3923
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3923, best: 0.3888)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1016
  ‚Ä¢ Validation Loss: 0.3934
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3934, best: 0.3888)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0349
  ‚Ä¢ Validation Loss: 0.4019
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4019, best: 0.3888)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0185
  ‚Ä¢ Validation Loss: 0.4026
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4026, best: 0.3888)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0700
  ‚Ä¢ Validation Loss: 0.3976
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3976, best: 0.3888)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1104
  ‚Ä¢ Validation Loss: 0.3957
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3957, best: 0.3888)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0895
  ‚Ä¢ Validation Loss: 0.3930
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3930, best: 0.3888)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1115
  ‚Ä¢ Validation Loss: 0.3913
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3913, best: 0.3888)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.3915
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3915, best: 0.3888)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0827
  ‚Ä¢ Validation Loss: 0.3916
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3916, best: 0.3888)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1011
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4048, best: 0.3888)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0479
  ‚Ä¢ Validation Loss: 0.3941
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3941, best: 0.3888)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0820
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3885
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0788
  ‚Ä¢ Validation Loss: 0.3911
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3911, best: 0.3885)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.3910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3910, best: 0.3885)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0886
  ‚Ä¢ Validation Loss: 0.3898
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3898, best: 0.3885)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0778
  ‚Ä¢ Validation Loss: 0.3887
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3887, best: 0.3885)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0658
  ‚Ä¢ Validation Loss: 0.3899
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3899, best: 0.3885)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0828
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3937, best: 0.3885)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0675
  ‚Ä¢ Validation Loss: 0.3960
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3960, best: 0.3885)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0957
  ‚Ä¢ Validation Loss: 0.3916
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3916, best: 0.3885)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1018
  ‚Ä¢ Validation Loss: 0.3970
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3970, best: 0.3885)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0994
  ‚Ä¢ Validation Loss: 0.3962
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3962, best: 0.3885)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1198
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3885, best: 0.3885)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1041
  ‚Ä¢ Validation Loss: 0.3896
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3896, best: 0.3885)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1202
  ‚Ä¢ Validation Loss: 0.3909
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3909, best: 0.3885)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0675
  ‚Ä¢ Validation Loss: 0.3954
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.3954, best: 0.3885)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0666
  ‚Ä¢ Validation Loss: 0.3953
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3953, best: 0.3885)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1070
  ‚Ä¢ Validation Loss: 0.3946
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3946, best: 0.3885)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1105
  ‚Ä¢ Validation Loss: 0.3901
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3901, best: 0.3885)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0817
  ‚Ä¢ Validation Loss: 0.3913
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3913, best: 0.3885)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0787
  ‚Ä¢ Validation Loss: 0.3908
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3908, best: 0.3885)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0376
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4047, best: 0.3885)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0524
  ‚Ä¢ Validation Loss: 0.3998
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3998, best: 0.3885)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0712
  ‚Ä¢ Validation Loss: 0.3988
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3988, best: 0.3885)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0769
  ‚Ä¢ Validation Loss: 0.3944
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3944, best: 0.3885)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0926
  ‚Ä¢ Validation Loss: 0.3888
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3888, best: 0.3885)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0749
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3965, best: 0.3885)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0651
  ‚Ä¢ Validation Loss: 0.3912
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3912, best: 0.3885)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0869
  ‚Ä¢ Validation Loss: 0.3861
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3861
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1313
  ‚Ä¢ Validation Loss: 0.3860
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3860
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1034
  ‚Ä¢ Validation Loss: 0.3910
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3910, best: 0.3860)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0961
  ‚Ä¢ Validation Loss: 0.3920
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3920, best: 0.3860)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0755
  ‚Ä¢ Validation Loss: 0.3842
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3842
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0577
  ‚Ä¢ Validation Loss: 0.3872
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3872, best: 0.3842)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1337
  ‚Ä¢ Validation Loss: 0.3855
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3855, best: 0.3842)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0757
  ‚Ä¢ Validation Loss: 0.3883
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3883, best: 0.3842)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1264
  ‚Ä¢ Validation Loss: 0.3862
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3862, best: 0.3842)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.3851
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3851, best: 0.3842)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1159
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3885, best: 0.3842)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0699
  ‚Ä¢ Validation Loss: 0.3860
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3860, best: 0.3842)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0859
  ‚Ä¢ Validation Loss: 0.3928
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3928, best: 0.3842)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1088
  ‚Ä¢ Validation Loss: 0.3869
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3869, best: 0.3842)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1103
  ‚Ä¢ Validation Loss: 0.3866
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3866, best: 0.3842)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0867
  ‚Ä¢ Validation Loss: 0.3893
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3893, best: 0.3842)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1086
  ‚Ä¢ Validation Loss: 0.3880
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3880, best: 0.3842)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1022
  ‚Ä¢ Validation Loss: 0.3843
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3843, best: 0.3842)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0915
  ‚Ä¢ Validation Loss: 0.3827
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3827
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1067
  ‚Ä¢ Validation Loss: 0.3849
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3849, best: 0.3827)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0934
  ‚Ä¢ Validation Loss: 0.3828
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3828, best: 0.3827)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0755
  ‚Ä¢ Validation Loss: 0.3900
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3900, best: 0.3827)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.3840
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3840, best: 0.3827)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1072
  ‚Ä¢ Validation Loss: 0.3836
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3836, best: 0.3827)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1129
  ‚Ä¢ Validation Loss: 0.3820
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1108
  ‚Ä¢ Validation Loss: 0.3815
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3815
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1397
  ‚Ä¢ Validation Loss: 0.3854
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3854, best: 0.3815)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1083
  ‚Ä¢ Validation Loss: 0.3848
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3848, best: 0.3815)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0690
  ‚Ä¢ Validation Loss: 0.3809
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3809
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0512
  ‚Ä¢ Validation Loss: 0.3806
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3806
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0157
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3814, best: 0.3806)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0272
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3821, best: 0.3806)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0702
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3810, best: 0.3806)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0451
  ‚Ä¢ Validation Loss: 0.3839
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3839, best: 0.3806)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0408
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3831, best: 0.3806)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0615
  ‚Ä¢ Validation Loss: 0.3811
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3811, best: 0.3806)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0643
  ‚Ä¢ Validation Loss: 0.3826
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3826, best: 0.3806)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0620
  ‚Ä¢ Validation Loss: 0.3795
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0665
  ‚Ä¢ Validation Loss: 0.3827
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3827, best: 0.3795)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0407
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3830, best: 0.3795)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0612
  ‚Ä¢ Validation Loss: 0.3819
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3819, best: 0.3795)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0364
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3814, best: 0.3795)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0415
  ‚Ä¢ Validation Loss: 0.3783
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3783
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0364
  ‚Ä¢ Validation Loss: 0.3820
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3820, best: 0.3783)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0335
  ‚Ä¢ Validation Loss: 0.3819
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3819, best: 0.3783)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0500
  ‚Ä¢ Validation Loss: 0.3840
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3840, best: 0.3783)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0561
  ‚Ä¢ Validation Loss: 0.3825
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3825, best: 0.3783)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.3819
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3819, best: 0.3783)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0503
  ‚Ä¢ Validation Loss: 0.3836
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3836, best: 0.3783)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0268
  ‚Ä¢ Validation Loss: 0.3813
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3813, best: 0.3783)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.3798
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3798, best: 0.3783)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0371
  ‚Ä¢ Validation Loss: 0.3801
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3801, best: 0.3783)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0520
  ‚Ä¢ Validation Loss: 0.3815
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3815, best: 0.3783)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0387
  ‚Ä¢ Validation Loss: 0.3824
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3824, best: 0.3783)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0295
  ‚Ä¢ Validation Loss: 0.3820
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3820, best: 0.3783)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0521
  ‚Ä¢ Validation Loss: 0.3805
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3805, best: 0.3783)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0416
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3814, best: 0.3783)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0456
  ‚Ä¢ Validation Loss: 0.3797
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3797, best: 0.3783)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0341
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3800, best: 0.3783)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0420
  ‚Ä¢ Validation Loss: 0.3805
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3805, best: 0.3783)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0334
  ‚Ä¢ Validation Loss: 0.3820
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3820, best: 0.3783)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0520
  ‚Ä¢ Validation Loss: 0.3837
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3837, best: 0.3783)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0314
  ‚Ä¢ Validation Loss: 0.3823
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3823, best: 0.3783)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0211
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3810, best: 0.3783)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0497
  ‚Ä¢ Validation Loss: 0.3799
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3799, best: 0.3783)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0590
  ‚Ä¢ Validation Loss: 0.3792
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3792, best: 0.3783)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0319
  ‚Ä¢ Validation Loss: 0.3816
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3816, best: 0.3783)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0347
  ‚Ä¢ Validation Loss: 0.3812
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3812, best: 0.3783)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0191
  ‚Ä¢ Validation Loss: 0.3816
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3816, best: 0.3783)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0390
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3800, best: 0.3783)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0366
  ‚Ä¢ Validation Loss: 0.3794
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3794, best: 0.3783)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0409
  ‚Ä¢ Validation Loss: 0.3783
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3783, best: 0.3783)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0603
  ‚Ä¢ Validation Loss: 0.3774
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3774
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0501
  ‚Ä¢ Validation Loss: 0.3797
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3797, best: 0.3774)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0497
  ‚Ä¢ Validation Loss: 0.3789
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3789, best: 0.3774)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0496
  ‚Ä¢ Validation Loss: 0.3796
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3796, best: 0.3774)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0317
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3821, best: 0.3774)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0366
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3810, best: 0.3774)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0390
  ‚Ä¢ Validation Loss: 0.3805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3805, best: 0.3774)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0622
  ‚Ä¢ Validation Loss: 0.3791
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3791, best: 0.3774)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0453
  ‚Ä¢ Validation Loss: 0.3797
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3797, best: 0.3774)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0428
  ‚Ä¢ Validation Loss: 0.3801
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3801, best: 0.3774)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0458
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3800, best: 0.3774)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0428
  ‚Ä¢ Validation Loss: 0.3807
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3807, best: 0.3774)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0410
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3790, best: 0.3774)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0406
  ‚Ä¢ Validation Loss: 0.3789
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3789, best: 0.3774)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0381
  ‚Ä¢ Validation Loss: 0.3796
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3796, best: 0.3774)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0572
  ‚Ä¢ Validation Loss: 0.3795
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3795, best: 0.3774)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3774
Total Epochs:   300
Models Saved:   ./Result/a1/Latin16746
TensorBoard:    ./Result/a1/Latin16746/tensorboard_logs
================================================================================

[06:04:26] Training completed. Best val loss: 0.3774

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL (ResNet-50): Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9879, Recall=0.9892, F1=0.9886, IoU=0.9774
Paratext            : Precision=0.7541, Recall=0.8103, F1=0.7812, IoU=0.6410
Decoration          : Precision=0.9743, Recall=0.9163, F1=0.9444, IoU=0.8947
Main Text           : Precision=0.8918, Recall=0.9158, F1=0.9036, IoU=0.8242
Title               : Precision=0.7445, Recall=0.7591, F1=0.7518, IoU=0.6023
Chapter Headings    : Precision=0.9011, Recall=0.7343, F1=0.8092, IoU=0.6796

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8756
Mean Recall:    0.8542
Mean F1-Score:  0.8631
Mean IoU:       0.7699
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL (ResNet-50): Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
Output Directory: ./Result/a1/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,431
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0533
  ‚Ä¢ Validation Loss: 0.7242
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7242
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7389
  ‚Ä¢ Validation Loss: 0.6550
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6550
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6759
  ‚Ä¢ Validation Loss: 0.5789
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5789
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5009
  ‚Ä¢ Validation Loss: 0.5617
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5617
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4441
  ‚Ä¢ Validation Loss: 0.5365
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5365
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3955
  ‚Ä¢ Validation Loss: 0.5290
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5290
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3561
  ‚Ä¢ Validation Loss: 0.5323
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5323, best: 0.5290)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3303
  ‚Ä¢ Validation Loss: 0.5254
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5254
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4292
  ‚Ä¢ Validation Loss: 0.5138
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5138
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3527
  ‚Ä¢ Validation Loss: 0.5119
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5119
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3502
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5152, best: 0.5119)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3396
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3754
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5020
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3445
  ‚Ä¢ Validation Loss: 0.4983
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4983
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4980
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3342
  ‚Ä¢ Validation Loss: 0.5000
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5000, best: 0.4980)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3708
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3628
  ‚Ä¢ Validation Loss: 0.4939
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4939
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3816
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4837
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3956
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4974, best: 0.4837)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3663
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4891, best: 0.4837)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3903
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4911, best: 0.4837)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3288
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4890, best: 0.4837)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3873
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4751
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3702
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4793, best: 0.4751)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3618
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4775, best: 0.4751)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3555
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4765, best: 0.4751)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3644
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4742
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3574
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3772
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4700
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3739
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4717, best: 0.4700)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3962
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4749, best: 0.4700)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3731
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4728, best: 0.4700)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3609
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4726, best: 0.4700)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3762
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4704, best: 0.4700)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3486
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4682
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3346
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4722, best: 0.4682)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3372
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4638
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3072
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4715, best: 0.4638)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3434
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4621
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3419
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4659, best: 0.4621)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3497
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4660, best: 0.4621)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3404
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4629, best: 0.4621)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3317
  ‚Ä¢ Validation Loss: 0.4635
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4635, best: 0.4621)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3241
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4639, best: 0.4621)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3450
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4623, best: 0.4621)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3199
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4634, best: 0.4621)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3246
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4629, best: 0.4621)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3428
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4636, best: 0.4621)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3334
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4626, best: 0.4621)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3350
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4733, best: 0.4621)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4702, best: 0.4621)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3174
  ‚Ä¢ Validation Loss: 0.4666
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4666, best: 0.4621)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2795
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4663, best: 0.4621)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2533
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4653, best: 0.4621)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3079
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4779, best: 0.4621)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2275
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4729, best: 0.4621)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2698
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4639, best: 0.4621)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2550
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4653, best: 0.4621)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2282
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4599
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2410
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4657, best: 0.4599)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2388
  ‚Ä¢ Validation Loss: 0.4542
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4542
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2348
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4647, best: 0.4542)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2756
  ‚Ä¢ Validation Loss: 0.4553
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4553, best: 0.4542)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2704
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4537
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2726
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4667, best: 0.4537)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2618
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4570, best: 0.4537)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2421
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4571, best: 0.4537)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2672
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4523
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2171
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4526, best: 0.4523)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2461
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4511
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2305
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4565, best: 0.4511)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2499
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4569, best: 0.4511)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2292
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4589, best: 0.4511)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2086
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4566, best: 0.4511)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.4555
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4555, best: 0.4511)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1836
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4430
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2895
  ‚Ä¢ Validation Loss: 0.4433
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4433, best: 0.4430)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2770
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4426
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2588
  ‚Ä¢ Validation Loss: 0.4459
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4459, best: 0.4426)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2158
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4458, best: 0.4426)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2602
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4447, best: 0.4426)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.4457
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4457, best: 0.4426)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2623
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4485, best: 0.4426)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2337
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4489, best: 0.4426)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2597
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4506, best: 0.4426)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2131
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4462, best: 0.4426)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2650
  ‚Ä¢ Validation Loss: 0.4393
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4393
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2280
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4492, best: 0.4393)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2677
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4485, best: 0.4393)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2713
  ‚Ä¢ Validation Loss: 0.4409
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4409, best: 0.4393)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2233
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4426, best: 0.4393)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2389
  ‚Ä¢ Validation Loss: 0.4518
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4518, best: 0.4393)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2258
  ‚Ä¢ Validation Loss: 0.4388
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4388
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2393
  ‚Ä¢ Validation Loss: 0.4406
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4406, best: 0.4388)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2371
  ‚Ä¢ Validation Loss: 0.4453
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4453, best: 0.4388)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2669
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4441, best: 0.4388)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1986
  ‚Ä¢ Validation Loss: 0.4420
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4420, best: 0.4388)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2036
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4351
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2042
  ‚Ä¢ Validation Loss: 0.4391
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4391, best: 0.4351)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2281
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4373, best: 0.4351)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4378
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4378, best: 0.4351)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2569
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4365, best: 0.4351)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2227
  ‚Ä¢ Validation Loss: 0.4378
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4378, best: 0.4351)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1990
  ‚Ä¢ Validation Loss: 0.4388
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4388, best: 0.4351)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2231
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4419, best: 0.4338)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1859
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4348, best: 0.4338)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2086
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4340, best: 0.4338)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2301
  ‚Ä¢ Validation Loss: 0.4387
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4387, best: 0.4338)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2142
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4363, best: 0.4338)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2216
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4326
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2487
  ‚Ä¢ Validation Loss: 0.4383
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4383, best: 0.4326)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2122
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4365, best: 0.4326)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2247
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4358, best: 0.4326)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2168
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4363, best: 0.4326)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2649
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4361, best: 0.4326)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0984
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4354, best: 0.4326)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1359
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4327, best: 0.4326)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1386
  ‚Ä¢ Validation Loss: 0.4350
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4350, best: 0.4326)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1124
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4368, best: 0.4326)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0960
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4341, best: 0.4326)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1623
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4318
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1041
  ‚Ä¢ Validation Loss: 0.4329
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4329, best: 0.4318)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4338, best: 0.4318)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1177
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4353, best: 0.4318)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1503
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4333, best: 0.4318)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1120
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4352, best: 0.4318)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0928
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4318
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0918
  ‚Ä¢ Validation Loss: 0.4304
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4304
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1334
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4351, best: 0.4304)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1057
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4323, best: 0.4304)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1193
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4337, best: 0.4304)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1182
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4351, best: 0.4304)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1013
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4341, best: 0.4304)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0891
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4363, best: 0.4304)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0962
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4365, best: 0.4304)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4359, best: 0.4304)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4346, best: 0.4304)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0828
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4341, best: 0.4304)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0943
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4340, best: 0.4304)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1616
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4331, best: 0.4304)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0889
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4355, best: 0.4304)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4330, best: 0.4304)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1211
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4359, best: 0.4304)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1186
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4333, best: 0.4304)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1389
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4328, best: 0.4304)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1206
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4325, best: 0.4304)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1156
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4323, best: 0.4304)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1391
  ‚Ä¢ Validation Loss: 0.4335
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4335, best: 0.4304)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0770
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4422, best: 0.4304)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.4383
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4383, best: 0.4304)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0873
  ‚Ä¢ Validation Loss: 0.4382
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4382, best: 0.4304)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0855
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4373, best: 0.4304)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1162
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4355, best: 0.4304)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1139
  ‚Ä¢ Validation Loss: 0.4342
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4342, best: 0.4304)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0694
  ‚Ä¢ Validation Loss: 0.4411
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4411, best: 0.4304)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1035
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4414, best: 0.4304)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1213
  ‚Ä¢ Validation Loss: 0.4413
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4413, best: 0.4304)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.4342
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4342, best: 0.4304)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1281
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4379, best: 0.4304)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4338, best: 0.4304)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1096
  ‚Ä¢ Validation Loss: 0.4417
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4417, best: 0.4304)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0997
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4463, best: 0.4304)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1586
  ‚Ä¢ Validation Loss: 0.4382
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4382, best: 0.4304)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1130
  ‚Ä¢ Validation Loss: 0.4366
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4366, best: 0.4304)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4387
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4387, best: 0.4304)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0997
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4362, best: 0.4304)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1344
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4345, best: 0.4304)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0977
  ‚Ä¢ Validation Loss: 0.4393
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4393, best: 0.4304)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1274
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4389, best: 0.4304)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1110
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4293
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1046
  ‚Ä¢ Validation Loss: 0.4401
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4401, best: 0.4293)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1344
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4371, best: 0.4293)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0996
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4368, best: 0.4293)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1203
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4339, best: 0.4293)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0925
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4384, best: 0.4293)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1229
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4323, best: 0.4293)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1076
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4371, best: 0.4293)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1234
  ‚Ä¢ Validation Loss: 0.4381
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4381, best: 0.4293)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0620
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4340, best: 0.4293)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1136
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4426, best: 0.4293)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0790
  ‚Ä¢ Validation Loss: 0.4381
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4381, best: 0.4293)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1540
  ‚Ä¢ Validation Loss: 0.4405
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4405, best: 0.4293)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1052
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4285
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1087
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4318, best: 0.4285)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0981
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4480, best: 0.4285)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0417
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4386, best: 0.4285)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1672
  ‚Ä¢ Validation Loss: 0.4306
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4306, best: 0.4285)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1032
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4313, best: 0.4285)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0999
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4357, best: 0.4285)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1078
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4326, best: 0.4285)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1095
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4352, best: 0.4285)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0779
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4333, best: 0.4285)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1343
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4379, best: 0.4285)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1155
  ‚Ä¢ Validation Loss: 0.4360
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4360, best: 0.4285)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1227
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4294, best: 0.4285)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0865
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4313, best: 0.4285)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4399, best: 0.4285)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1586
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4343, best: 0.4285)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1082
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4356, best: 0.4285)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0747
  ‚Ä¢ Validation Loss: 0.4381
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4381, best: 0.4285)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0915
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4321, best: 0.4285)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0618
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1062
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4310, best: 0.4277)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0867
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4309, best: 0.4277)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1007
  ‚Ä¢ Validation Loss: 0.4262
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4262
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0576
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4363, best: 0.4262)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0774
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4361, best: 0.4262)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4310, best: 0.4262)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0866
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4301, best: 0.4262)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1213
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4340, best: 0.4262)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0989
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4300, best: 0.4262)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1003
  ‚Ä¢ Validation Loss: 0.4366
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4366, best: 0.4262)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1218
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4311, best: 0.4262)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1432
  ‚Ä¢ Validation Loss: 0.4303
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4303, best: 0.4262)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1288
  ‚Ä¢ Validation Loss: 0.4334
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4334, best: 0.4262)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1291
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4284, best: 0.4262)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1465
  ‚Ä¢ Validation Loss: 0.4268
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4268, best: 0.4262)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1116
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4295, best: 0.4262)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0584
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4288, best: 0.4262)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1060
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4283, best: 0.4262)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1296
  ‚Ä¢ Validation Loss: 0.4314
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4314, best: 0.4262)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1476
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4325, best: 0.4262)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1350
  ‚Ä¢ Validation Loss: 0.4342
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4342, best: 0.4262)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1171
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4331, best: 0.4262)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1196
  ‚Ä¢ Validation Loss: 0.4307
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4307, best: 0.4262)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1123
  ‚Ä¢ Validation Loss: 0.4349
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4349, best: 0.4262)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1314
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4277, best: 0.4262)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1556
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4296, best: 0.4262)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1150
  ‚Ä¢ Validation Loss: 0.4278
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4278, best: 0.4262)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1481
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4245
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.4255
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4255, best: 0.4245)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1338
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4313, best: 0.4245)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0986
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4319, best: 0.4245)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0723
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4248, best: 0.4245)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0285
  ‚Ä¢ Validation Loss: 0.4282
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4282, best: 0.4245)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0137
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4308, best: 0.4245)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0449
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4325, best: 0.4245)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0175
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4290, best: 0.4245)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0184
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4277, best: 0.4245)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4274, best: 0.4245)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0351
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4285, best: 0.4245)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0432
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4361, best: 0.4245)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0323
  ‚Ä¢ Validation Loss: 0.4382
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4382, best: 0.4245)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0183
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4337, best: 0.4245)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0233
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4269, best: 0.4245)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0054
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4298, best: 0.4245)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0133
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4267, best: 0.4245)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0293
  ‚Ä¢ Validation Loss: 0.4314
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4314, best: 0.4245)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0134
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4325, best: 0.4245)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0261
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4327, best: 0.4245)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0345
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4286, best: 0.4245)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0382
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4263, best: 0.4245)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0117
  ‚Ä¢ Validation Loss: 0.4279
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4279, best: 0.4245)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0201
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4283, best: 0.4245)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0380
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4299, best: 0.4245)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0056
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4309, best: 0.4245)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0297
  ‚Ä¢ Validation Loss: 0.4282
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4282, best: 0.4245)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0287
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4283, best: 0.4245)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0243
  ‚Ä¢ Validation Loss: 0.4281
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4281, best: 0.4245)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0143
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4277, best: 0.4245)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0334
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4309, best: 0.4245)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0392
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4285, best: 0.4245)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0189
  ‚Ä¢ Validation Loss: 0.4278
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4278, best: 0.4245)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0123
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4298, best: 0.4245)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0153
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4285, best: 0.4245)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0260
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4290, best: 0.4245)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0218
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4295, best: 0.4245)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0176
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4284, best: 0.4245)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0352
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4284, best: 0.4245)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0166
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4286, best: 0.4245)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4284, best: 0.4245)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0109
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4280, best: 0.4245)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0059
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4296, best: 0.4245)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0181
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4265, best: 0.4245)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0382
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4264, best: 0.4245)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0378
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4274, best: 0.4245)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0240
  ‚Ä¢ Validation Loss: 0.4287
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4287, best: 0.4245)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0279
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4267, best: 0.4245)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0247
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4295, best: 0.4245)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0165
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4286, best: 0.4245)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0108
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4285, best: 0.4245)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0083
  ‚Ä¢ Validation Loss: 0.4282
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4282, best: 0.4245)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0242
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4289, best: 0.4245)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0326
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4290, best: 0.4245)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0107
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4286, best: 0.4245)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0263
  ‚Ä¢ Validation Loss: 0.4282
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4282, best: 0.4245)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0318
  ‚Ä¢ Validation Loss: 0.4275
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4275, best: 0.4245)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0500
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4271, best: 0.4245)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0287
  ‚Ä¢ Validation Loss: 0.4251
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4251, best: 0.4245)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0472
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4274, best: 0.4245)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0395
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4260, best: 0.4245)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0176
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4276, best: 0.4245)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0179
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4266, best: 0.4245)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0282
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4263, best: 0.4245)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0112
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4252, best: 0.4245)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0302
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4270, best: 0.4245)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0288
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4242
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0323
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4247, best: 0.4242)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4242
Total Epochs:   300
Models Saved:   ./Result/a1/Syr341
TensorBoard:    ./Result/a1/Syr341/tensorboard_logs
================================================================================

[07:11:00] Training completed. Best val loss: 0.4242

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL (ResNet-50): Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì ResNet-50 Encoder (official)
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: ResNet-50 (official)
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9656, Recall=0.9807, F1=0.9731, IoU=0.9476
Paratext            : Precision=0.5306, Recall=0.4324, F1=0.4765, IoU=0.3128
Decoration          : Precision=0.9755, Recall=0.5659, F1=0.7162, IoU=0.5579
Main Text           : Precision=0.8523, Recall=0.8373, F1=0.8448, IoU=0.7312
Title               : Precision=0.3952, Recall=0.2216, F1=0.2839, IoU=0.1655

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7438
Mean Recall:    0.6076
Mean F1-Score:  0.6589
Mean IoU:       0.5430
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8215
Mean Recall:    0.7535
Mean F1-Score:  0.7803
Mean IoU:       0.6734
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL WITH RESNET-50 ENCODER
Results Location: ./Result/a1/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Tue Nov 18 07:14:18 AM CET 2025
= Job-ID             : 1392370 on tinygpu
= Job-Name           : 1st
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 04:34:22
= Total RAM usage    : 10.5 GiB of requested  GiB (%)   
= Node list          : tg069
= Subm/Elig/Start/End: 2025-11-17T23:02:31 / 2025-11-17T23:02:31 / 2025-11-18T02:39:40 / 2025-11-18T07:14:02
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              67.8G   104.9G   209.7G        N/A     236K     500K   1,000K        N/A    
    /home/woody             0.0K  1000.0G  1500.0G        N/A       1    5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 403776, 60 %, 40 %, 4406 MiB, 3963565 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 592363, 17 %, 8 %, 834 MiB, 188578 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 601122, 60 %, 39 %, 4406 MiB, 3939038 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 761638, 17 %, 8 %, 834 MiB, 186083 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 768924, 60 %, 40 %, 4406 MiB, 3913366 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 870729, 18 %, 8 %, 834 MiB, 183118 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 874557, 63 %, 41 %, 4404 MiB, 3787696 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:18:00.0, 950138, 17 %, 8 %, 834 MiB, 186177 ms
