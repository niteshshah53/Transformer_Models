### Starting TaskPrologue of job 1327366 on tg069 at Sat Nov 15 07:55:16 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Sat Nov 15 19:55:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:AF:00.0 Off |                  N/A |
| 28%   28C    P8             13W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE NETWORK MODEL
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple (concatenation)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úó Deep Supervision: disabled (base model)
  ‚úó Multi-Scale Aggregation: disabled (base model)
  ‚úó Fourier Feature Fusion: disabled (using simple fusion)
  ‚úó Smart Skip Connections: disabled (using simple fusion)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Loss functions created: CE (weighted), Focal (Œ≥=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.5056
  ‚Ä¢ Validation Loss: 0.7970
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7970
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7610
  ‚Ä¢ Validation Loss: 0.7176
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7176
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6875
  ‚Ä¢ Validation Loss: 0.6795
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6566
  ‚Ä¢ Validation Loss: 0.6333
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6333
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6282
  ‚Ä¢ Validation Loss: 0.5964
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5964
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6096
  ‚Ä¢ Validation Loss: 0.5759
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5759
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5841
  ‚Ä¢ Validation Loss: 0.5736
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5736
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5709
  ‚Ä¢ Validation Loss: 0.5502
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5502
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5568
  ‚Ä¢ Validation Loss: 0.5616
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5616, best: 0.5502)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5412
  ‚Ä¢ Validation Loss: 0.5447
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5447
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5349
  ‚Ä¢ Validation Loss: 0.5361
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5361
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5333
  ‚Ä¢ Validation Loss: 0.5320
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5320
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5243
  ‚Ä¢ Validation Loss: 0.5234
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5234
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5018
  ‚Ä¢ Validation Loss: 0.5177
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5065
  ‚Ä¢ Validation Loss: 0.5190
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5190, best: 0.5177)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5042
  ‚Ä¢ Validation Loss: 0.5118
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4809
  ‚Ä¢ Validation Loss: 0.5118
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4849
  ‚Ä¢ Validation Loss: 0.5093
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5093
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4482
  ‚Ä¢ Validation Loss: 0.5073
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5073
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4734
  ‚Ä¢ Validation Loss: 0.5042
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5042
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4629
  ‚Ä¢ Validation Loss: 0.5055
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5055, best: 0.5042)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4720
  ‚Ä¢ Validation Loss: 0.5018
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5018
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4541
  ‚Ä¢ Validation Loss: 0.4958
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4958
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4533
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4928
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4511
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4915
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4378
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4833
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4368
  ‚Ä¢ Validation Loss: 0.4858
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4858, best: 0.4833)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4300
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4815
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4438
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4772
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4275
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4789, best: 0.4741)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4737
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4233
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4758, best: 0.4737)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4193
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4168
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4732, best: 0.4709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4055
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4695
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4164
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4139
  ‚Ä¢ Validation Loss: 0.4666
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4666
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4066
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4647
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4063
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4663, best: 0.4647)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4094
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4639
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4026
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4627
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4026
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4630, best: 0.4627)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4013
  ‚Ä¢ Validation Loss: 0.4619
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4619
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4021
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4627, best: 0.4619)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4073
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4629, best: 0.4619)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4073
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4623, best: 0.4619)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4153
  ‚Ä¢ Validation Loss: 0.4620
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4620, best: 0.4619)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4019
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4622, best: 0.4619)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4001
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4622, best: 0.4619)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4140
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4709, best: 0.4619)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4157
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4597
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4150
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4705, best: 0.4597)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4009
  ‚Ä¢ Validation Loss: 0.4558
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4025
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4577, best: 0.4558)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4047
  ‚Ä¢ Validation Loss: 0.4536
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4536
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3985
  ‚Ä¢ Validation Loss: 0.4528
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4528
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3876
  ‚Ä¢ Validation Loss: 0.4572
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4572, best: 0.4528)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4029
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4467
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3789
  ‚Ä¢ Validation Loss: 0.4457
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4457
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3857
  ‚Ä¢ Validation Loss: 0.4451
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4451
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3860
  ‚Ä¢ Validation Loss: 0.4449
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4449
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3731
  ‚Ä¢ Validation Loss: 0.4412
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4412
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3838
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4419, best: 0.4412)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3975
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3795
  ‚Ä¢ Validation Loss: 0.4407
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4407, best: 0.4373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3778
  ‚Ä¢ Validation Loss: 0.4407
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4407, best: 0.4373)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3772
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4339
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3719
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4374, best: 0.4339)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3627
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4319
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3650
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4340, best: 0.4319)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3684
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4285
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3607
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4302, best: 0.4285)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3436
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4339, best: 0.4285)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3629
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4358, best: 0.4285)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3618
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4319, best: 0.4285)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3596
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4308, best: 0.4285)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3589
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4319, best: 0.4285)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3379
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4239
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3627
  ‚Ä¢ Validation Loss: 0.4255
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4255, best: 0.4239)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3559
  ‚Ä¢ Validation Loss: 0.4279
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4279, best: 0.4239)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3518
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3394
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4204, best: 0.4202)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3360
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4207, best: 0.4202)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3426
  ‚Ä¢ Validation Loss: 0.4208
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4208, best: 0.4202)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3463
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4244, best: 0.4202)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3130
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4189
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3182
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4172
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3501
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4170
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3500
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4271, best: 0.4170)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3423
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4185, best: 0.4170)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3485
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4221, best: 0.4170)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 93/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3366
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4148
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3393
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4185, best: 0.4148)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3303
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4153, best: 0.4148)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3321
  ‚Ä¢ Validation Loss: 0.4123
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4123
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3319
  ‚Ä¢ Validation Loss: 0.4118
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3167
  ‚Ä¢ Validation Loss: 0.4096
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4096
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3247
  ‚Ä¢ Validation Loss: 0.4174
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4174, best: 0.4096)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3283
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4107, best: 0.4096)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 101/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3214
  ‚Ä¢ Validation Loss: 0.4109
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4109, best: 0.4096)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3362
  ‚Ä¢ Validation Loss: 0.4099
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4099, best: 0.4096)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 103/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3298
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4117, best: 0.4096)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3034
  ‚Ä¢ Validation Loss: 0.4085
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4085
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3202
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4102, best: 0.4085)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 106/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3265
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4055
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3231
  ‚Ä¢ Validation Loss: 0.4124
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4124, best: 0.4055)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3312
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4113, best: 0.4055)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3198
  ‚Ä¢ Validation Loss: 0.4098
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4098, best: 0.4055)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3285
  ‚Ä¢ Validation Loss: 0.4082
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4082, best: 0.4055)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3105
  ‚Ä¢ Validation Loss: 0.4064
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4064, best: 0.4055)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3210
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4135, best: 0.4055)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 113/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3198
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4065, best: 0.4055)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 114/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3281
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4066, best: 0.4055)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 115/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3159
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4071, best: 0.4055)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 116/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3056
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4038
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3197
  ‚Ä¢ Validation Loss: 0.4037
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4037
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3067
  ‚Ä¢ Validation Loss: 0.4062
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4062, best: 0.4037)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2935
  ‚Ä¢ Validation Loss: 0.4042
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4042, best: 0.4037)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2969
  ‚Ä¢ Validation Loss: 0.4017
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4017
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2260
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4035, best: 0.4017)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2699
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4045, best: 0.4017)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2742
  ‚Ä¢ Validation Loss: 0.4056
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4056, best: 0.4017)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2604
  ‚Ä¢ Validation Loss: 0.4026
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4026, best: 0.4017)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2725
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4035, best: 0.4017)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 126/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3217
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4032, best: 0.4017)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2642
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4011
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2850
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4028, best: 0.4011)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2929
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4032, best: 0.4011)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2734
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4025, best: 0.4011)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2594
  ‚Ä¢ Validation Loss: 0.4020
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4020, best: 0.4011)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 132/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3030
  ‚Ä¢ Validation Loss: 0.4029
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4029, best: 0.4011)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2148
  ‚Ä¢ Validation Loss: 0.4017
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4017, best: 0.4011)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 134/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3021
  ‚Ä¢ Validation Loss: 0.4012
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4012, best: 0.4011)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2841
  ‚Ä¢ Validation Loss: 0.4002
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4002
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.4015
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4015, best: 0.4002)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2139
  ‚Ä¢ Validation Loss: 0.4021
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4021, best: 0.4002)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2855
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4011, best: 0.4002)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 139/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3098
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4011, best: 0.4002)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2748
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4008, best: 0.4002)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2860
  ‚Ä¢ Validation Loss: 0.4006
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4006, best: 0.4002)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2860
  ‚Ä¢ Validation Loss: 0.4006
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4006, best: 0.4002)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 143/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3058
  ‚Ä¢ Validation Loss: 0.4007
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4007, best: 0.4002)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2452
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4011, best: 0.4002)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2198
  ‚Ä¢ Validation Loss: 0.4014
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4014, best: 0.4002)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 146/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3000
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4008, best: 0.4002)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2174
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4011, best: 0.4002)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2932
  ‚Ä¢ Validation Loss: 0.4007
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4007, best: 0.4002)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 149/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3007
  ‚Ä¢ Validation Loss: 0.4013
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4013, best: 0.4002)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2905
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4011, best: 0.4002)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2122
  ‚Ä¢ Validation Loss: 0.4053
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4053, best: 0.4002)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3063
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4045, best: 0.4002)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2635
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4158, best: 0.4002)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2580
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4052, best: 0.4002)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4059, best: 0.4002)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2523
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4169, best: 0.4002)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2804
  ‚Ä¢ Validation Loss: 0.4159
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4159, best: 0.4002)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2218
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4073, best: 0.4002)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2634
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4071, best: 0.4002)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4041, best: 0.4002)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2178
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3992
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 162/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3174
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4046, best: 0.3992)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2893
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4048, best: 0.3992)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2843
  ‚Ä¢ Validation Loss: 0.4089
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4089, best: 0.3992)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1780
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4041, best: 0.3992)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2860
  ‚Ä¢ Validation Loss: 0.4007
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4007, best: 0.3992)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2540
  ‚Ä¢ Validation Loss: 0.4030
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4030, best: 0.3992)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2997
  ‚Ä¢ Validation Loss: 0.4013
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4013, best: 0.3992)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4086
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4086, best: 0.3992)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2497
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4046, best: 0.3992)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3042
  ‚Ä¢ Validation Loss: 0.4092
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4092, best: 0.3992)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1940
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4008, best: 0.3992)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2739
  ‚Ä¢ Validation Loss: 0.3987
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3987
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1937
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3992, best: 0.3987)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 175/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3033
  ‚Ä¢ Validation Loss: 0.4022
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4022, best: 0.3987)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2941
  ‚Ä¢ Validation Loss: 0.4053
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4053, best: 0.3987)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2670
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3958
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2715
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4147, best: 0.3958)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2150
  ‚Ä¢ Validation Loss: 0.4064
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4064, best: 0.3958)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2109
  ‚Ä¢ Validation Loss: 0.4056
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4056, best: 0.3958)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2169
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4008, best: 0.3958)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2056
  ‚Ä¢ Validation Loss: 0.3987
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3987, best: 0.3958)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2505
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4038, best: 0.3958)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2212
  ‚Ä¢ Validation Loss: 0.3993
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3993, best: 0.3958)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1986
  ‚Ä¢ Validation Loss: 0.4034
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4034, best: 0.3958)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2338
  ‚Ä¢ Validation Loss: 0.4005
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4005, best: 0.3958)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.3943
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3943
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1936
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3992, best: 0.3943)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2523
  ‚Ä¢ Validation Loss: 0.3963
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3963, best: 0.3943)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2181
  ‚Ä¢ Validation Loss: 0.3951
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3951, best: 0.3943)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2010
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.3940
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3940, best: 0.3921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2109
  ‚Ä¢ Validation Loss: 0.3960
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3960, best: 0.3921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2476
  ‚Ä¢ Validation Loss: 0.3917
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3917
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2322
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3958, best: 0.3917)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2212
  ‚Ä¢ Validation Loss: 0.4010
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4010, best: 0.3917)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2693
  ‚Ä¢ Validation Loss: 0.4008
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4008, best: 0.3917)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2547
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3980, best: 0.3917)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2550
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3921, best: 0.3917)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2281
  ‚Ä¢ Validation Loss: 0.3947
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.3947, best: 0.3917)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2427
  ‚Ä¢ Validation Loss: 0.3850
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3850
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1855
  ‚Ä¢ Validation Loss: 0.3920
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3920, best: 0.3850)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2598
  ‚Ä¢ Validation Loss: 0.3967
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3967, best: 0.3850)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2491
  ‚Ä¢ Validation Loss: 0.3966
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3966, best: 0.3850)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2338
  ‚Ä¢ Validation Loss: 0.3968
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3968, best: 0.3850)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2520
  ‚Ä¢ Validation Loss: 0.3899
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3899, best: 0.3850)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.3927
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3927, best: 0.3850)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2568
  ‚Ä¢ Validation Loss: 0.3945
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3945, best: 0.3850)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2181
  ‚Ä¢ Validation Loss: 0.3938
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3938, best: 0.3850)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 210/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3830
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2082
  ‚Ä¢ Validation Loss: 0.3853
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3853, best: 0.3830)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 212/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2846
  ‚Ä¢ Validation Loss: 0.3896
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3896, best: 0.3830)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2451
  ‚Ä¢ Validation Loss: 0.3850
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3850, best: 0.3830)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2220
  ‚Ä¢ Validation Loss: 0.3871
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3871, best: 0.3830)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1967
  ‚Ä¢ Validation Loss: 0.3891
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3891, best: 0.3830)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2275
  ‚Ä¢ Validation Loss: 0.3852
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3852, best: 0.3830)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2306
  ‚Ä¢ Validation Loss: 0.3828
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3828
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.3847
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3847, best: 0.3828)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2099
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3800
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2192
  ‚Ä¢ Validation Loss: 0.3877
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3877, best: 0.3800)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 221/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2848
  ‚Ä¢ Validation Loss: 0.3887
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3887, best: 0.3800)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1905
  ‚Ä¢ Validation Loss: 0.3833
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3833, best: 0.3800)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2286
  ‚Ä¢ Validation Loss: 0.3876
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3876, best: 0.3800)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2436
  ‚Ä¢ Validation Loss: 0.3837
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3837, best: 0.3800)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2254
  ‚Ä¢ Validation Loss: 0.3833
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3833, best: 0.3800)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2095
  ‚Ä¢ Validation Loss: 0.3901
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3901, best: 0.3800)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1971
  ‚Ä¢ Validation Loss: 0.3857
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3857, best: 0.3800)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2453
  ‚Ä¢ Validation Loss: 0.3851
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3851, best: 0.3800)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2275
  ‚Ä¢ Validation Loss: 0.3825
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3825, best: 0.3800)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2317
  ‚Ä¢ Validation Loss: 0.3807
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3807, best: 0.3800)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2246
  ‚Ä¢ Validation Loss: 0.3799
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3799
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2126
  ‚Ä¢ Validation Loss: 0.3825
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3825, best: 0.3799)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1700
  ‚Ä¢ Validation Loss: 0.3823
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3823, best: 0.3799)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2231
  ‚Ä¢ Validation Loss: 0.3835
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3835, best: 0.3799)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2042
  ‚Ä¢ Validation Loss: 0.3858
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3858, best: 0.3799)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2457
  ‚Ä¢ Validation Loss: 0.3909
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3909, best: 0.3799)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2211
  ‚Ä¢ Validation Loss: 0.3858
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3858, best: 0.3799)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2274
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3821, best: 0.3799)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2580
  ‚Ä¢ Validation Loss: 0.3823
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3823, best: 0.3799)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2184
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3831, best: 0.3799)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2627
  ‚Ä¢ Validation Loss: 0.3811
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3811, best: 0.3799)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2062
  ‚Ä¢ Validation Loss: 0.3826
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3826, best: 0.3799)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2165
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3800, best: 0.3799)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.3775
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3775
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2060
  ‚Ä¢ Validation Loss: 0.3857
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3857, best: 0.3775)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2210
  ‚Ä¢ Validation Loss: 0.3858
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3858, best: 0.3775)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2542
  ‚Ä¢ Validation Loss: 0.3775
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3775, best: 0.3775)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1866
  ‚Ä¢ Validation Loss: 0.3786
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3786, best: 0.3775)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2198
  ‚Ä¢ Validation Loss: 0.3792
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3792, best: 0.3775)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2300
  ‚Ä¢ Validation Loss: 0.3771
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3771
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.3741
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2546
  ‚Ä¢ Validation Loss: 0.3749
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3749, best: 0.3741)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2151
  ‚Ä¢ Validation Loss: 0.3788
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3788, best: 0.3741)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2232
  ‚Ä¢ Validation Loss: 0.3746
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3746, best: 0.3741)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1964
  ‚Ä¢ Validation Loss: 0.3745
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3745, best: 0.3741)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2143
  ‚Ä¢ Validation Loss: 0.3758
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3758, best: 0.3741)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2038
  ‚Ä¢ Validation Loss: 0.3748
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3748, best: 0.3741)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2459
  ‚Ä¢ Validation Loss: 0.3737
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3737
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1930
  ‚Ä¢ Validation Loss: 0.3732
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3732
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1081
  ‚Ä¢ Validation Loss: 0.3728
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3728
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1385
  ‚Ä¢ Validation Loss: 0.3785
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3785, best: 0.3728)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1511
  ‚Ä¢ Validation Loss: 0.3773
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3773, best: 0.3728)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1231
  ‚Ä¢ Validation Loss: 0.3761
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3761, best: 0.3728)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1155
  ‚Ä¢ Validation Loss: 0.3788
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3788, best: 0.3728)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3831, best: 0.3728)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1483
  ‚Ä¢ Validation Loss: 0.3743
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3743, best: 0.3728)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1345
  ‚Ä¢ Validation Loss: 0.3726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3726
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1449
  ‚Ä¢ Validation Loss: 0.3730
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3730, best: 0.3726)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1760
  ‚Ä¢ Validation Loss: 0.3744
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3744, best: 0.3726)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1597
  ‚Ä¢ Validation Loss: 0.3734
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3734, best: 0.3726)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1061
  ‚Ä¢ Validation Loss: 0.3738
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3738, best: 0.3726)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1209
  ‚Ä¢ Validation Loss: 0.3732
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3732, best: 0.3726)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0707
  ‚Ä¢ Validation Loss: 0.3738
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3738, best: 0.3726)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1598
  ‚Ä¢ Validation Loss: 0.3746
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3746, best: 0.3726)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1025
  ‚Ä¢ Validation Loss: 0.3749
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3749, best: 0.3726)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1446
  ‚Ä¢ Validation Loss: 0.3753
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3753, best: 0.3726)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1483
  ‚Ä¢ Validation Loss: 0.3727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3727, best: 0.3726)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.3734
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3734, best: 0.3726)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1515
  ‚Ä¢ Validation Loss: 0.3798
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3798, best: 0.3726)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0889
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3800, best: 0.3726)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1189
  ‚Ä¢ Validation Loss: 0.3729
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3729, best: 0.3726)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1181
  ‚Ä¢ Validation Loss: 0.3771
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3771, best: 0.3726)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1146
  ‚Ä¢ Validation Loss: 0.3762
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3762, best: 0.3726)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1174
  ‚Ä¢ Validation Loss: 0.3718
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1607
  ‚Ä¢ Validation Loss: 0.3767
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3767, best: 0.3718)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1236
  ‚Ä¢ Validation Loss: 0.3733
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3733, best: 0.3718)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1655
  ‚Ä¢ Validation Loss: 0.3752
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3752, best: 0.3718)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1500
  ‚Ä¢ Validation Loss: 0.3730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3730, best: 0.3718)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1521
  ‚Ä¢ Validation Loss: 0.3736
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3736, best: 0.3718)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1744
  ‚Ä¢ Validation Loss: 0.3730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3730, best: 0.3718)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1939
  ‚Ä¢ Validation Loss: 0.3765
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3765, best: 0.3718)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.3751
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3751, best: 0.3718)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1978
  ‚Ä¢ Validation Loss: 0.3726
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3726, best: 0.3718)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1124
  ‚Ä¢ Validation Loss: 0.3728
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3728, best: 0.3718)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1473
  ‚Ä¢ Validation Loss: 0.3743
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3743, best: 0.3718)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1342
  ‚Ä¢ Validation Loss: 0.3735
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3735, best: 0.3718)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1505
  ‚Ä¢ Validation Loss: 0.3767
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3767, best: 0.3718)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1509
  ‚Ä¢ Validation Loss: 0.3727
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3727, best: 0.3718)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1291
  ‚Ä¢ Validation Loss: 0.3736
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3736, best: 0.3718)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1190
  ‚Ä¢ Validation Loss: 0.3745
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3745, best: 0.3718)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3718
Total Epochs:   300
Models Saved:   ./Result/a1/Latin2
TensorBoard:    ./Result/a1/Latin2/tensorboard_logs
================================================================================

[20:53:23] Training completed. Best val loss: 0.3718

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9844, Recall=0.9935, F1=0.9889, IoU=0.9781
Paratext            : Precision=0.7490, Recall=0.6422, F1=0.6915, IoU=0.5285
Decoration          : Precision=0.8823, Recall=0.8802, F1=0.8812, IoU=0.7877
Main Text           : Precision=0.8607, Recall=0.7614, F1=0.8081, IoU=0.6779
Title               : Precision=0.8491, Recall=0.7717, F1=0.8086, IoU=0.6787
Chapter Headings    : Precision=0.6748, Recall=0.4029, F1=0.5045, IoU=0.3374

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8334
Mean Recall:    0.7420
Mean F1-Score:  0.7805
Mean IoU:       0.6647
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Loss functions created: CE (weighted), Focal (Œ≥=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.4887
  ‚Ä¢ Validation Loss: 0.8228
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8228
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7674
  ‚Ä¢ Validation Loss: 0.7685
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7685
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7039
  ‚Ä¢ Validation Loss: 0.7019
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7019
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6641
  ‚Ä¢ Validation Loss: 0.6392
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6392
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6326
  ‚Ä¢ Validation Loss: 0.6021
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6021
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6283
  ‚Ä¢ Validation Loss: 0.5785
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5785
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6056
  ‚Ä¢ Validation Loss: 0.5630
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5630
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5914
  ‚Ä¢ Validation Loss: 0.5622
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5622
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5772
  ‚Ä¢ Validation Loss: 0.5474
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5474
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5710
  ‚Ä¢ Validation Loss: 0.5377
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5377
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5533
  ‚Ä¢ Validation Loss: 0.5391
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5391, best: 0.5377)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5425
  ‚Ä¢ Validation Loss: 0.5250
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5250
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5390
  ‚Ä¢ Validation Loss: 0.5206
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5206
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5254
  ‚Ä¢ Validation Loss: 0.5174
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5174
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5169
  ‚Ä¢ Validation Loss: 0.5190
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5190, best: 0.5174)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5038
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5076
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4864
  ‚Ä¢ Validation Loss: 0.5071
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5071
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4890
  ‚Ä¢ Validation Loss: 0.4997
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4997
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4903
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4957
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4816
  ‚Ä¢ Validation Loss: 0.4985
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4985, best: 0.4957)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4716
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4685
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4915
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4687
  ‚Ä¢ Validation Loss: 0.4899
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4899
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4629
  ‚Ä¢ Validation Loss: 0.4831
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4831
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4548
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4883, best: 0.4831)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4532
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4832, best: 0.4831)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4583
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4902, best: 0.4831)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4560
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4935, best: 0.4831)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4476
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4811
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4455
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4778
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4480
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4800, best: 0.4778)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4378
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4803, best: 0.4778)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4104
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4760
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4276
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4769, best: 0.4760)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4343
  ‚Ä¢ Validation Loss: 0.4781
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4781, best: 0.4760)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4394
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4730
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4319
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4712
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4134
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4725, best: 0.4712)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4359
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4721, best: 0.4712)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4281
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4719, best: 0.4712)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4229
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4698
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4335
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4709, best: 0.4698)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4188
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4258
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4689, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4410
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4685, best: 0.4684)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4256
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4691, best: 0.4684)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4266
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4327
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4674
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4182
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4678, best: 0.4674)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4238
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4674)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4330
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4691, best: 0.4674)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4304
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4696, best: 0.4674)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4049
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4704, best: 0.4674)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4397
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4692, best: 0.4674)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4215
  ‚Ä¢ Validation Loss: 0.4614
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4614
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4088
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4614)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4099
  ‚Ä¢ Validation Loss: 0.4584
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4584
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4030
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4585, best: 0.4584)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4118
  ‚Ä¢ Validation Loss: 0.4548
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4548
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3984
  ‚Ä¢ Validation Loss: 0.4563
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4563, best: 0.4548)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4041
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4597, best: 0.4548)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3938
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4508
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3970
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4460
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3875
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4444
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3911
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4481, best: 0.4444)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3900
  ‚Ä¢ Validation Loss: 0.4420
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4420
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3810
  ‚Ä¢ Validation Loss: 0.4484
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4484, best: 0.4420)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3803
  ‚Ä¢ Validation Loss: 0.4411
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4411
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3885
  ‚Ä¢ Validation Loss: 0.4360
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4360
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3717
  ‚Ä¢ Validation Loss: 0.4402
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4402, best: 0.4360)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3444
  ‚Ä¢ Validation Loss: 0.4402
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4402, best: 0.4360)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3571
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4323
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3413
  ‚Ä¢ Validation Loss: 0.4418
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4418, best: 0.4323)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3686
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4353, best: 0.4323)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3527
  ‚Ä¢ Validation Loss: 0.4380
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4380, best: 0.4323)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3553
  ‚Ä¢ Validation Loss: 0.4275
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4275
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3727
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4301, best: 0.4275)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3470
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4302, best: 0.4275)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3561
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4330, best: 0.4275)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3475
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4266
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3551
  ‚Ä¢ Validation Loss: 0.4258
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4258
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3468
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4313, best: 0.4258)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3539
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4269, best: 0.4258)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3460
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4218
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3372
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4265, best: 0.4218)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3458
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4219, best: 0.4218)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3422
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4211
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3485
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4195
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3352
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4202, best: 0.4195)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3419
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4196, best: 0.4195)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3483
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4225, best: 0.4195)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 92/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3417
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4217, best: 0.4195)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 93/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3379
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4194
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3012
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4204, best: 0.4194)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3475
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4196, best: 0.4194)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3435
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4237, best: 0.4194)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 97/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3335
  ‚Ä¢ Validation Loss: 0.4141
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4141
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3205
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4163, best: 0.4141)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3198
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4163, best: 0.4141)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 100/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3240
  ‚Ä¢ Validation Loss: 0.4143
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4143, best: 0.4141)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3097
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4140
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3215
  ‚Ä¢ Validation Loss: 0.4125
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4125
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3145
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4133, best: 0.4125)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 104/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3291
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4163, best: 0.4125)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3249
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4140, best: 0.4125)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 106/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3217
  ‚Ä¢ Validation Loss: 0.4103
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4103
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2962
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4108, best: 0.4103)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3193
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4117, best: 0.4103)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2954
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4148, best: 0.4103)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3173
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4138, best: 0.4103)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3106
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4166, best: 0.4103)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3233
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4079
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2905
  ‚Ä¢ Validation Loss: 0.4093
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4093, best: 0.4079)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 114/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3172
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4102, best: 0.4079)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2809
  ‚Ä¢ Validation Loss: 0.4074
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4074
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3102
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4116, best: 0.4074)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2965
  ‚Ä¢ Validation Loss: 0.4088
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4088, best: 0.4074)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2945
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4079, best: 0.4074)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 119/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3212
  ‚Ä¢ Validation Loss: 0.4070
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4070
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2988
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4057
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2631
  ‚Ä¢ Validation Loss: 0.4085
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4085, best: 0.4057)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2850
  ‚Ä¢ Validation Loss: 0.4088
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4088, best: 0.4057)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2545
  ‚Ä¢ Validation Loss: 0.4075
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4075, best: 0.4057)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4105, best: 0.4057)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2219
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4059, best: 0.4057)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2696
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4063, best: 0.4057)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2691
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4066, best: 0.4057)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2520
  ‚Ä¢ Validation Loss: 0.4092
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4092, best: 0.4057)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2660
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4094, best: 0.4057)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2700
  ‚Ä¢ Validation Loss: 0.4054
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4054
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2547
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4058, best: 0.4054)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4071, best: 0.4054)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3065
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4068, best: 0.4054)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2805
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4058, best: 0.4054)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2974
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4057, best: 0.4054)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4063, best: 0.4054)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3119
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4059, best: 0.4054)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2170
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4052
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2353
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4057, best: 0.4052)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2689
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4052, best: 0.4052)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2180
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4048
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2769
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4044
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2951
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4048, best: 0.4044)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2441
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4047, best: 0.4044)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2550
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4058, best: 0.4044)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 146/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3107
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4045, best: 0.4044)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2941
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4058, best: 0.4044)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 148/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3189
  ‚Ä¢ Validation Loss: 0.4053
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4053, best: 0.4044)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2720
  ‚Ä¢ Validation Loss: 0.4049
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4049, best: 0.4044)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2417
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4047, best: 0.4044)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2184
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4121, best: 0.4044)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2686
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4160, best: 0.4044)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2699
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4095, best: 0.4044)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2767
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4146, best: 0.4044)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2786
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4122, best: 0.4044)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2390
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4149, best: 0.4044)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2225
  ‚Ä¢ Validation Loss: 0.4118
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4118, best: 0.4044)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3017
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4138, best: 0.4044)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2873
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4101, best: 0.4044)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2347
  ‚Ä¢ Validation Loss: 0.4100
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4100, best: 0.4044)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2917
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4071, best: 0.4044)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2204
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4063, best: 0.4044)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2736
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4084, best: 0.4044)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2775
  ‚Ä¢ Validation Loss: 0.4078
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4078, best: 0.4044)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2599
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4079, best: 0.4044)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3065
  ‚Ä¢ Validation Loss: 0.4099
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4099, best: 0.4044)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2664
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4036
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1973
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4091, best: 0.4036)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3026
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4072, best: 0.4036)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2947
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4011
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2770
  ‚Ä¢ Validation Loss: 0.4029
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4029, best: 0.4011)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3001
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4048, best: 0.4011)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2805
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4149, best: 0.4011)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2189
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4035, best: 0.4011)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2585
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4057, best: 0.4011)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2493
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4036, best: 0.4011)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2692
  ‚Ä¢ Validation Loss: 0.4037
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4037, best: 0.4011)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2175
  ‚Ä¢ Validation Loss: 0.4037
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4037, best: 0.4011)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2597
  ‚Ä¢ Validation Loss: 0.4003
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4003
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2929
  ‚Ä¢ Validation Loss: 0.4041
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4041, best: 0.4003)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2644
  ‚Ä¢ Validation Loss: 0.4001
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4001
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4061, best: 0.4001)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2524
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4058, best: 0.4001)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2720
  ‚Ä¢ Validation Loss: 0.4009
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4009, best: 0.4001)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2474
  ‚Ä¢ Validation Loss: 0.3978
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2438
  ‚Ä¢ Validation Loss: 0.3982
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3982, best: 0.3978)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 187/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2997
  ‚Ä¢ Validation Loss: 0.3979
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3979, best: 0.3978)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2872
  ‚Ä¢ Validation Loss: 0.4015
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4015, best: 0.3978)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2535
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4048, best: 0.3978)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2359
  ‚Ä¢ Validation Loss: 0.4075
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4075, best: 0.3978)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3980, best: 0.3978)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2634
  ‚Ä¢ Validation Loss: 0.3960
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3960
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2817
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4059, best: 0.3960)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2136
  ‚Ä¢ Validation Loss: 0.3985
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3985, best: 0.3960)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2472
  ‚Ä¢ Validation Loss: 0.3970
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3970, best: 0.3960)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2426
  ‚Ä¢ Validation Loss: 0.4027
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4027, best: 0.3960)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2280
  ‚Ä¢ Validation Loss: 0.4017
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4017, best: 0.3960)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2114
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1897
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3958, best: 0.3921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2443
  ‚Ä¢ Validation Loss: 0.3982
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.3982, best: 0.3921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2764
  ‚Ä¢ Validation Loss: 0.3956
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3956, best: 0.3921)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2723
  ‚Ä¢ Validation Loss: 0.3938
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3938, best: 0.3921)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2701
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4048, best: 0.3921)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2922
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4025, best: 0.3921)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2827
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3937, best: 0.3921)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2318
  ‚Ä¢ Validation Loss: 0.3922
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3922, best: 0.3921)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2242
  ‚Ä¢ Validation Loss: 0.3896
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3896
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.3895
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2334
  ‚Ä¢ Validation Loss: 0.3867
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2206
  ‚Ä¢ Validation Loss: 0.3852
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3852
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2000
  ‚Ä¢ Validation Loss: 0.3866
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3866, best: 0.3852)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2047
  ‚Ä¢ Validation Loss: 0.3919
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3919, best: 0.3852)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2631
  ‚Ä¢ Validation Loss: 0.3865
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3865, best: 0.3852)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2345
  ‚Ä¢ Validation Loss: 0.3947
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3947, best: 0.3852)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.3908
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3908, best: 0.3852)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2741
  ‚Ä¢ Validation Loss: 0.3876
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3876, best: 0.3852)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2759
  ‚Ä¢ Validation Loss: 0.3906
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3906, best: 0.3852)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2562
  ‚Ä¢ Validation Loss: 0.3949
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3949, best: 0.3852)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2370
  ‚Ä¢ Validation Loss: 0.3884
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3884, best: 0.3852)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2517
  ‚Ä¢ Validation Loss: 0.3873
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3873, best: 0.3852)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2011
  ‚Ä¢ Validation Loss: 0.3932
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3932, best: 0.3852)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2110
  ‚Ä¢ Validation Loss: 0.3961
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3961, best: 0.3852)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2114
  ‚Ä¢ Validation Loss: 0.3853
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3853, best: 0.3852)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2469
  ‚Ä¢ Validation Loss: 0.3872
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3872, best: 0.3852)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2357
  ‚Ä¢ Validation Loss: 0.3894
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3894, best: 0.3852)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2294
  ‚Ä¢ Validation Loss: 0.3932
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3932, best: 0.3852)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2441
  ‚Ä¢ Validation Loss: 0.3870
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3870, best: 0.3852)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2469
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3831
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2752
  ‚Ä¢ Validation Loss: 0.3828
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3828
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2190
  ‚Ä¢ Validation Loss: 0.3898
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3898, best: 0.3828)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2625
  ‚Ä¢ Validation Loss: 0.3873
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3873, best: 0.3828)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2515
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3810
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2331
  ‚Ä¢ Validation Loss: 0.3846
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3846, best: 0.3810)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2392
  ‚Ä¢ Validation Loss: 0.3840
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3840, best: 0.3810)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2585
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3818, best: 0.3810)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2490
  ‚Ä¢ Validation Loss: 0.3864
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3864, best: 0.3810)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2165
  ‚Ä¢ Validation Loss: 0.3895
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3895, best: 0.3810)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3885, best: 0.3810)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2495
  ‚Ä¢ Validation Loss: 0.3846
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3846, best: 0.3810)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2139
  ‚Ä¢ Validation Loss: 0.3875
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3875, best: 0.3810)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2044
  ‚Ä¢ Validation Loss: 0.3852
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3852, best: 0.3810)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2169
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3831, best: 0.3810)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2767
  ‚Ä¢ Validation Loss: 0.3859
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3859, best: 0.3810)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2560
  ‚Ä¢ Validation Loss: 0.3866
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3866, best: 0.3810)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2193
  ‚Ä¢ Validation Loss: 0.3871
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3871, best: 0.3810)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2393
  ‚Ä¢ Validation Loss: 0.3876
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3876, best: 0.3810)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2437
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3818, best: 0.3810)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2168
  ‚Ä¢ Validation Loss: 0.3819
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3819, best: 0.3810)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2067
  ‚Ä¢ Validation Loss: 0.3850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3850, best: 0.3810)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2419
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3830, best: 0.3810)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.3850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3850, best: 0.3810)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2125
  ‚Ä¢ Validation Loss: 0.3847
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3847, best: 0.3810)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2491
  ‚Ä¢ Validation Loss: 0.3835
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3835, best: 0.3810)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2669
  ‚Ä¢ Validation Loss: 0.3849
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3849, best: 0.3810)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3830, best: 0.3810)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2482
  ‚Ä¢ Validation Loss: 0.3802
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3802
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2351
  ‚Ä¢ Validation Loss: 0.3822
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3822, best: 0.3802)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1843
  ‚Ä¢ Validation Loss: 0.3836
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3836, best: 0.3802)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1704
  ‚Ä¢ Validation Loss: 0.3867
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3867, best: 0.3802)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1890
  ‚Ä¢ Validation Loss: 0.3835
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3835, best: 0.3802)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1487
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3818, best: 0.3802)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2017
  ‚Ä¢ Validation Loss: 0.3827
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3827, best: 0.3802)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.3833
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3833, best: 0.3802)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1988
  ‚Ä¢ Validation Loss: 0.3794
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3794
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2143
  ‚Ä¢ Validation Loss: 0.3817
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3817, best: 0.3794)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1368
  ‚Ä¢ Validation Loss: 0.3822
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3822, best: 0.3794)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1226
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3818, best: 0.3794)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1796
  ‚Ä¢ Validation Loss: 0.3788
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3788
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2229
  ‚Ä¢ Validation Loss: 0.3811
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3811, best: 0.3788)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1211
  ‚Ä¢ Validation Loss: 0.3832
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3832, best: 0.3788)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1847
  ‚Ä¢ Validation Loss: 0.3768
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3768
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1925
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3821, best: 0.3768)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.3797
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3797, best: 0.3768)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1893
  ‚Ä¢ Validation Loss: 0.3793
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3793, best: 0.3768)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1870
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3790, best: 0.3768)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2377
  ‚Ä¢ Validation Loss: 0.3799
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3799, best: 0.3768)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1456
  ‚Ä¢ Validation Loss: 0.3813
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3813, best: 0.3768)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1942
  ‚Ä¢ Validation Loss: 0.3834
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3834, best: 0.3768)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1562
  ‚Ä¢ Validation Loss: 0.3778
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3778, best: 0.3768)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.3794
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3794, best: 0.3768)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1423
  ‚Ä¢ Validation Loss: 0.3805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3805, best: 0.3768)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.3797
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3797, best: 0.3768)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2129
  ‚Ä¢ Validation Loss: 0.3795
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3795, best: 0.3768)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1403
  ‚Ä¢ Validation Loss: 0.3800
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3800, best: 0.3768)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2419
  ‚Ä¢ Validation Loss: 0.3784
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3784, best: 0.3768)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1518
  ‚Ä¢ Validation Loss: 0.3798
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3798, best: 0.3768)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1223
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3810, best: 0.3768)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1567
  ‚Ä¢ Validation Loss: 0.3808
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3808, best: 0.3768)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2002
  ‚Ä¢ Validation Loss: 0.3781
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3781, best: 0.3768)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1679
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3790, best: 0.3768)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2209
  ‚Ä¢ Validation Loss: 0.3782
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3782, best: 0.3768)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1614
  ‚Ä¢ Validation Loss: 0.3804
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3804, best: 0.3768)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1404
  ‚Ä¢ Validation Loss: 0.3805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3805, best: 0.3768)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1739
  ‚Ä¢ Validation Loss: 0.3787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3787, best: 0.3768)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1917
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3790, best: 0.3768)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1964
  ‚Ä¢ Validation Loss: 0.3808
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3808, best: 0.3768)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1431
  ‚Ä¢ Validation Loss: 0.3787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3787, best: 0.3768)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.3787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3787, best: 0.3768)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1570
  ‚Ä¢ Validation Loss: 0.3796
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3796, best: 0.3768)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1738
  ‚Ä¢ Validation Loss: 0.3784
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3784, best: 0.3768)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3768
Total Epochs:   300
Models Saved:   ./Result/a1/Latin14396
TensorBoard:    ./Result/a1/Latin14396/tensorboard_logs
================================================================================

[21:53:06] Training completed. Best val loss: 0.3768

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9846, Recall=0.9914, F1=0.9880, IoU=0.9763
Paratext            : Precision=0.6892, Recall=0.4387, F1=0.5362, IoU=0.3663
Decoration          : Precision=0.8882, Recall=0.9399, F1=0.9133, IoU=0.8405
Main Text           : Precision=0.8996, Recall=0.8369, F1=0.8671, IoU=0.7654
Title               : Precision=0.8186, Recall=0.7791, F1=0.7983, IoU=0.6644
Chapter Headings    : Precision=0.8344, Recall=0.6155, F1=0.7084, IoU=0.5485

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8524
Mean Recall:    0.7669
Mean F1-Score:  0.8019
Mean IoU:       0.6935
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Loss functions created: CE (weighted), Focal (Œ≥=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.5227
  ‚Ä¢ Validation Loss: 0.8084
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8084
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7870
  ‚Ä¢ Validation Loss: 0.7566
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7566
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7011
  ‚Ä¢ Validation Loss: 0.6795
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6592
  ‚Ä¢ Validation Loss: 0.6270
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6270
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6318
  ‚Ä¢ Validation Loss: 0.5711
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5711
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6239
  ‚Ä¢ Validation Loss: 0.5729
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5729, best: 0.5711)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5931
  ‚Ä¢ Validation Loss: 0.5466
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5466
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5661
  ‚Ä¢ Validation Loss: 0.5377
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5377
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5585
  ‚Ä¢ Validation Loss: 0.5324
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5324
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5426
  ‚Ä¢ Validation Loss: 0.5347
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5347, best: 0.5324)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5340
  ‚Ä¢ Validation Loss: 0.5211
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5211
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5238
  ‚Ä¢ Validation Loss: 0.5189
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5189
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5166
  ‚Ä¢ Validation Loss: 0.5147
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5147
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5172
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4993
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5017
  ‚Ä¢ Validation Loss: 0.5003
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5003, best: 0.4993)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4990
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4884
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4878
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4960, best: 0.4884)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4877
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4827
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4877
  ‚Ä¢ Validation Loss: 0.4817
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4817
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4836
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4906, best: 0.4817)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4688
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4717
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4686
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4710
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4676
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4678
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4513
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4630
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4596
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4624
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4478
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4554
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4510
  ‚Ä¢ Validation Loss: 0.4557
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4557, best: 0.4554)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4438
  ‚Ä¢ Validation Loss: 0.4514
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4514
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4464
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4481
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4334
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4527, best: 0.4481)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4354
  ‚Ä¢ Validation Loss: 0.4486
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4486, best: 0.4481)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4261
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4436
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4291
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4441, best: 0.4436)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4244
  ‚Ä¢ Validation Loss: 0.4401
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4401
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4238
  ‚Ä¢ Validation Loss: 0.4443
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4443, best: 0.4401)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4221
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4460, best: 0.4401)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4236
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4390
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4120
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4396, best: 0.4390)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4123
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4403, best: 0.4390)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4078
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4431, best: 0.4390)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4125
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4361
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4112
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4376, best: 0.4361)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4076
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4355
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4110
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4358, best: 0.4355)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4108
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4359, best: 0.4355)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4054
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4347
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4114
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4361, best: 0.4347)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4039
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4354, best: 0.4347)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4082
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4361, best: 0.4347)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4030
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4359, best: 0.4347)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4119
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4447, best: 0.4347)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4129
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4346
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4203
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4670, best: 0.4346)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4197
  ‚Ä¢ Validation Loss: 0.4438
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4438, best: 0.4346)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4081
  ‚Ä¢ Validation Loss: 0.4349
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4349, best: 0.4346)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3919
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3970
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4244
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3900
  ‚Ä¢ Validation Loss: 0.4203
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4203
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4007
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4203)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3867
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4238, best: 0.4203)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3763
  ‚Ä¢ Validation Loss: 0.4137
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4137
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3889
  ‚Ä¢ Validation Loss: 0.4360
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4360, best: 0.4137)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3680
  ‚Ä¢ Validation Loss: 0.4144
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4144, best: 0.4137)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3763
  ‚Ä¢ Validation Loss: 0.4255
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4255, best: 0.4137)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3675
  ‚Ä¢ Validation Loss: 0.4178
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4178, best: 0.4137)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3699
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4072
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3661
  ‚Ä¢ Validation Loss: 0.4201
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4201, best: 0.4072)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3769
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4247, best: 0.4072)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3509
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4032
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3555
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4353, best: 0.4032)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3712
  ‚Ä¢ Validation Loss: 0.4031
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4031
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3742
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4195, best: 0.4031)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.4039
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4039, best: 0.4031)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3480
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4045, best: 0.4031)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3374
  ‚Ä¢ Validation Loss: 0.4092
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4092, best: 0.4031)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3475
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3980
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3250
  ‚Ä¢ Validation Loss: 0.3995
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3995, best: 0.3980)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3340
  ‚Ä¢ Validation Loss: 0.3981
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3981, best: 0.3980)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3270
  ‚Ä¢ Validation Loss: 0.4271
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4271, best: 0.3980)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3299
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3965
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3242
  ‚Ä¢ Validation Loss: 0.4002
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4002, best: 0.3965)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3223
  ‚Ä¢ Validation Loss: 0.3988
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3988, best: 0.3965)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3164
  ‚Ä¢ Validation Loss: 0.3910
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3209
  ‚Ä¢ Validation Loss: 0.3913
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3913, best: 0.3910)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3178
  ‚Ä¢ Validation Loss: 0.3928
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3928, best: 0.3910)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3104
  ‚Ä¢ Validation Loss: 0.3949
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3949, best: 0.3910)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3235
  ‚Ä¢ Validation Loss: 0.3944
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3944, best: 0.3910)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3142
  ‚Ä¢ Validation Loss: 0.3893
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3893
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3155
  ‚Ä¢ Validation Loss: 0.3872
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3872
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3230
  ‚Ä¢ Validation Loss: 0.3977
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3977, best: 0.3872)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3047
  ‚Ä¢ Validation Loss: 0.3906
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3906, best: 0.3872)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3159
  ‚Ä¢ Validation Loss: 0.3846
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3846
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3088
  ‚Ä¢ Validation Loss: 0.3832
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3832
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3221
  ‚Ä¢ Validation Loss: 0.3834
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3834, best: 0.3832)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2701
  ‚Ä¢ Validation Loss: 0.3878
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3878, best: 0.3832)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3067
  ‚Ä¢ Validation Loss: 0.3874
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3874, best: 0.3832)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2889
  ‚Ä¢ Validation Loss: 0.3803
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3803
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3051
  ‚Ä¢ Validation Loss: 0.3808
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3808, best: 0.3803)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3079
  ‚Ä¢ Validation Loss: 0.3809
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3809, best: 0.3803)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 100/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2985
  ‚Ä¢ Validation Loss: 0.3832
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.3832, best: 0.3803)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 101/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3070
  ‚Ä¢ Validation Loss: 0.3847
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3847, best: 0.3803)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3073
  ‚Ä¢ Validation Loss: 0.3845
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3845, best: 0.3803)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 103/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2895
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3790
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2984
  ‚Ä¢ Validation Loss: 0.3784
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3784
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2872
  ‚Ä¢ Validation Loss: 0.3746
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3746
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2655
  ‚Ä¢ Validation Loss: 0.3819
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3819, best: 0.3746)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2790
  ‚Ä¢ Validation Loss: 0.3804
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3804, best: 0.3746)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 108/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2851
  ‚Ä¢ Validation Loss: 0.3828
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3828, best: 0.3746)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 109/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2967
  ‚Ä¢ Validation Loss: 0.3785
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3785, best: 0.3746)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3015
  ‚Ä¢ Validation Loss: 0.3798
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3798, best: 0.3746)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2877
  ‚Ä¢ Validation Loss: 0.3761
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3761, best: 0.3746)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3040
  ‚Ä¢ Validation Loss: 0.3811
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3811, best: 0.3746)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2832
  ‚Ä¢ Validation Loss: 0.3764
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3764, best: 0.3746)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 114/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2841
  ‚Ä¢ Validation Loss: 0.3779
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3779, best: 0.3746)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2749
  ‚Ä¢ Validation Loss: 0.3744
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3744
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.3777
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3777, best: 0.3744)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 117/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2951
  ‚Ä¢ Validation Loss: 0.3756
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3756, best: 0.3744)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 118/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2907
  ‚Ä¢ Validation Loss: 0.3783
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3783, best: 0.3744)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.3754
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3754, best: 0.3744)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2760
  ‚Ä¢ Validation Loss: 0.3746
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3746, best: 0.3744)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2337
  ‚Ä¢ Validation Loss: 0.3744
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3744, best: 0.3744)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1990
  ‚Ä¢ Validation Loss: 0.3761
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3761, best: 0.3744)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2023
  ‚Ä¢ Validation Loss: 0.3758
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3758, best: 0.3744)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2265
  ‚Ä¢ Validation Loss: 0.3719
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3719
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2435
  ‚Ä¢ Validation Loss: 0.3744
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3744, best: 0.3719)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2787
  ‚Ä¢ Validation Loss: 0.3716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3716
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2269
  ‚Ä¢ Validation Loss: 0.3709
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.3721
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3721, best: 0.3709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2463
  ‚Ä¢ Validation Loss: 0.3730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3730, best: 0.3709)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2051
  ‚Ä¢ Validation Loss: 0.3743
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3743, best: 0.3709)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2297
  ‚Ä¢ Validation Loss: 0.3730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3730, best: 0.3709)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2639
  ‚Ä¢ Validation Loss: 0.3731
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3731, best: 0.3709)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2618
  ‚Ä¢ Validation Loss: 0.3724
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3724, best: 0.3709)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2505
  ‚Ä¢ Validation Loss: 0.3718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3718, best: 0.3709)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 135/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2876
  ‚Ä¢ Validation Loss: 0.3711
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3711, best: 0.3709)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2592
  ‚Ä¢ Validation Loss: 0.3720
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3720, best: 0.3709)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.3716
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3716, best: 0.3709)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2695
  ‚Ä¢ Validation Loss: 0.3710
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3710, best: 0.3709)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2607
  ‚Ä¢ Validation Loss: 0.3709
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 140/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2846
  ‚Ä¢ Validation Loss: 0.3718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3718, best: 0.3709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2468
  ‚Ä¢ Validation Loss: 0.3720
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3720, best: 0.3709)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 142/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2851
  ‚Ä¢ Validation Loss: 0.3713
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3713, best: 0.3709)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.3717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3717, best: 0.3709)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.3719
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3719, best: 0.3709)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2393
  ‚Ä¢ Validation Loss: 0.3715
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3715, best: 0.3709)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2659
  ‚Ä¢ Validation Loss: 0.3711
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3711, best: 0.3709)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2750
  ‚Ä¢ Validation Loss: 0.3717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3717, best: 0.3709)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2517
  ‚Ä¢ Validation Loss: 0.3716
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3716, best: 0.3709)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2655
  ‚Ä¢ Validation Loss: 0.3712
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3712, best: 0.3709)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2464
  ‚Ä¢ Validation Loss: 0.3712
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3712, best: 0.3709)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2217
  ‚Ä¢ Validation Loss: 0.3817
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3817, best: 0.3709)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2522
  ‚Ä¢ Validation Loss: 0.3809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3809, best: 0.3709)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2257
  ‚Ä¢ Validation Loss: 0.3727
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3727, best: 0.3709)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.3762
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3762, best: 0.3709)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2673
  ‚Ä¢ Validation Loss: 0.3809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3809, best: 0.3709)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2303
  ‚Ä¢ Validation Loss: 0.3755
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3755, best: 0.3709)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2654
  ‚Ä¢ Validation Loss: 0.3784
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3784, best: 0.3709)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2520
  ‚Ä¢ Validation Loss: 0.3824
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3824, best: 0.3709)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2251
  ‚Ä¢ Validation Loss: 0.3778
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3778, best: 0.3709)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2361
  ‚Ä¢ Validation Loss: 0.3711
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3711, best: 0.3709)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2274
  ‚Ä¢ Validation Loss: 0.3775
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3775, best: 0.3709)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1815
  ‚Ä¢ Validation Loss: 0.3850
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3850, best: 0.3709)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2557
  ‚Ä¢ Validation Loss: 0.3781
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3781, best: 0.3709)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2611
  ‚Ä¢ Validation Loss: 0.3724
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3724, best: 0.3709)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2165
  ‚Ä¢ Validation Loss: 0.3722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3722, best: 0.3709)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2474
  ‚Ä¢ Validation Loss: 0.3739
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3739, best: 0.3709)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2725
  ‚Ä¢ Validation Loss: 0.3684
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2390
  ‚Ä¢ Validation Loss: 0.3662
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3662
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2473
  ‚Ä¢ Validation Loss: 0.3715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3715, best: 0.3662)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2670
  ‚Ä¢ Validation Loss: 0.3623
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3623
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2474
  ‚Ä¢ Validation Loss: 0.3773
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3773, best: 0.3623)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2446
  ‚Ä¢ Validation Loss: 0.3769
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3769, best: 0.3623)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2466
  ‚Ä¢ Validation Loss: 0.3648
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3648, best: 0.3623)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2700
  ‚Ä¢ Validation Loss: 0.3682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3682, best: 0.3623)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2507
  ‚Ä¢ Validation Loss: 0.3685
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3685, best: 0.3623)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2563
  ‚Ä¢ Validation Loss: 0.3741
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3741, best: 0.3623)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2360
  ‚Ä¢ Validation Loss: 0.3678
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3678, best: 0.3623)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2621
  ‚Ä¢ Validation Loss: 0.3660
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3660, best: 0.3623)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.3677
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3677, best: 0.3623)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2584
  ‚Ä¢ Validation Loss: 0.3788
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3788, best: 0.3623)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2278
  ‚Ä¢ Validation Loss: 0.3719
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3719, best: 0.3623)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2450
  ‚Ä¢ Validation Loss: 0.3679
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3679, best: 0.3623)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1919
  ‚Ä¢ Validation Loss: 0.3713
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3713, best: 0.3623)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2404
  ‚Ä¢ Validation Loss: 0.3686
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3686, best: 0.3623)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2253
  ‚Ä¢ Validation Loss: 0.3697
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3697, best: 0.3623)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2510
  ‚Ä¢ Validation Loss: 0.3705
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3705, best: 0.3623)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2611
  ‚Ä¢ Validation Loss: 0.3692
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3692, best: 0.3623)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2547
  ‚Ä¢ Validation Loss: 0.3622
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3622
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2049
  ‚Ä¢ Validation Loss: 0.3591
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3591
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2366
  ‚Ä¢ Validation Loss: 0.3618
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3618, best: 0.3591)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2590
  ‚Ä¢ Validation Loss: 0.3587
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3587
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2127
  ‚Ä¢ Validation Loss: 0.3746
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3746, best: 0.3587)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2253
  ‚Ä¢ Validation Loss: 0.3579
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3579
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2115
  ‚Ä¢ Validation Loss: 0.3665
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3665, best: 0.3579)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2457
  ‚Ä¢ Validation Loss: 0.3623
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3623, best: 0.3579)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2560
  ‚Ä¢ Validation Loss: 0.3674
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3674, best: 0.3579)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2481
  ‚Ä¢ Validation Loss: 0.3601
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3601, best: 0.3579)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2538
  ‚Ä¢ Validation Loss: 0.3601
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3601, best: 0.3579)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2358
  ‚Ä¢ Validation Loss: 0.3590
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3590, best: 0.3579)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2023
  ‚Ä¢ Validation Loss: 0.3654
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.3654, best: 0.3579)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2248
  ‚Ä¢ Validation Loss: 0.3603
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3603, best: 0.3579)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2304
  ‚Ä¢ Validation Loss: 0.3618
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3618, best: 0.3579)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2265
  ‚Ä¢ Validation Loss: 0.3646
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3646, best: 0.3579)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1938
  ‚Ä¢ Validation Loss: 0.3595
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3595, best: 0.3579)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2303
  ‚Ä¢ Validation Loss: 0.3621
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3621, best: 0.3579)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2484
  ‚Ä¢ Validation Loss: 0.3618
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3618, best: 0.3579)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2076
  ‚Ä¢ Validation Loss: 0.3623
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3623, best: 0.3579)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1681
  ‚Ä¢ Validation Loss: 0.3601
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3601, best: 0.3579)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2405
  ‚Ä¢ Validation Loss: 0.3591
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3591, best: 0.3579)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2298
  ‚Ä¢ Validation Loss: 0.3543
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3543
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2330
  ‚Ä¢ Validation Loss: 0.3571
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3571, best: 0.3543)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2101
  ‚Ä¢ Validation Loss: 0.3567
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3567, best: 0.3543)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2269
  ‚Ä¢ Validation Loss: 0.3605
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3605, best: 0.3543)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2337
  ‚Ä¢ Validation Loss: 0.3598
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3598, best: 0.3543)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2447
  ‚Ä¢ Validation Loss: 0.3725
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3725, best: 0.3543)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2166
  ‚Ä¢ Validation Loss: 0.3592
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3592, best: 0.3543)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1911
  ‚Ä¢ Validation Loss: 0.3540
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3540
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2402
  ‚Ä¢ Validation Loss: 0.3619
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3619, best: 0.3540)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2091
  ‚Ä¢ Validation Loss: 0.3605
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3605, best: 0.3540)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.3517
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3517
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2152
  ‚Ä¢ Validation Loss: 0.3633
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3633, best: 0.3517)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2238
  ‚Ä¢ Validation Loss: 0.3578
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3578, best: 0.3517)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2124
  ‚Ä¢ Validation Loss: 0.3572
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3572, best: 0.3517)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2292
  ‚Ä¢ Validation Loss: 0.3547
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3547, best: 0.3517)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 225/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2588
  ‚Ä¢ Validation Loss: 0.3498
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3498
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2243
  ‚Ä¢ Validation Loss: 0.3544
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3544, best: 0.3498)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2397
  ‚Ä¢ Validation Loss: 0.3526
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3526, best: 0.3498)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1691
  ‚Ä¢ Validation Loss: 0.3535
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3535, best: 0.3498)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1987
  ‚Ä¢ Validation Loss: 0.3529
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3529, best: 0.3498)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2036
  ‚Ä¢ Validation Loss: 0.3547
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3547, best: 0.3498)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2401
  ‚Ä¢ Validation Loss: 0.3541
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3541, best: 0.3498)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2191
  ‚Ä¢ Validation Loss: 0.3509
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3509, best: 0.3498)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.3552
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3552, best: 0.3498)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2194
  ‚Ä¢ Validation Loss: 0.3493
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3493
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2285
  ‚Ä¢ Validation Loss: 0.3533
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3533, best: 0.3493)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2412
  ‚Ä¢ Validation Loss: 0.3569
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3569, best: 0.3493)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2229
  ‚Ä¢ Validation Loss: 0.3513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3513, best: 0.3493)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2125
  ‚Ä¢ Validation Loss: 0.3622
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3622, best: 0.3493)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2181
  ‚Ä¢ Validation Loss: 0.3539
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3539, best: 0.3493)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 240/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2456
  ‚Ä¢ Validation Loss: 0.3645
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3645, best: 0.3493)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 241/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2587
  ‚Ä¢ Validation Loss: 0.3530
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3530, best: 0.3493)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2370
  ‚Ä¢ Validation Loss: 0.3538
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3538, best: 0.3493)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2323
  ‚Ä¢ Validation Loss: 0.3517
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3517, best: 0.3493)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1334
  ‚Ä¢ Validation Loss: 0.3564
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3564, best: 0.3493)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2081
  ‚Ä¢ Validation Loss: 0.3538
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3538, best: 0.3493)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 246/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2584
  ‚Ä¢ Validation Loss: 0.3506
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3506, best: 0.3493)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2003
  ‚Ä¢ Validation Loss: 0.3511
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3511, best: 0.3493)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2103
  ‚Ä¢ Validation Loss: 0.3523
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3523, best: 0.3493)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2307
  ‚Ä¢ Validation Loss: 0.3482
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3482
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2391
  ‚Ä¢ Validation Loss: 0.3489
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3489, best: 0.3482)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 251/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.3513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3513, best: 0.3482)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2034
  ‚Ä¢ Validation Loss: 0.3529
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3529, best: 0.3482)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2222
  ‚Ä¢ Validation Loss: 0.3534
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3534, best: 0.3482)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1415
  ‚Ä¢ Validation Loss: 0.3512
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3512, best: 0.3482)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1296
  ‚Ä¢ Validation Loss: 0.3539
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3539, best: 0.3482)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1808
  ‚Ä¢ Validation Loss: 0.3507
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3507, best: 0.3482)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1824
  ‚Ä¢ Validation Loss: 0.3522
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3522, best: 0.3482)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 258/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2513
  ‚Ä¢ Validation Loss: 0.3485
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3485, best: 0.3482)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2188
  ‚Ä¢ Validation Loss: 0.3540
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3540, best: 0.3482)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2038
  ‚Ä¢ Validation Loss: 0.3508
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3508, best: 0.3482)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1624
  ‚Ä¢ Validation Loss: 0.3481
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3481
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1790
  ‚Ä¢ Validation Loss: 0.3473
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3473
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1891
  ‚Ä¢ Validation Loss: 0.3488
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3488, best: 0.3473)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2012
  ‚Ä¢ Validation Loss: 0.3480
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3480, best: 0.3473)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.3499
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3499, best: 0.3473)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1876
  ‚Ä¢ Validation Loss: 0.3484
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3484, best: 0.3473)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1566
  ‚Ä¢ Validation Loss: 0.3478
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3478, best: 0.3473)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1836
  ‚Ä¢ Validation Loss: 0.3498
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3498, best: 0.3473)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1469
  ‚Ä¢ Validation Loss: 0.3510
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3510, best: 0.3473)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1644
  ‚Ä¢ Validation Loss: 0.3504
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3504, best: 0.3473)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2043
  ‚Ä¢ Validation Loss: 0.3498
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3498, best: 0.3473)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1475
  ‚Ä¢ Validation Loss: 0.3472
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3472
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1561
  ‚Ä¢ Validation Loss: 0.3497
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3497, best: 0.3472)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2097
  ‚Ä¢ Validation Loss: 0.3524
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3524, best: 0.3472)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1448
  ‚Ä¢ Validation Loss: 0.3507
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3507, best: 0.3472)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1392
  ‚Ä¢ Validation Loss: 0.3472
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3472, best: 0.3472)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1779
  ‚Ä¢ Validation Loss: 0.3485
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3485, best: 0.3472)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1826
  ‚Ä¢ Validation Loss: 0.3488
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3488, best: 0.3472)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2033
  ‚Ä¢ Validation Loss: 0.3493
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3493, best: 0.3472)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1731
  ‚Ä¢ Validation Loss: 0.3468
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3468
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1853
  ‚Ä¢ Validation Loss: 0.3504
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3504, best: 0.3468)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1747
  ‚Ä¢ Validation Loss: 0.3511
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3511, best: 0.3468)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2281
  ‚Ä¢ Validation Loss: 0.3496
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3496, best: 0.3468)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2039
  ‚Ä¢ Validation Loss: 0.3478
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3478, best: 0.3468)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1682
  ‚Ä¢ Validation Loss: 0.3481
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3481, best: 0.3468)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1576
  ‚Ä¢ Validation Loss: 0.3505
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3505, best: 0.3468)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2027
  ‚Ä¢ Validation Loss: 0.3525
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3525, best: 0.3468)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1640
  ‚Ä¢ Validation Loss: 0.3472
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3472, best: 0.3468)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1140
  ‚Ä¢ Validation Loss: 0.3480
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3480, best: 0.3468)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1789
  ‚Ä¢ Validation Loss: 0.3534
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3534, best: 0.3468)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1996
  ‚Ä¢ Validation Loss: 0.3499
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3499, best: 0.3468)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1290
  ‚Ä¢ Validation Loss: 0.3504
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3504, best: 0.3468)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1599
  ‚Ä¢ Validation Loss: 0.3498
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3498, best: 0.3468)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2159
  ‚Ä¢ Validation Loss: 0.3502
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3502, best: 0.3468)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2014
  ‚Ä¢ Validation Loss: 0.3488
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3488, best: 0.3468)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1792
  ‚Ä¢ Validation Loss: 0.3503
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3503, best: 0.3468)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1874
  ‚Ä¢ Validation Loss: 0.3516
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3516, best: 0.3468)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1801
  ‚Ä¢ Validation Loss: 0.3499
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3499, best: 0.3468)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1844
  ‚Ä¢ Validation Loss: 0.3486
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3486, best: 0.3468)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1875
  ‚Ä¢ Validation Loss: 0.3476
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3476, best: 0.3468)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3468
Total Epochs:   300
Models Saved:   ./Result/a1/Latin16746
TensorBoard:    ./Result/a1/Latin16746/tensorboard_logs
================================================================================

[22:58:30] Training completed. Best val loss: 0.3468

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9811, Recall=0.9932, F1=0.9871, IoU=0.9745
Paratext            : Precision=0.7584, Recall=0.7479, F1=0.7531, IoU=0.6040
Decoration          : Precision=0.9802, Recall=0.8864, F1=0.9310, IoU=0.8709
Main Text           : Precision=0.9209, Recall=0.8551, F1=0.8868, IoU=0.7966
Title               : Precision=0.8025, Recall=0.6230, F1=0.7014, IoU=0.5402
Chapter Headings    : Precision=0.8973, Recall=0.6977, F1=0.7850, IoU=0.6461

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8901
Mean Recall:    0.8005
Mean F1-Score:  0.8407
Mean IoU:       0.7387
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Loss functions created: CE (weighted), Focal (Œ≥=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,431
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.3949
  ‚Ä¢ Validation Loss: 0.8715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8715
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8177
  ‚Ä¢ Validation Loss: 0.7792
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7792
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7206
  ‚Ä¢ Validation Loss: 0.7183
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7183
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6951
  ‚Ä¢ Validation Loss: 0.6703
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6703
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6507
  ‚Ä¢ Validation Loss: 0.5963
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5963
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6295
  ‚Ä¢ Validation Loss: 0.5501
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5501
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5667
  ‚Ä¢ Validation Loss: 0.5355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5355
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4744
  ‚Ä¢ Validation Loss: 0.5231
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5231
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4965
  ‚Ä¢ Validation Loss: 0.5521
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5521, best: 0.5231)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3273
  ‚Ä¢ Validation Loss: 0.5132
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5132
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4649
  ‚Ä¢ Validation Loss: 0.5059
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5059
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4284
  ‚Ä¢ Validation Loss: 0.4992
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4992
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3076
  ‚Ä¢ Validation Loss: 0.5027
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5027, best: 0.4992)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4161
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4967
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4447
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4934
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3754
  ‚Ä¢ Validation Loss: 0.5044
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5044, best: 0.4934)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4512
  ‚Ä¢ Validation Loss: 0.4949
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4949, best: 0.4934)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4509
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2966
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4824
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3098
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4884, best: 0.4824)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3917
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4823
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4166
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4807
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4316
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4868, best: 0.4807)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3652
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4808, best: 0.4807)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3767
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4839, best: 0.4807)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4340
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4801, best: 0.4795)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4115
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4833, best: 0.4795)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4032
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4775
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4766
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4269
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4784, best: 0.4766)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4362
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4729
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3996
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4760, best: 0.4729)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3372
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4754, best: 0.4729)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4087
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4763, best: 0.4729)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3607
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4751, best: 0.4729)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4385
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4737, best: 0.4729)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4196
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4719
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4401
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4732, best: 0.4719)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4287
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4721, best: 0.4719)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4018
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3337
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4723, best: 0.4718)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4230
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4713
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3872
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4718, best: 0.4713)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3624
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4721, best: 0.4713)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4123
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4713)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3454
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4721, best: 0.4713)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3550
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4713)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3589
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4718, best: 0.4713)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4221
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4713)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3567
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4717, best: 0.4713)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3828
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4713)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4223
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4756, best: 0.4713)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3939
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4754, best: 0.4713)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3957
  ‚Ä¢ Validation Loss: 0.4734
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4734, best: 0.4713)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3423
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4673
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3888
  ‚Ä¢ Validation Loss: 0.4656
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4656
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3277
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4676, best: 0.4656)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4058
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4667, best: 0.4656)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3549
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4684, best: 0.4656)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4059
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4651
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3180
  ‚Ä¢ Validation Loss: 0.4649
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4649
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3980
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4618
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3606
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4592
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2977
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4653, best: 0.4592)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3429
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4599, best: 0.4592)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3435
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4594, best: 0.4592)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3680
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4618, best: 0.4592)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3888
  ‚Ä¢ Validation Loss: 0.4609
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4609, best: 0.4592)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3152
  ‚Ä¢ Validation Loss: 0.4567
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4567
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3632
  ‚Ä¢ Validation Loss: 0.4547
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2808
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4471
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3055
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4537, best: 0.4471)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3543
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4568, best: 0.4471)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3724
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4554, best: 0.4471)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3545
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4466
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2854
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4466)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3054
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4524, best: 0.4466)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3842
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4510, best: 0.4466)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3225
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4512, best: 0.4466)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3458
  ‚Ä¢ Validation Loss: 0.4509
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4509, best: 0.4466)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3715
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4454
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2945
  ‚Ä¢ Validation Loss: 0.4482
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4482, best: 0.4454)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3555
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4447
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2957
  ‚Ä¢ Validation Loss: 0.4418
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4418
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3170
  ‚Ä¢ Validation Loss: 0.4502
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4502, best: 0.4418)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3936
  ‚Ä¢ Validation Loss: 0.4405
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4405
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3663
  ‚Ä¢ Validation Loss: 0.4434
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4434, best: 0.4405)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3258
  ‚Ä¢ Validation Loss: 0.4402
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4402
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2867
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4351
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3321
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4379, best: 0.4351)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3780
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4410, best: 0.4351)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2680
  ‚Ä¢ Validation Loss: 0.4395
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4395, best: 0.4351)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3367
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4430, best: 0.4351)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3705
  ‚Ä¢ Validation Loss: 0.4502
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4502, best: 0.4351)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3564
  ‚Ä¢ Validation Loss: 0.4342
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4342
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3333
  ‚Ä¢ Validation Loss: 0.4387
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4387, best: 0.4342)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3447
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4385, best: 0.4342)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3411
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4320
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3144
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.4320
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3289
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4313
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3647
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4320, best: 0.4313)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3702
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4286
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2911
  ‚Ä¢ Validation Loss: 0.4335
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4335, best: 0.4286)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3203
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4302, best: 0.4286)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1960
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4293, best: 0.4286)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 107/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3754
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4280
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2918
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4290, best: 0.4280)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3384
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4298, best: 0.4280)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3435
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3639
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4289, best: 0.4277)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3622
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4326, best: 0.4277)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2997
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4249
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2942
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4261, best: 0.4249)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3562
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4256, best: 0.4249)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3323
  ‚Ä¢ Validation Loss: 0.4257
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4257, best: 0.4249)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 117/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3552
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4249, best: 0.4249)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2666
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4263, best: 0.4249)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3276
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4270, best: 0.4249)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2903
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4248
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3344
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4274, best: 0.4248)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3119
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4236
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3380
  ‚Ä¢ Validation Loss: 0.4246
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4246, best: 0.4236)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3163
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4252, best: 0.4236)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2994
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4267, best: 0.4236)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3270
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4233
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 127/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3406
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4238, best: 0.4233)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 128/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3427
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4216
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3172
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4230, best: 0.4216)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3292
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4238, best: 0.4216)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 131/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3471
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4236, best: 0.4216)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4225, best: 0.4216)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2879
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4230, best: 0.4216)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 134/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3469
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4225, best: 0.4216)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2574
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2594
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4216)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2063
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1931
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2521
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4236, best: 0.4216)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3036
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4229, best: 0.4216)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2482
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4220, best: 0.4216)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3241
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4216)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2381
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4215
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4215)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2334
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4219, best: 0.4215)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2374
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4215)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2883
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4223, best: 0.4215)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1970
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4227, best: 0.4215)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2788
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4215)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4220, best: 0.4215)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2646
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4343, best: 0.4215)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2183
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4376, best: 0.4215)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2516
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4344, best: 0.4215)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2586
  ‚Ä¢ Validation Loss: 0.4330
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4330, best: 0.4215)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2586
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4355, best: 0.4215)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2803
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4232, best: 0.4215)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2303
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4252, best: 0.4215)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2928
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4254, best: 0.4215)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1967
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4397, best: 0.4215)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2470
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4238, best: 0.4215)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2300
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4215
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2894
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4238, best: 0.4215)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2360
  ‚Ä¢ Validation Loss: 0.4231
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4231, best: 0.4215)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2627
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4215, best: 0.4215)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2168
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4265, best: 0.4215)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2557
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4248, best: 0.4215)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2441
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4269, best: 0.4215)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1992
  ‚Ä¢ Validation Loss: 0.4182
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4182
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2318
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4182)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1979
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4171
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4202, best: 0.4171)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2368
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4171, best: 0.4171)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2134
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4243, best: 0.4171)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2031
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4293, best: 0.4171)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3048
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4170
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2612
  ‚Ä¢ Validation Loss: 0.4151
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4151
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2204
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4151)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2355
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2516
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4204, best: 0.4146)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1784
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4107
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2063
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4108, best: 0.4107)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2562
  ‚Ä¢ Validation Loss: 0.4154
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4154, best: 0.4107)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2846
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4162, best: 0.4107)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1841
  ‚Ä¢ Validation Loss: 0.4155
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4155, best: 0.4107)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2385
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4163, best: 0.4107)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1596
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4112, best: 0.4107)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2084
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4153, best: 0.4107)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2486
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4192, best: 0.4107)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2621
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4207, best: 0.4107)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2443
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4117, best: 0.4107)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1790
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4117, best: 0.4107)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1854
  ‚Ä¢ Validation Loss: 0.4107
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4107
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2016
  ‚Ä¢ Validation Loss: 0.4103
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4103
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3100
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4101
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2215
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4113, best: 0.4101)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2276
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4183, best: 0.4101)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2410
  ‚Ä¢ Validation Loss: 0.4111
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4101)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1995
  ‚Ä¢ Validation Loss: 0.4143
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4143, best: 0.4101)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1699
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4299, best: 0.4101)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0872
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4319, best: 0.4101)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.4121
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4121, best: 0.4101)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1608
  ‚Ä¢ Validation Loss: 0.4130
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4130, best: 0.4101)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4105, best: 0.4101)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2562
  ‚Ä¢ Validation Loss: 0.4120
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4120, best: 0.4101)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2429
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4068
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1371
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4176, best: 0.4068)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2299
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4116, best: 0.4068)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1573
  ‚Ä¢ Validation Loss: 0.4087
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4087, best: 0.4068)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1898
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4163, best: 0.4068)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1445
  ‚Ä¢ Validation Loss: 0.4111
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4068)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1845
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4147, best: 0.4068)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1800
  ‚Ä¢ Validation Loss: 0.4111
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4068)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1374
  ‚Ä¢ Validation Loss: 0.4104
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4104, best: 0.4068)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1999
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4066
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.4056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4056
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2152
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4048
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1902
  ‚Ä¢ Validation Loss: 0.4077
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4077, best: 0.4048)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2026
  ‚Ä¢ Validation Loss: 0.4099
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4099, best: 0.4048)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1665
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4073, best: 0.4048)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1730
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4126, best: 0.4048)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1993
  ‚Ä¢ Validation Loss: 0.4056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4056, best: 0.4048)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1891
  ‚Ä¢ Validation Loss: 0.4205
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4205, best: 0.4048)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1808
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4162, best: 0.4048)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2308
  ‚Ä¢ Validation Loss: 0.4082
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4082, best: 0.4048)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2179
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4127, best: 0.4048)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2222
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4071, best: 0.4048)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1866
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4073, best: 0.4048)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2010
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4032
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1919
  ‚Ä¢ Validation Loss: 0.4125
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4125, best: 0.4032)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2418
  ‚Ä¢ Validation Loss: 0.4077
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4077, best: 0.4032)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1416
  ‚Ä¢ Validation Loss: 0.4031
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4031
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2110
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4065, best: 0.4031)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2061
  ‚Ä¢ Validation Loss: 0.4053
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4053, best: 0.4031)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2714
  ‚Ä¢ Validation Loss: 0.4051
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4051, best: 0.4031)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2310
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4153, best: 0.4031)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1980
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4047, best: 0.4031)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2127
  ‚Ä¢ Validation Loss: 0.3987
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3987
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2227
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4059, best: 0.3987)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2384
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4035, best: 0.3987)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2294
  ‚Ä¢ Validation Loss: 0.4018
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4018, best: 0.3987)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2236
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4047, best: 0.3987)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2505
  ‚Ä¢ Validation Loss: 0.3979
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3979
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2139
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4035, best: 0.3979)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2112
  ‚Ä¢ Validation Loss: 0.4034
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4034, best: 0.3979)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2132
  ‚Ä¢ Validation Loss: 0.4006
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4006, best: 0.3979)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2364
  ‚Ä¢ Validation Loss: 0.4042
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4042, best: 0.3979)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2144
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4065, best: 0.3979)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2193
  ‚Ä¢ Validation Loss: 0.4026
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4026, best: 0.3979)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2511
  ‚Ä¢ Validation Loss: 0.4039
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4039, best: 0.3979)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1855
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4058, best: 0.3979)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1892
  ‚Ä¢ Validation Loss: 0.4015
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4015, best: 0.3979)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.4024
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4024, best: 0.3979)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2176
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4035, best: 0.3979)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2134
  ‚Ä¢ Validation Loss: 0.4054
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4054, best: 0.3979)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2064
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4038, best: 0.3979)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1666
  ‚Ä¢ Validation Loss: 0.4087
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4087, best: 0.3979)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2170
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4050, best: 0.3979)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2198
  ‚Ä¢ Validation Loss: 0.3984
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3984, best: 0.3979)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2205
  ‚Ä¢ Validation Loss: 0.3974
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3974
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2540
  ‚Ä¢ Validation Loss: 0.4062
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4062, best: 0.3974)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1699
  ‚Ä¢ Validation Loss: 0.3997
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3997, best: 0.3974)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2530
  ‚Ä¢ Validation Loss: 0.3995
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3995, best: 0.3974)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2239
  ‚Ä¢ Validation Loss: 0.3997
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3997, best: 0.3974)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2152
  ‚Ä¢ Validation Loss: 0.4004
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4004, best: 0.3974)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2119
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3965
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2254
  ‚Ä¢ Validation Loss: 0.4000
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4000, best: 0.3965)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2187
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4025, best: 0.3965)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2156
  ‚Ä¢ Validation Loss: 0.4034
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4034, best: 0.3965)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1883
  ‚Ä¢ Validation Loss: 0.3986
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3986, best: 0.3965)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1929
  ‚Ä¢ Validation Loss: 0.3970
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3970, best: 0.3965)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2108
  ‚Ä¢ Validation Loss: 0.3995
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3995, best: 0.3965)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1676
  ‚Ä¢ Validation Loss: 0.4005
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4005, best: 0.3965)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2247
  ‚Ä¢ Validation Loss: 0.3977
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3977, best: 0.3965)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2091
  ‚Ä¢ Validation Loss: 0.4006
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4006, best: 0.3965)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2271
  ‚Ä¢ Validation Loss: 0.3993
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3993, best: 0.3965)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1981
  ‚Ä¢ Validation Loss: 0.3984
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3984, best: 0.3965)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2047
  ‚Ä¢ Validation Loss: 0.3964
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3964
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2338
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3965, best: 0.3964)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2095
  ‚Ä¢ Validation Loss: 0.3943
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3943
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1819
  ‚Ä¢ Validation Loss: 0.3997
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3997, best: 0.3943)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2084
  ‚Ä¢ Validation Loss: 0.3936
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3936
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2356
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2238
  ‚Ä¢ Validation Loss: 0.3962
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3962, best: 0.3921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2017
  ‚Ä¢ Validation Loss: 0.3968
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3968, best: 0.3921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2027
  ‚Ä¢ Validation Loss: 0.4012
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4012, best: 0.3921)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2235
  ‚Ä¢ Validation Loss: 0.3975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3975, best: 0.3921)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1790
  ‚Ä¢ Validation Loss: 0.3951
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3951, best: 0.3921)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1643
  ‚Ä¢ Validation Loss: 0.3964
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3964, best: 0.3921)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2511
  ‚Ä¢ Validation Loss: 0.3986
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3986, best: 0.3921)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2470
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4044, best: 0.3921)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3992, best: 0.3921)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2623
  ‚Ä¢ Validation Loss: 0.4009
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4009, best: 0.3921)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1653
  ‚Ä¢ Validation Loss: 0.4000
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4000, best: 0.3921)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2424
  ‚Ä¢ Validation Loss: 0.3983
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3983, best: 0.3921)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1689
  ‚Ä¢ Validation Loss: 0.4018
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4018, best: 0.3921)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1706
  ‚Ä¢ Validation Loss: 0.4023
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4023, best: 0.3921)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1956
  ‚Ä¢ Validation Loss: 0.3988
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3988, best: 0.3921)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1916
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3980, best: 0.3921)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1903
  ‚Ä¢ Validation Loss: 0.3995
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3995, best: 0.3921)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1849
  ‚Ä¢ Validation Loss: 0.3972
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3972, best: 0.3921)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3921
Total Epochs:   300
Models Saved:   ./Result/a1/Syr341
TensorBoard:    ./Result/a1/Syr341/tensorboard_logs
================================================================================

[23:57:57] Training completed. Best val loss: 0.3921

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9690, Recall=0.9784, F1=0.9737, IoU=0.9487
Paratext            : Precision=0.5845, Recall=0.4471, F1=0.5066, IoU=0.3393
Decoration          : Precision=0.9641, Recall=0.6972, F1=0.8092, IoU=0.6795
Main Text           : Precision=0.8450, Recall=0.8414, F1=0.8432, IoU=0.7289
Title               : Precision=0.5165, Recall=0.2491, F1=0.3361, IoU=0.2020

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7758
Mean Recall:    0.6426
Mean F1-Score:  0.6938
Mean IoU:       0.5797
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8379
Mean Recall:    0.7380
Mean F1-Score:  0.7792
Mean IoU:       0.6692
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Results Location: ./Result/a1/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Sun Nov 16 12:00:31 AM CET 2025
= Job-ID             : 1327366 on tinygpu
= Job-Name           : 1st
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 04:05:20
= Total RAM usage    : 6.8 GiB of requested  GiB (%)   
= Node list          : tg069
= Subm/Elig/Start/End: 2025-11-15T19:54:41 / 2025-11-15T19:54:41 / 2025-11-15T19:54:57 / 2025-11-16T00:00:17
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              65.6G   104.9G   209.7G        N/A     235K     500K   1,000K        N/A    
    /home/woody             0.0K  1000.0G  1500.0G        N/A       1    5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3216087, 63 %, 46 %, 10490 MiB, 3470559 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3227280, 16 %, 12 %, 1870 MiB, 140167 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3227320, 64 %, 47 %, 10490 MiB, 3422570 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3238558, 17 %, 13 %, 1870 MiB, 137004 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3238605, 59 %, 43 %, 10490 MiB, 3771321 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3251872, 17 %, 13 %, 1870 MiB, 138833 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3251919, 65 %, 47 %, 10490 MiB, 3406857 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 3263108, 16 %, 12 %, 1870 MiB, 144853 ms
