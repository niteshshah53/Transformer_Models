### Starting TaskPrologue of job 1394314 on tg06b at Wed Nov 19 04:29:52 AM CET 2025
Running on cores 4-5,12-13,20-21,28-29 with governor ondemand
Wed Nov 19 04:29:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:86:00.0 Off |                  N/A |
| 26%   26C    P8             21W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
============================================================================
Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart (attention-based skip connections)
    - MultiheadAttention-based fusion
    - Attention mechanism selects relevant features
    - Applied at 3 skip connection stages
  ‚úì Deep Supervision: ENABLED (multi-resolution auxiliary outputs)
    - 3 auxiliary outputs at decoder stages
    - Outputs at native resolutions (H/16, H/8, H/4)
    - Multi-resolution loss computation (MSAGHNet-style)
    - Ground truth downsampled to match resolutions
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)
  ‚úì Loss: CB Loss (Class-Balanced, beta=0.9999) + Focal (Œ≥=2.0) + Dice
  ‚úó SE-MSFE: disabled
  ‚úó MSFA+MCT Bottleneck: disabled
  ‚úó GCFF Fusion: disabled
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Fourier Feature Fusion: disabled

Training Parameters:
  - Batch Size: 16
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING BASELINE + DEEP SUPERVISION + SMART: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a1/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 16
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SMART
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 16
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 16
   - Steps per epoch: 34


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=10,613,746
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.0072
  ‚Ä¢ Validation Loss: 0.8881
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8881
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8180
  ‚Ä¢ Validation Loss: 0.7301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7301
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7421
  ‚Ä¢ Validation Loss: 0.6956
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6956
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7283
  ‚Ä¢ Validation Loss: 0.6690
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6690
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6928
  ‚Ä¢ Validation Loss: 0.6610
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6610
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6728
  ‚Ä¢ Validation Loss: 0.6421
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6421
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6494
  ‚Ä¢ Validation Loss: 0.6322
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6322
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6375
  ‚Ä¢ Validation Loss: 0.6202
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6164
  ‚Ä¢ Validation Loss: 0.6118
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5951
  ‚Ä¢ Validation Loss: 0.6068
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6068
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5918
  ‚Ä¢ Validation Loss: 0.5918
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5918
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5784
  ‚Ä¢ Validation Loss: 0.5876
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5876
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5782
  ‚Ä¢ Validation Loss: 0.5787
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5787
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5646
  ‚Ä¢ Validation Loss: 0.5756
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5756
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5645
  ‚Ä¢ Validation Loss: 0.5778
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5778, best: 0.5756)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5600
  ‚Ä¢ Validation Loss: 0.5687
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5687
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5527
  ‚Ä¢ Validation Loss: 0.5745
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5745, best: 0.5687)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5451
  ‚Ä¢ Validation Loss: 0.5695
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5695, best: 0.5687)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5389
  ‚Ä¢ Validation Loss: 0.5602
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5602
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5328
  ‚Ä¢ Validation Loss: 0.5589
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5589
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5262
  ‚Ä¢ Validation Loss: 0.5553
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5553
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5247
  ‚Ä¢ Validation Loss: 0.5560
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5560, best: 0.5553)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5188
  ‚Ä¢ Validation Loss: 0.5522
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5522
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5250
  ‚Ä¢ Validation Loss: 0.5503
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5503
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5063
  ‚Ä¢ Validation Loss: 0.5516
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5516, best: 0.5503)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5157
  ‚Ä¢ Validation Loss: 0.5410
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5410
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5166
  ‚Ä¢ Validation Loss: 0.5483
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5483, best: 0.5410)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5052
  ‚Ä¢ Validation Loss: 0.5408
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5408
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4987
  ‚Ä¢ Validation Loss: 0.5389
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5389
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4942
  ‚Ä¢ Validation Loss: 0.5389
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5389
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4912
  ‚Ä¢ Validation Loss: 0.5294
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5294
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4933
  ‚Ä¢ Validation Loss: 0.5281
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5281
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4978
  ‚Ä¢ Validation Loss: 0.5273
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5273
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4887
  ‚Ä¢ Validation Loss: 0.5239
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5239
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4782
  ‚Ä¢ Validation Loss: 0.5214
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5214
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4837
  ‚Ä¢ Validation Loss: 0.5212
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5212
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4770
  ‚Ä¢ Validation Loss: 0.5243
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5243, best: 0.5212)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4713
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5199
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4827
  ‚Ä¢ Validation Loss: 0.5183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5183
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4702
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5182
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4658
  ‚Ä¢ Validation Loss: 0.5197
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5197, best: 0.5182)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4643
  ‚Ä¢ Validation Loss: 0.5179
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5179
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4661
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5161
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4669
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5157
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4627
  ‚Ä¢ Validation Loss: 0.5155
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5155
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4703
  ‚Ä¢ Validation Loss: 0.5137
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5137
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4563
  ‚Ä¢ Validation Loss: 0.5173
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5173, best: 0.5137)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4675
  ‚Ä¢ Validation Loss: 0.5154
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5154, best: 0.5137)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4700
  ‚Ä¢ Validation Loss: 0.5176
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5176, best: 0.5137)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4797
  ‚Ä¢ Validation Loss: 0.5165
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5165, best: 0.5137)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4876
  ‚Ä¢ Validation Loss: 0.5286
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5286, best: 0.5137)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4911
  ‚Ä¢ Validation Loss: 0.5196
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5196, best: 0.5137)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4703
  ‚Ä¢ Validation Loss: 0.5217
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5217, best: 0.5137)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4798
  ‚Ä¢ Validation Loss: 0.5278
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5278, best: 0.5137)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4666
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5157, best: 0.5137)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4597
  ‚Ä¢ Validation Loss: 0.5169
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5169, best: 0.5137)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4619
  ‚Ä¢ Validation Loss: 0.5132
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5132
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4765
  ‚Ä¢ Validation Loss: 0.5079
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5079
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4559
  ‚Ä¢ Validation Loss: 0.5184
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5184, best: 0.5079)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4528
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4988
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4337
  ‚Ä¢ Validation Loss: 0.5018
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5018, best: 0.4988)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4044
  ‚Ä¢ Validation Loss: 0.4986
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4986
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4352
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4990, best: 0.4986)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3992
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4981
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4201
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4973
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4131
  ‚Ä¢ Validation Loss: 0.4949
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4949
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4236
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4938
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4290
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4954, best: 0.4938)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4050
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4854
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4015
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4891, best: 0.4854)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4136
  ‚Ä¢ Validation Loss: 0.4871
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4871, best: 0.4854)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4046
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4940, best: 0.4854)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3911
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4846
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4204
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4850, best: 0.4846)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4838
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3814
  ‚Ä¢ Validation Loss: 0.4841
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4841, best: 0.4838)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3597
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4837
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4036
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4832
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3631
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4823
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3534
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4803
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3623
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4725
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3887
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4839, best: 0.4725)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3672
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4752, best: 0.4725)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3632
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4815, best: 0.4725)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3650
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3606
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4869, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3739
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4722, best: 0.4684)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3393
  ‚Ä¢ Validation Loss: 0.4664
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4664
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3749
  ‚Ä¢ Validation Loss: 0.4707
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4707, best: 0.4664)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3621
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4725, best: 0.4664)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3464
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4662
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3725
  ‚Ä¢ Validation Loss: 0.4643
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4643
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3512
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4640
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3561
  ‚Ä¢ Validation Loss: 0.4645
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4645, best: 0.4640)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3685
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4646, best: 0.4640)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3672
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3317
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4626, best: 0.4605)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3429
  ‚Ä¢ Validation Loss: 0.4642
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4642, best: 0.4605)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3566
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4603
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3501
  ‚Ä¢ Validation Loss: 0.4619
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4619, best: 0.4603)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3314
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4624, best: 0.4603)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3534
  ‚Ä¢ Validation Loss: 0.4559
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4559
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3444
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4617, best: 0.4559)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3442
  ‚Ä¢ Validation Loss: 0.4604
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4604, best: 0.4559)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3455
  ‚Ä¢ Validation Loss: 0.4560
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4560, best: 0.4559)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3210
  ‚Ä¢ Validation Loss: 0.4558
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3450
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4586, best: 0.4558)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3549
  ‚Ä¢ Validation Loss: 0.4541
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4541
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3402
  ‚Ä¢ Validation Loss: 0.4614
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4614, best: 0.4541)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3531
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4524
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3133
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4575, best: 0.4524)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3273
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4603, best: 0.4524)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3168
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4532, best: 0.4524)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3113
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4573, best: 0.4524)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3357
  ‚Ä¢ Validation Loss: 0.4519
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4519
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3434
  ‚Ä¢ Validation Loss: 0.4556
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4556, best: 0.4519)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3518
  ‚Ä¢ Validation Loss: 0.4531
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4531, best: 0.4519)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3294
  ‚Ä¢ Validation Loss: 0.4562
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4562, best: 0.4519)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3348
  ‚Ä¢ Validation Loss: 0.4514
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4514
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3168
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4511
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3061
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4508
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3078
  ‚Ä¢ Validation Loss: 0.4515
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4515, best: 0.4508)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2412
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4517, best: 0.4508)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2486
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4517, best: 0.4508)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1796
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4526, best: 0.4508)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2211
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4510, best: 0.4508)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1920
  ‚Ä¢ Validation Loss: 0.4519
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4519, best: 0.4508)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2321
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2610
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4510, best: 0.4507)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2289
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4492
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2306
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4507, best: 0.4492)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2231
  ‚Ä¢ Validation Loss: 0.4509
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4509, best: 0.4492)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2058
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4504, best: 0.4492)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1782
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4508, best: 0.4492)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2580
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4517, best: 0.4492)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2104
  ‚Ä¢ Validation Loss: 0.4495
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4495, best: 0.4492)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2124
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4496, best: 0.4492)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1695
  ‚Ä¢ Validation Loss: 0.4495
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4495, best: 0.4492)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2149
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4507, best: 0.4492)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2414
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4500, best: 0.4492)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2884
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4493, best: 0.4492)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1980
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4494, best: 0.4492)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2406
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4500, best: 0.4492)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1941
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4493, best: 0.4492)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2216
  ‚Ä¢ Validation Loss: 0.4499
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4499, best: 0.4492)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2077
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4494, best: 0.4492)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2296
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4493, best: 0.4492)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2360
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4523, best: 0.4492)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2935
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4494, best: 0.4492)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1804
  ‚Ä¢ Validation Loss: 0.4497
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4497, best: 0.4492)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2257
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4702, best: 0.4492)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2061
  ‚Ä¢ Validation Loss: 0.4825
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4825, best: 0.4492)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2121
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4820, best: 0.4492)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1483
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4596, best: 0.4492)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2298
  ‚Ä¢ Validation Loss: 0.4656
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4656, best: 0.4492)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1949
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4608, best: 0.4492)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2100
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4592, best: 0.4492)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1565
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4608, best: 0.4492)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2160
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4586, best: 0.4492)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1540
  ‚Ä¢ Validation Loss: 0.4545
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4545, best: 0.4492)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2283
  ‚Ä¢ Validation Loss: 0.4610
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4610, best: 0.4492)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2563
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4594, best: 0.4492)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2143
  ‚Ä¢ Validation Loss: 0.4545
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4545, best: 0.4492)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1807
  ‚Ä¢ Validation Loss: 0.4637
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4637, best: 0.4492)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1221
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4641, best: 0.4492)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1730
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4524, best: 0.4492)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1894
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4526, best: 0.4492)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1451
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4532, best: 0.4492)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1925
  ‚Ä¢ Validation Loss: 0.4564
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4564, best: 0.4492)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1920
  ‚Ä¢ Validation Loss: 0.4555
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4555, best: 0.4492)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1835
  ‚Ä¢ Validation Loss: 0.4556
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4556, best: 0.4492)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2279
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4508, best: 0.4492)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1344
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4595, best: 0.4492)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2116
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4504, best: 0.4492)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1810
  ‚Ä¢ Validation Loss: 0.4525
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4525, best: 0.4492)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1322
  ‚Ä¢ Validation Loss: 0.4477
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4477
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1670
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4608, best: 0.4477)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1895
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4511, best: 0.4477)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1422
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4511, best: 0.4477)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2108
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4507, best: 0.4477)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1687
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4472
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1307
  ‚Ä¢ Validation Loss: 0.4525
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4525, best: 0.4472)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1997
  ‚Ä¢ Validation Loss: 0.4483
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4483, best: 0.4472)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1493
  ‚Ä¢ Validation Loss: 0.4483
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4483, best: 0.4472)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1808
  ‚Ä¢ Validation Loss: 0.4531
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4531, best: 0.4472)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4485, best: 0.4472)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1360
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4570, best: 0.4472)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2450
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4479, best: 0.4472)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1710
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4524, best: 0.4472)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2070
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4492, best: 0.4472)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1891
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4455
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1562
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4492, best: 0.4455)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1506
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4454
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1238
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4510, best: 0.4454)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2031
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4506, best: 0.4454)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1584
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4570, best: 0.4454)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1390
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4530, best: 0.4454)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1716
  ‚Ä¢ Validation Loss: 0.4528
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4528, best: 0.4454)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2114
  ‚Ä¢ Validation Loss: 0.4452
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4452
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1748
  ‚Ä¢ Validation Loss: 0.4421
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    ‚úì New best checkpoint saved! Val loss: 0.4421
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1469
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4490, best: 0.4421)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1491
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4441, best: 0.4421)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1840
  ‚Ä¢ Validation Loss: 0.4433
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4433, best: 0.4421)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1974
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4442, best: 0.4421)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1955
  ‚Ä¢ Validation Loss: 0.4540
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4540, best: 0.4421)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2238
  ‚Ä¢ Validation Loss: 0.4503
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4503, best: 0.4421)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1658
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4489, best: 0.4421)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1919
  ‚Ä¢ Validation Loss: 0.4434
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4434, best: 0.4421)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1954
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4441, best: 0.4421)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2194
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4447, best: 0.4421)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1631
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4506, best: 0.4421)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1861
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4493, best: 0.4421)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1454
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4416
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1571
  ‚Ä¢ Validation Loss: 0.4482
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4482, best: 0.4416)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2071
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4444, best: 0.4416)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1903
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4416, best: 0.4416)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2019
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4372
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1949
  ‚Ä¢ Validation Loss: 0.4418
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4418, best: 0.4372)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1396
  ‚Ä¢ Validation Loss: 0.4391
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4391, best: 0.4372)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2179
  ‚Ä¢ Validation Loss: 0.4407
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4407, best: 0.4372)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1585
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4478, best: 0.4372)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1503
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4410, best: 0.4372)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2087
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4408, best: 0.4372)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2475
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4460, best: 0.4372)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1182
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4441, best: 0.4372)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1750
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4436, best: 0.4372)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1829
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4397, best: 0.4372)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1996
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4357
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0565
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4426, best: 0.4357)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0356
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4416, best: 0.4357)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0701
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4386, best: 0.4357)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0542
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4390, best: 0.4357)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0455
  ‚Ä¢ Validation Loss: 0.4392
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4392, best: 0.4357)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0672
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4374, best: 0.4357)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0905
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4384, best: 0.4357)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0466
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4372, best: 0.4357)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1018
  ‚Ä¢ Validation Loss: 0.4366
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4366, best: 0.4357)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0805
  ‚Ä¢ Validation Loss: 0.4398
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4398, best: 0.4357)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0835
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4347
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4385, best: 0.4347)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0476
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4376, best: 0.4347)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0935
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4369, best: 0.4347)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0911
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0683
  ‚Ä¢ Validation Loss: 0.4412
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4412, best: 0.4345)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0562
  ‚Ä¢ Validation Loss: 0.4383
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4383, best: 0.4345)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0729
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4376, best: 0.4345)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0750
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4376, best: 0.4345)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0964
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4369, best: 0.4345)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0974
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4386, best: 0.4345)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0539
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4347, best: 0.4345)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0448
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4353, best: 0.4345)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1149
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4363, best: 0.4345)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4355, best: 0.4345)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0432
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4356, best: 0.4345)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0819
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4399, best: 0.4345)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4402
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4402, best: 0.4345)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0681
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4362, best: 0.4345)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0753
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4370, best: 0.4345)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1075
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4354, best: 0.4345)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0745
  ‚Ä¢ Validation Loss: 0.4342
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4342
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0442
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4346, best: 0.4338)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0617
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4341, best: 0.4338)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0355
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4337
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1206
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4337, best: 0.4337)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0852
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4333
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0910
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4367, best: 0.4333)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0784
  ‚Ä¢ Validation Loss: 0.4336
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4336, best: 0.4333)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0846
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4346, best: 0.4333)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0651
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4353, best: 0.4333)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1078
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4347, best: 0.4333)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0733
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4352, best: 0.4333)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1022
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4376, best: 0.4333)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0706
  ‚Ä¢ Validation Loss: 0.4350
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4350, best: 0.4333)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1132
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4348, best: 0.4333)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0646
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4339, best: 0.4333)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4341
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4341, best: 0.4333)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0917
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4374, best: 0.4333)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0848
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4326
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0731
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4332, best: 0.4326)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0822
  ‚Ä¢ Validation Loss: 0.4364
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4364, best: 0.4326)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0792
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4345, best: 0.4326)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0953
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4332, best: 0.4326)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0770
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4333, best: 0.4326)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0992
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4318
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0826
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4337, best: 0.4318)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4357, best: 0.4318)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0295
  ‚Ä¢ Validation Loss: 0.4347
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4347, best: 0.4318)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0627
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4332, best: 0.4318)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0745
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4353, best: 0.4318)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0873
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4337, best: 0.4318)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1098
  ‚Ä¢ Validation Loss: 0.4328
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4328, best: 0.4318)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0351
  ‚Ä¢ Validation Loss: 0.4335
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4335, best: 0.4318)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1210
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4331, best: 0.4318)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0613
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4326, best: 0.4318)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0907
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4340, best: 0.4318)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0485
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4344, best: 0.4318)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0534
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4331, best: 0.4318)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0870
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4323, best: 0.4318)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0785
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4322, best: 0.4318)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4318
Total Epochs:   300
Models Saved:   ./Result/a1/Latin2
TensorBoard:    ./Result/a1/Latin2/tensorboard_logs
================================================================================

[06:09:05] Training completed. Best val loss: 0.4318

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING BASELINE + DEEP SUPERVISION + SMART: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9883, Recall=0.9922, F1=0.9903, IoU=0.9808
Paratext            : Precision=0.7525, Recall=0.6996, F1=0.7251, IoU=0.5687
Decoration          : Precision=0.8715, Recall=0.8700, F1=0.8708, IoU=0.7711
Main Text           : Precision=0.8463, Recall=0.8309, F1=0.8385, IoU=0.7220
Title               : Precision=0.7811, Recall=0.8074, F1=0.7940, IoU=0.6584
Chapter Headings    : Precision=0.7871, Recall=0.3987, F1=0.5293, IoU=0.3599

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8378
Mean Recall:    0.7665
Mean F1-Score:  0.7913
Mean IoU:       0.6768
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING BASELINE + DEEP SUPERVISION + SMART: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a1/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 16
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SMART
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 16
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 16
   - Steps per epoch: 34


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=10,613,746
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.1484
  ‚Ä¢ Validation Loss: 0.9148
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.9148
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8806
  ‚Ä¢ Validation Loss: 0.7078
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8215
  ‚Ä¢ Validation Loss: 0.6766
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6766
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7567
  ‚Ä¢ Validation Loss: 0.6570
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7293
  ‚Ä¢ Validation Loss: 0.6639
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6639, best: 0.6570)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7156
  ‚Ä¢ Validation Loss: 0.6367
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6367
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6974
  ‚Ä¢ Validation Loss: 0.6351
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6351
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6766
  ‚Ä¢ Validation Loss: 0.6058
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6058
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6574
  ‚Ä¢ Validation Loss: 0.5968
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5968
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6302
  ‚Ä¢ Validation Loss: 0.5939
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5939
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6155
  ‚Ä¢ Validation Loss: 0.5835
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5835
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6085
  ‚Ä¢ Validation Loss: 0.5780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5780
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5974
  ‚Ä¢ Validation Loss: 0.5785
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5785, best: 0.5780)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5876
  ‚Ä¢ Validation Loss: 0.5677
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5677
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5765
  ‚Ä¢ Validation Loss: 0.5654
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5882
  ‚Ä¢ Validation Loss: 0.5827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5827, best: 0.5654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5804
  ‚Ä¢ Validation Loss: 0.5688
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5688, best: 0.5654)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5678
  ‚Ä¢ Validation Loss: 0.5620
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5620
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5589
  ‚Ä¢ Validation Loss: 0.5570
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5638
  ‚Ä¢ Validation Loss: 0.5624
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5624, best: 0.5570)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5493
  ‚Ä¢ Validation Loss: 0.5579
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5579, best: 0.5570)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5568
  ‚Ä¢ Validation Loss: 0.5532
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5532
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5403
  ‚Ä¢ Validation Loss: 0.5575
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5575, best: 0.5532)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5437
  ‚Ä¢ Validation Loss: 0.5507
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5360
  ‚Ä¢ Validation Loss: 0.5541
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5541, best: 0.5507)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5301
  ‚Ä¢ Validation Loss: 0.5461
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5461
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5419
  ‚Ä¢ Validation Loss: 0.5502
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5502, best: 0.5461)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5228
  ‚Ä¢ Validation Loss: 0.5470
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5470, best: 0.5461)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5259
  ‚Ä¢ Validation Loss: 0.5432
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5432
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5334
  ‚Ä¢ Validation Loss: 0.5468
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5468, best: 0.5432)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5218
  ‚Ä¢ Validation Loss: 0.5446
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5446, best: 0.5432)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5201
  ‚Ä¢ Validation Loss: 0.5424
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5424
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5309
  ‚Ä¢ Validation Loss: 0.5444
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5444, best: 0.5424)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5163
  ‚Ä¢ Validation Loss: 0.5449
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5449, best: 0.5424)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5146
  ‚Ä¢ Validation Loss: 0.5404
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5404
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5123
  ‚Ä¢ Validation Loss: 0.5362
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5362
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5017
  ‚Ä¢ Validation Loss: 0.5389
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5389, best: 0.5362)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5116
  ‚Ä¢ Validation Loss: 0.5378
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5378, best: 0.5362)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5141
  ‚Ä¢ Validation Loss: 0.5355
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5355
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5076
  ‚Ä¢ Validation Loss: 0.5353
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5353
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4980
  ‚Ä¢ Validation Loss: 0.5379
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5379, best: 0.5353)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4995
  ‚Ä¢ Validation Loss: 0.5336
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5336
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4986
  ‚Ä¢ Validation Loss: 0.5343
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5343, best: 0.5336)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5115
  ‚Ä¢ Validation Loss: 0.5325
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5325
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5190
  ‚Ä¢ Validation Loss: 0.5331
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5331, best: 0.5325)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5053
  ‚Ä¢ Validation Loss: 0.5329
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5329, best: 0.5325)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4968
  ‚Ä¢ Validation Loss: 0.5323
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5323
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5086
  ‚Ä¢ Validation Loss: 0.5325
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5325, best: 0.5323)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5055
  ‚Ä¢ Validation Loss: 0.5329
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5329, best: 0.5323)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5061
  ‚Ä¢ Validation Loss: 0.5324
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5324, best: 0.5323)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5074
  ‚Ä¢ Validation Loss: 0.5403
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5403, best: 0.5323)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5178
  ‚Ä¢ Validation Loss: 0.5471
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5471, best: 0.5323)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5035
  ‚Ä¢ Validation Loss: 0.5396
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5396, best: 0.5323)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5146
  ‚Ä¢ Validation Loss: 0.5301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5301
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5002
  ‚Ä¢ Validation Loss: 0.5345
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5345, best: 0.5301)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5032
  ‚Ä¢ Validation Loss: 0.5243
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5243
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5057
  ‚Ä¢ Validation Loss: 0.5211
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5211
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4713
  ‚Ä¢ Validation Loss: 0.5175
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5175
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4879
  ‚Ä¢ Validation Loss: 0.5129
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5129
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4720
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5199, best: 0.5129)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4766
  ‚Ä¢ Validation Loss: 0.5229
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5229, best: 0.5129)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4735
  ‚Ä¢ Validation Loss: 0.5272
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5272, best: 0.5129)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4560
  ‚Ä¢ Validation Loss: 0.5141
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5141, best: 0.5129)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4404
  ‚Ä¢ Validation Loss: 0.5108
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5108
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4415
  ‚Ä¢ Validation Loss: 0.5088
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5088
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4242
  ‚Ä¢ Validation Loss: 0.5150
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5150, best: 0.5088)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4577
  ‚Ä¢ Validation Loss: 0.5039
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5039
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4174
  ‚Ä¢ Validation Loss: 0.4971
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4971
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3999
  ‚Ä¢ Validation Loss: 0.5008
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5008, best: 0.4971)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4282
  ‚Ä¢ Validation Loss: 0.5072
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5072, best: 0.4971)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3961
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4976, best: 0.4971)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4296
  ‚Ä¢ Validation Loss: 0.4923
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4923
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4162
  ‚Ä¢ Validation Loss: 0.4985
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4985, best: 0.4923)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4055
  ‚Ä¢ Validation Loss: 0.4991
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4991, best: 0.4923)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4251
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4945, best: 0.4923)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4904
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4046
  ‚Ä¢ Validation Loss: 0.4955
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4955, best: 0.4904)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3910
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4990, best: 0.4904)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3841
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4924, best: 0.4904)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3748
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4981, best: 0.4904)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4020
  ‚Ä¢ Validation Loss: 0.4879
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4879
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3798
  ‚Ä¢ Validation Loss: 0.4900
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4900, best: 0.4879)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3784
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4913, best: 0.4879)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3805
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4866
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3404
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4826
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3722
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4855, best: 0.4826)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3479
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4854, best: 0.4826)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3707
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4804
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3762
  ‚Ä¢ Validation Loss: 0.4791
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4791
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3016
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4780
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3137
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4813, best: 0.4780)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3343
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4846, best: 0.4780)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3500
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4760
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3689
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4779, best: 0.4760)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3352
  ‚Ä¢ Validation Loss: 0.4792
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4792, best: 0.4760)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3474
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4772, best: 0.4760)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3273
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4768, best: 0.4760)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3101
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4760
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3380
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4761, best: 0.4760)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3566
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.4747
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3624
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4710
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3721
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4736, best: 0.4710)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3289
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4754, best: 0.4710)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3310
  ‚Ä¢ Validation Loss: 0.4770
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4770, best: 0.4710)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3059
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4750, best: 0.4710)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3544
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4696
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3485
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4701, best: 0.4696)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3276
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4738, best: 0.4696)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3201
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4700, best: 0.4696)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3344
  ‚Ä¢ Validation Loss: 0.4671
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4671
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3185
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4716, best: 0.4671)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3341
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4695, best: 0.4671)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3208
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4660
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2981
  ‚Ä¢ Validation Loss: 0.4661
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4661, best: 0.4660)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3607
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4675, best: 0.4660)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3484
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4659
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2891
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4659, best: 0.4659)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3427
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4657
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3232
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4675, best: 0.4657)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3486
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4704, best: 0.4657)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3364
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4677, best: 0.4657)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 122/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3706
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4684, best: 0.4657)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3249
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4630
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2379
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4634, best: 0.4630)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1875
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4655, best: 0.4630)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2240
  ‚Ä¢ Validation Loss: 0.4650
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4650, best: 0.4630)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2462
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4652, best: 0.4630)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1910
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4634, best: 0.4630)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2054
  ‚Ä¢ Validation Loss: 0.4671
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4671, best: 0.4630)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2126
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4639, best: 0.4630)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.4632
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4632, best: 0.4630)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1894
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4640, best: 0.4630)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2103
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4630
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2108
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4639, best: 0.4630)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4626
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2420
  ‚Ä¢ Validation Loss: 0.4645
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4645, best: 0.4626)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2448
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4625
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2130
  ‚Ä¢ Validation Loss: 0.4632
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4632, best: 0.4625)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2471
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4636, best: 0.4625)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4627, best: 0.4625)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2204
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4624
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1893
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4617
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1908
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4621, best: 0.4617)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2237
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4625, best: 0.4617)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2438
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4617
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2633
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4634, best: 0.4617)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2530
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4621, best: 0.4617)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1808
  ‚Ä¢ Validation Loss: 0.4616
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4616
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2524
  ‚Ä¢ Validation Loss: 0.4620
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4620, best: 0.4616)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2014
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4639, best: 0.4616)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2704
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4700, best: 0.4616)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1840
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4704, best: 0.4616)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1773
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4757, best: 0.4616)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2276
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4616)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1693
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4711, best: 0.4616)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2096
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4690, best: 0.4616)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2090
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4616)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2080
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4746, best: 0.4616)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1697
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4709, best: 0.4616)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2042
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4724, best: 0.4616)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2089
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4691, best: 0.4616)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1901
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4677, best: 0.4616)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1561
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4627, best: 0.4616)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1962
  ‚Ä¢ Validation Loss: 0.4649
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4649, best: 0.4616)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1915
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4688, best: 0.4616)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1857
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4708, best: 0.4616)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2154
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4682, best: 0.4616)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1927
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4709, best: 0.4616)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4639, best: 0.4616)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2019
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4667, best: 0.4616)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2073
  ‚Ä¢ Validation Loss: 0.4612
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4612
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1616
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4629, best: 0.4612)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1523
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4659, best: 0.4612)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4670, best: 0.4612)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1571
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4636, best: 0.4612)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2446
  ‚Ä¢ Validation Loss: 0.4707
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4707, best: 0.4612)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2301
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4641, best: 0.4612)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2410
  ‚Ä¢ Validation Loss: 0.4620
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4620, best: 0.4612)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1876
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4700, best: 0.4612)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2083
  ‚Ä¢ Validation Loss: 0.4614
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4614, best: 0.4612)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1562
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4706, best: 0.4612)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1721
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4677, best: 0.4612)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2545
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4611
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2006
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4581
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2419
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4639, best: 0.4581)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1889
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4565
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2219
  ‚Ä¢ Validation Loss: 0.4579
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4579, best: 0.4565)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2124
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4601, best: 0.4565)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2306
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4617, best: 0.4565)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2760
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4602, best: 0.4565)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1823
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4568, best: 0.4565)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2029
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4571, best: 0.4565)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1478
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4561
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2463
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4561, best: 0.4561)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2339
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4523
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1946
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4593, best: 0.4523)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.4582
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4582, best: 0.4523)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1998
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4573, best: 0.4523)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1970
  ‚Ä¢ Validation Loss: 0.4541
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4541, best: 0.4523)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2577
  ‚Ä¢ Validation Loss: 0.4548
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4548, best: 0.4523)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2144
  ‚Ä¢ Validation Loss: 0.4642
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4642, best: 0.4523)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1860
  ‚Ä¢ Validation Loss: 0.4555
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4555, best: 0.4523)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2029
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4554, best: 0.4523)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1698
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4565, best: 0.4523)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1677
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4526, best: 0.4523)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1882
  ‚Ä¢ Validation Loss: 0.4550
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4550, best: 0.4523)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2392
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4574, best: 0.4523)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2029
  ‚Ä¢ Validation Loss: 0.4543
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4543, best: 0.4523)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2425
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4570, best: 0.4523)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2530
  ‚Ä¢ Validation Loss: 0.4515
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4515
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2054
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4574, best: 0.4515)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1930
  ‚Ä¢ Validation Loss: 0.4550
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4550, best: 0.4515)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2041
  ‚Ä¢ Validation Loss: 0.4528
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4528, best: 0.4515)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1936
  ‚Ä¢ Validation Loss: 0.4549
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4549, best: 0.4515)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2106
  ‚Ä¢ Validation Loss: 0.4521
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4521, best: 0.4515)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2053
  ‚Ä¢ Validation Loss: 0.4519
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4519, best: 0.4515)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2389
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4581, best: 0.4515)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2231
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4516, best: 0.4515)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1779
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4524, best: 0.4515)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2151
  ‚Ä¢ Validation Loss: 0.4522
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4522, best: 0.4515)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2347
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4516, best: 0.4515)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1711
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4561, best: 0.4515)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1283
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4569, best: 0.4515)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0408
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4510
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4511, best: 0.4510)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1063
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4485
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0564
  ‚Ä¢ Validation Loss: 0.4541
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4541, best: 0.4485)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0576
  ‚Ä¢ Validation Loss: 0.4525
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4525, best: 0.4485)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1290
  ‚Ä¢ Validation Loss: 0.4582
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4582, best: 0.4485)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4492, best: 0.4485)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1335
  ‚Ä¢ Validation Loss: 0.4536
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4536, best: 0.4485)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0265
  ‚Ä¢ Validation Loss: 0.4535
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4535, best: 0.4485)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0932
  ‚Ä¢ Validation Loss: 0.4551
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4551, best: 0.4485)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4533
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4533, best: 0.4485)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4513, best: 0.4485)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1106
  ‚Ä¢ Validation Loss: 0.4521
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4521, best: 0.4485)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1029
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4513, best: 0.4485)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1173
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4500, best: 0.4485)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0740
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4504, best: 0.4485)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0717
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4493, best: 0.4485)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1000
  ‚Ä¢ Validation Loss: 0.4522
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4522, best: 0.4485)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1325
  ‚Ä¢ Validation Loss: 0.4497
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4497, best: 0.4485)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1011
  ‚Ä¢ Validation Loss: 0.4503
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4503, best: 0.4485)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1121
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4507, best: 0.4485)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1051
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4478
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0997
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4504, best: 0.4478)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0972
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4460
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1053
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4490, best: 0.4460)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0588
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4479, best: 0.4460)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1353
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4504, best: 0.4460)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0913
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4516, best: 0.4460)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1067
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4489, best: 0.4460)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1459
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4469, best: 0.4460)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1209
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4496, best: 0.4460)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1307
  ‚Ä¢ Validation Loss: 0.4519
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4519, best: 0.4460)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0968
  ‚Ä¢ Validation Loss: 0.4488
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4488, best: 0.4460)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.4498
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4498, best: 0.4460)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4473
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4473, best: 0.4460)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0783
  ‚Ä¢ Validation Loss: 0.4473
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4473, best: 0.4460)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1695
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4491, best: 0.4460)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1419
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4506, best: 0.4460)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0979
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4490, best: 0.4460)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1183
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4485, best: 0.4460)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1073
  ‚Ä¢ Validation Loss: 0.4487
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4487, best: 0.4460)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1125
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4496, best: 0.4460)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0882
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4492, best: 0.4460)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1005
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4500, best: 0.4460)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0877
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4489, best: 0.4460)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1095
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4500, best: 0.4460)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1238
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4490, best: 0.4460)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0673
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4472, best: 0.4460)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0946
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4471, best: 0.4460)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1127
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4478, best: 0.4460)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1508
  ‚Ä¢ Validation Loss: 0.4487
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4487, best: 0.4460)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1220
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4454
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1370
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4504, best: 0.4454)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0758
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4474, best: 0.4454)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1483
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4458, best: 0.4454)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0861
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4462, best: 0.4454)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1343
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4467, best: 0.4454)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4471, best: 0.4454)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1611
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4485, best: 0.4454)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1120
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4489, best: 0.4454)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0839
  ‚Ä¢ Validation Loss: 0.4468
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4468, best: 0.4454)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1330
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4481, best: 0.4454)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0748
  ‚Ä¢ Validation Loss: 0.4453
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4453
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4458, best: 0.4453)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1231
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4462, best: 0.4453)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0636
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4462, best: 0.4453)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0852
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4461, best: 0.4453)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4454, best: 0.4453)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0912
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4461, best: 0.4453)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0950
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4461, best: 0.4453)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0814
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4462, best: 0.4453)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1247
  ‚Ä¢ Validation Loss: 0.4451
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4451
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1625
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4454, best: 0.4451)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0798
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4464, best: 0.4451)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1579
  ‚Ä¢ Validation Loss: 0.4445
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4445
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1005
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4454, best: 0.4445)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0879
  ‚Ä¢ Validation Loss: 0.4448
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4448, best: 0.4445)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4445
Total Epochs:   300
Models Saved:   ./Result/a1/Latin14396
TensorBoard:    ./Result/a1/Latin14396/tensorboard_logs
================================================================================

[07:52:22] Training completed. Best val loss: 0.4445

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING BASELINE + DEEP SUPERVISION + SMART: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9895, Recall=0.9900, F1=0.9898, IoU=0.9797
Paratext            : Precision=0.6814, Recall=0.5144, F1=0.5862, IoU=0.4146
Decoration          : Precision=0.9342, Recall=0.9468, F1=0.9405, IoU=0.8876
Main Text           : Precision=0.8923, Recall=0.8949, F1=0.8936, IoU=0.8077
Title               : Precision=0.8675, Recall=0.8435, F1=0.8554, IoU=0.7473
Chapter Headings    : Precision=0.8721, Recall=0.7675, F1=0.8164, IoU=0.6898

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8728
Mean Recall:    0.8262
Mean F1-Score:  0.8470
Mean IoU:       0.7545
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING BASELINE + DEEP SUPERVISION + SMART: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a1/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 16
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SMART
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 16
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 16
   - Steps per epoch: 34


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=10,613,746
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.3323
  ‚Ä¢ Validation Loss: 0.9067
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.9067
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8708
  ‚Ä¢ Validation Loss: 0.7040
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7040
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8052
  ‚Ä¢ Validation Loss: 0.6691
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6691
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7729
  ‚Ä¢ Validation Loss: 0.6589
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6589
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7441
  ‚Ä¢ Validation Loss: 0.6396
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6396
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7157
  ‚Ä¢ Validation Loss: 0.6247
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6247
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6903
  ‚Ä¢ Validation Loss: 0.6104
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6104
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6666
  ‚Ä¢ Validation Loss: 0.5967
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5967
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6302
  ‚Ä¢ Validation Loss: 0.6024
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6024, best: 0.5967)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6287
  ‚Ä¢ Validation Loss: 0.5780
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5780
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6085
  ‚Ä¢ Validation Loss: 0.5708
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5708
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6039
  ‚Ä¢ Validation Loss: 0.5623
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5623
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5885
  ‚Ä¢ Validation Loss: 0.5521
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5521
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5719
  ‚Ä¢ Validation Loss: 0.5496
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5496
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5692
  ‚Ä¢ Validation Loss: 0.5435
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5435
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5632
  ‚Ä¢ Validation Loss: 0.5414
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5414
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5487
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5440
  ‚Ä¢ Validation Loss: 0.5410
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5410, best: 0.5373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5316
  ‚Ä¢ Validation Loss: 0.5256
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5256
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5303
  ‚Ä¢ Validation Loss: 0.5361
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5361, best: 0.5256)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5228
  ‚Ä¢ Validation Loss: 0.5209
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5209
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5220
  ‚Ä¢ Validation Loss: 0.5243
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5243, best: 0.5209)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5108
  ‚Ä¢ Validation Loss: 0.5151
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5151
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5049
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5161, best: 0.5151)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5018
  ‚Ä¢ Validation Loss: 0.5123
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5123
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5042
  ‚Ä¢ Validation Loss: 0.5103
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5103
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4909
  ‚Ä¢ Validation Loss: 0.5053
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5053
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4938
  ‚Ä¢ Validation Loss: 0.5025
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5025
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4906
  ‚Ä¢ Validation Loss: 0.5047
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5047, best: 0.5025)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4788
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4988
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4783
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4974
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4853
  ‚Ä¢ Validation Loss: 0.5006
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5006, best: 0.4974)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4745
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4956
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4753
  ‚Ä¢ Validation Loss: 0.4943
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4943
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4694
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4952, best: 0.4943)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4659
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4928
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4695
  ‚Ä¢ Validation Loss: 0.4930
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4930, best: 0.4928)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4716
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4924
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4628
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4712
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4916, best: 0.4902)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4718
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4678
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4913, best: 0.4895)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4647
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4904, best: 0.4895)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4710
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4902, best: 0.4895)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4639
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4590
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4901, best: 0.4895)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4657
  ‚Ä¢ Validation Loss: 0.4899
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4899, best: 0.4895)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4685
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4891
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4660
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4884
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4664
  ‚Ä¢ Validation Loss: 0.4892
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4892, best: 0.4884)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4718
  ‚Ä¢ Validation Loss: 0.5013
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5013, best: 0.4884)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4752
  ‚Ä¢ Validation Loss: 0.4970
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4970, best: 0.4884)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4788
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4976, best: 0.4884)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4725
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4894, best: 0.4884)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4565
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4631
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4965, best: 0.4867)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4559
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4509
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4915, best: 0.4867)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4508
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4901, best: 0.4867)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4487
  ‚Ä¢ Validation Loss: 0.4840
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4840
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4397
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4407
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4778
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4423
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4795, best: 0.4778)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4187
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4743
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4170
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4785, best: 0.4743)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4203
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4732
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4364
  ‚Ä¢ Validation Loss: 0.4859
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4859, best: 0.4732)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4197
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4754, best: 0.4732)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3951
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4749, best: 0.4732)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4162
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4756, best: 0.4732)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4089
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4690
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3879
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4668
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3972
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4574
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3919
  ‚Ä¢ Validation Loss: 0.4550
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4550
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3787
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4550)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3985
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4571, best: 0.4550)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4023
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4594, best: 0.4550)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3781
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4573, best: 0.4550)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4578
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4578, best: 0.4550)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3729
  ‚Ä¢ Validation Loss: 0.4547
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3754
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3816
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4490
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3669
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4513, best: 0.4490)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3429
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4480
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3619
  ‚Ä¢ Validation Loss: 0.4453
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4453
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3463
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4469, best: 0.4453)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3564
  ‚Ä¢ Validation Loss: 0.4475
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4475, best: 0.4453)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3473
  ‚Ä¢ Validation Loss: 0.4456
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4456, best: 0.4453)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3517
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4431
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3532
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4426
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3513
  ‚Ä¢ Validation Loss: 0.4484
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4484, best: 0.4426)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3433
  ‚Ä¢ Validation Loss: 0.4465
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4465, best: 0.4426)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3499
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4414
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3593
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4461, best: 0.4414)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3517
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4414
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3464
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4431, best: 0.4414)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 97/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3512
  ‚Ä¢ Validation Loss: 0.4395
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4395
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3212
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4458, best: 0.4395)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3373
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4372
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3373
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4399, best: 0.4372)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3427
  ‚Ä¢ Validation Loss: 0.4394
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4394, best: 0.4372)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3396
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4397, best: 0.4372)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3456
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4389, best: 0.4372)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 104/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3495
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4379, best: 0.4372)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3497
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4384, best: 0.4372)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3230
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4371
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3160
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4362
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3324
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4368, best: 0.4362)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3494
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4372, best: 0.4362)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3438
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4353
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3352
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4352
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3453
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4315
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3331
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4376, best: 0.4315)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3137
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4338, best: 0.4315)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3315
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4352, best: 0.4315)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 116/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3399
  ‚Ä¢ Validation Loss: 0.4339
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4339, best: 0.4315)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3298
  ‚Ä¢ Validation Loss: 0.4324
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4324, best: 0.4315)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 118/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3417
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4322, best: 0.4315)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3291
  ‚Ä¢ Validation Loss: 0.4312
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4312
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2748
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4313, best: 0.4312)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2867
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4315, best: 0.4312)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2613
  ‚Ä¢ Validation Loss: 0.4313
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4313, best: 0.4312)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2417
  ‚Ä¢ Validation Loss: 0.4310
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4310
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2081
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4320, best: 0.4310)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2463
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4333, best: 0.4310)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2737
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4309
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2582
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4321, best: 0.4309)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2512
  ‚Ä¢ Validation Loss: 0.4319
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4319, best: 0.4309)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2604
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4309
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2771
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4301
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2465
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4325, best: 0.4301)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2621
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4299
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4311, best: 0.4299)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2932
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4296
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2750
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4299, best: 0.4296)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2172
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4298, best: 0.4296)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2248
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4298, best: 0.4296)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2398
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4308, best: 0.4296)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2830
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4301, best: 0.4296)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2803
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4290
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2728
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4291, best: 0.4290)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2908
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4295, best: 0.4290)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2668
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4293, best: 0.4290)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2789
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4293, best: 0.4290)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2952
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4291, best: 0.4290)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2896
  ‚Ä¢ Validation Loss: 0.4292
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4292, best: 0.4290)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2743
  ‚Ä¢ Validation Loss: 0.4292
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4292, best: 0.4290)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2584
  ‚Ä¢ Validation Loss: 0.4292
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4292, best: 0.4290)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2921
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4300, best: 0.4290)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2762
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4301, best: 0.4290)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2220
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4370, best: 0.4290)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1988
  ‚Ä¢ Validation Loss: 0.4400
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4400, best: 0.4290)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2097
  ‚Ä¢ Validation Loss: 0.4414
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4414, best: 0.4290)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2473
  ‚Ä¢ Validation Loss: 0.4433
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4433, best: 0.4290)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2864
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4315, best: 0.4290)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2753
  ‚Ä¢ Validation Loss: 0.4329
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4329, best: 0.4290)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2603
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4397, best: 0.4290)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2174
  ‚Ä¢ Validation Loss: 0.4351
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4351, best: 0.4290)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2374
  ‚Ä¢ Validation Loss: 0.4401
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4401, best: 0.4290)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1646
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4410, best: 0.4290)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2490
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4340, best: 0.4290)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4354, best: 0.4290)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2071
  ‚Ä¢ Validation Loss: 0.4364
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4364, best: 0.4290)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2793
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4344, best: 0.4290)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2210
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4397, best: 0.4290)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2332
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4331, best: 0.4290)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2093
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4301, best: 0.4290)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1995
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4327, best: 0.4290)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2299
  ‚Ä¢ Validation Loss: 0.4374
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4374, best: 0.4290)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1779
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4355, best: 0.4290)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4392
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4392, best: 0.4290)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2361
  ‚Ä¢ Validation Loss: 0.4327
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4327, best: 0.4290)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2373
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4308, best: 0.4290)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2396
  ‚Ä¢ Validation Loss: 0.4380
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4380, best: 0.4290)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2572
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4356, best: 0.4290)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2284
  ‚Ä¢ Validation Loss: 0.4287
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4287
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4321, best: 0.4287)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2450
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4300, best: 0.4287)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2218
  ‚Ä¢ Validation Loss: 0.4314
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4314, best: 0.4287)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2925
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2449
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4276
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2594
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4270
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2388
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4228
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2506
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4293, best: 0.4228)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4323, best: 0.4228)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2463
  ‚Ä¢ Validation Loss: 0.4241
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4241, best: 0.4228)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1897
  ‚Ä¢ Validation Loss: 0.4305
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4305, best: 0.4228)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2065
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4332, best: 0.4228)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4367, best: 0.4228)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2131
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4212
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2710
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4228, best: 0.4212)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2253
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4280, best: 0.4212)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2626
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4237, best: 0.4212)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2638
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4245, best: 0.4212)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2482
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4191
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2257
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4277, best: 0.4191)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2079
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4294, best: 0.4191)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1416
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4289, best: 0.4191)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1262
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4237, best: 0.4191)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1277
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4237, best: 0.4191)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0933
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4244, best: 0.4191)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0917
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4211, best: 0.4191)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1031
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4233, best: 0.4191)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.4155
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4155
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0633
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4237, best: 0.4155)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1028
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4263, best: 0.4155)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4234, best: 0.4155)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0908
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4259, best: 0.4155)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1037
  ‚Ä¢ Validation Loss: 0.4200
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4200, best: 0.4155)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0241
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4206, best: 0.4155)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1793
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4194, best: 0.4155)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1088
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4227, best: 0.4155)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0535
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4214, best: 0.4155)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1330
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4233, best: 0.4155)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0538
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4254, best: 0.4155)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1342
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4242, best: 0.4155)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0716
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4239, best: 0.4155)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1024
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4225, best: 0.4155)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0609
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4230, best: 0.4155)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1083
  ‚Ä¢ Validation Loss: 0.4218
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4218, best: 0.4155)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0907
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4221, best: 0.4155)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1197
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4229, best: 0.4155)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1549
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4242, best: 0.4155)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4185, best: 0.4155)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1738
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4193, best: 0.4155)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0961
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4190, best: 0.4155)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1454
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4184, best: 0.4155)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4221, best: 0.4155)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1266
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4196, best: 0.4155)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1323
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4163, best: 0.4155)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.4144
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4144
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1053
  ‚Ä¢ Validation Loss: 0.4186
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4186, best: 0.4144)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1300
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4183, best: 0.4144)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0857
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4189, best: 0.4144)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1362
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4184, best: 0.4144)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0414
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4213, best: 0.4144)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1230
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4177, best: 0.4144)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1184
  ‚Ä¢ Validation Loss: 0.4205
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4205, best: 0.4144)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1115
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4183, best: 0.4144)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1258
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4211, best: 0.4144)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1414
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4132
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1334
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4153, best: 0.4132)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0998
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4161, best: 0.4132)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0925
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4213, best: 0.4132)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4197, best: 0.4132)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0656
  ‚Ä¢ Validation Loss: 0.4154
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4154, best: 0.4132)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0794
  ‚Ä¢ Validation Loss: 0.4155
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4155, best: 0.4132)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1266
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4163, best: 0.4132)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4176, best: 0.4132)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0998
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4190, best: 0.4132)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1244
  ‚Ä¢ Validation Loss: 0.4155
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4155, best: 0.4132)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0754
  ‚Ä¢ Validation Loss: 0.4153
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4153, best: 0.4132)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1038
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4176, best: 0.4132)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1022
  ‚Ä¢ Validation Loss: 0.4157
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4157, best: 0.4132)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1003
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4185, best: 0.4132)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1097
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4193, best: 0.4132)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1269
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4163, best: 0.4132)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1530
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4206, best: 0.4132)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1227
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4152, best: 0.4132)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1310
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4189, best: 0.4132)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1023
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4216, best: 0.4132)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4131
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4131
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1465
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4162, best: 0.4131)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4168, best: 0.4131)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1551
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4160, best: 0.4131)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0779
  ‚Ä¢ Validation Loss: 0.4134
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4134, best: 0.4131)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1096
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4217, best: 0.4131)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0677
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4160, best: 0.4131)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1169
  ‚Ä¢ Validation Loss: 0.4157
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4157, best: 0.4131)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0936
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4193, best: 0.4131)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1145
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4148, best: 0.4131)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0836
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4169, best: 0.4131)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1391
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4146, best: 0.4131)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0879
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4127
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0968
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4117
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0920
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4171, best: 0.4117)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0895
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4194, best: 0.4117)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4150
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4150, best: 0.4117)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1170
  ‚Ä¢ Validation Loss: 0.4181
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4181, best: 0.4117)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1052
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4197, best: 0.4117)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1538
  ‚Ä¢ Validation Loss: 0.4167
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4167, best: 0.4117)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1215
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4140, best: 0.4117)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1299
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4132, best: 0.4117)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1136
  ‚Ä¢ Validation Loss: 0.4137
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4137, best: 0.4117)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1703
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4172, best: 0.4117)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1488
  ‚Ä¢ Validation Loss: 0.4172
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4172, best: 0.4117)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1006
  ‚Ä¢ Validation Loss: 0.4154
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4154, best: 0.4117)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1067
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4163, best: 0.4117)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1109
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4147, best: 0.4117)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1430
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4156, best: 0.4117)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0814
  ‚Ä¢ Validation Loss: 0.4201
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4201, best: 0.4117)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1595
  ‚Ä¢ Validation Loss: 0.4133
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4133, best: 0.4117)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0897
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4152, best: 0.4117)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1271
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4117
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1403
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4140, best: 0.4117)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1323
  ‚Ä¢ Validation Loss: 0.4192
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4192, best: 0.4117)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1469
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4158, best: 0.4117)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1039
  ‚Ä¢ Validation Loss: 0.4125
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4125, best: 0.4117)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4146, best: 0.4117)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1505
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4171, best: 0.4117)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4117
Total Epochs:   300
Models Saved:   ./Result/a1/Latin16746
TensorBoard:    ./Result/a1/Latin16746/tensorboard_logs
================================================================================

[09:28:41] Training completed. Best val loss: 0.4117

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING BASELINE + DEEP SUPERVISION + SMART: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9895, Recall=0.9886, F1=0.9891, IoU=0.9783
Paratext            : Precision=0.7585, Recall=0.8291, F1=0.7922, IoU=0.6560
Decoration          : Precision=0.9745, Recall=0.9157, F1=0.9442, IoU=0.8943
Main Text           : Precision=0.8886, Recall=0.9337, F1=0.9106, IoU=0.8358
Title               : Precision=0.7770, Recall=0.8240, F1=0.7998, IoU=0.6664
Chapter Headings    : Precision=0.9145, Recall=0.7521, F1=0.8254, IoU=0.7027

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8838
Mean Recall:    0.8739
Mean F1-Score:  0.8769
Mean IoU:       0.7889
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING BASELINE + DEEP SUPERVISION + SMART: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a1/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 16
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SMART
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 16
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 16
   - Steps per epoch: 34


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=10,607,630
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a1/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.4573
  ‚Ä¢ Validation Loss: 0.9449
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.9449
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.9077
  ‚Ä¢ Validation Loss: 0.7531
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7531
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7738
  ‚Ä¢ Validation Loss: 0.6471
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6471
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7243
  ‚Ä¢ Validation Loss: 0.6345
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6901
  ‚Ä¢ Validation Loss: 0.6241
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6241
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6900
  ‚Ä¢ Validation Loss: 0.6075
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6075
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6145
  ‚Ä¢ Validation Loss: 0.5987
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5987
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6377
  ‚Ä¢ Validation Loss: 0.5920
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5920
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6249
  ‚Ä¢ Validation Loss: 0.5816
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5816
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5107
  ‚Ä¢ Validation Loss: 0.5794
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5794
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4917
  ‚Ä¢ Validation Loss: 0.5739
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5739
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5332
  ‚Ä¢ Validation Loss: 0.5711
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5711
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4960
  ‚Ä¢ Validation Loss: 0.5680
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4693
  ‚Ä¢ Validation Loss: 0.5618
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5618
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4840
  ‚Ä¢ Validation Loss: 0.5653
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5653, best: 0.5618)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4264
  ‚Ä¢ Validation Loss: 0.5716
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5716, best: 0.5618)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4336
  ‚Ä¢ Validation Loss: 0.5618
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5618, best: 0.5618)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5168
  ‚Ä¢ Validation Loss: 0.5613
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5613
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4346
  ‚Ä¢ Validation Loss: 0.5572
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5572
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4628
  ‚Ä¢ Validation Loss: 0.5576
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5576, best: 0.5572)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4127
  ‚Ä¢ Validation Loss: 0.5555
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5555
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4577
  ‚Ä¢ Validation Loss: 0.5619
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5619, best: 0.5555)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3672
  ‚Ä¢ Validation Loss: 0.5543
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5543
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4764
  ‚Ä¢ Validation Loss: 0.5536
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5536
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4469
  ‚Ä¢ Validation Loss: 0.5579
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5579, best: 0.5536)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4256
  ‚Ä¢ Validation Loss: 0.5499
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5499
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4411
  ‚Ä¢ Validation Loss: 0.5556
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5556, best: 0.5499)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4318
  ‚Ä¢ Validation Loss: 0.5490
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5490
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4305
  ‚Ä¢ Validation Loss: 0.5465
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5465
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4740
  ‚Ä¢ Validation Loss: 0.5433
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5433
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4921
  ‚Ä¢ Validation Loss: 0.5525
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5525, best: 0.5433)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4654
  ‚Ä¢ Validation Loss: 0.5438
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5438, best: 0.5433)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4407
  ‚Ä¢ Validation Loss: 0.5424
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5424
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4596
  ‚Ä¢ Validation Loss: 0.5385
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5385
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4321
  ‚Ä¢ Validation Loss: 0.5424
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5424, best: 0.5385)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4780
  ‚Ä¢ Validation Loss: 0.5401
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5401, best: 0.5385)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4309
  ‚Ä¢ Validation Loss: 0.5398
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5398, best: 0.5385)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4607
  ‚Ä¢ Validation Loss: 0.5383
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5383
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4569
  ‚Ä¢ Validation Loss: 0.5403
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5403, best: 0.5383)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4094
  ‚Ä¢ Validation Loss: 0.5398
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5398, best: 0.5383)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4208
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3913
  ‚Ä¢ Validation Loss: 0.5409
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5409, best: 0.5373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4376
  ‚Ä¢ Validation Loss: 0.5376
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5376, best: 0.5373)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4287
  ‚Ä¢ Validation Loss: 0.5370
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5370
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4708
  ‚Ä¢ Validation Loss: 0.5379
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5379, best: 0.5370)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4414
  ‚Ä¢ Validation Loss: 0.5359
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5359
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4626
  ‚Ä¢ Validation Loss: 0.5363
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5363, best: 0.5359)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4506
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5364, best: 0.5359)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4661
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5373, best: 0.5359)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4368
  ‚Ä¢ Validation Loss: 0.5372
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5372, best: 0.5359)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4439
  ‚Ä¢ Validation Loss: 0.5586
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5586, best: 0.5359)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4578
  ‚Ä¢ Validation Loss: 0.5446
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5446, best: 0.5359)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4182
  ‚Ä¢ Validation Loss: 0.5512
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5512, best: 0.5359)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4265
  ‚Ä¢ Validation Loss: 0.5490
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5490, best: 0.5359)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4543
  ‚Ä¢ Validation Loss: 0.5374
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5374, best: 0.5359)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4666
  ‚Ä¢ Validation Loss: 0.5430
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5430, best: 0.5359)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4599
  ‚Ä¢ Validation Loss: 0.5423
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5423, best: 0.5359)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4068
  ‚Ä¢ Validation Loss: 0.5448
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5448, best: 0.5359)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4665
  ‚Ä¢ Validation Loss: 0.5400
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5400, best: 0.5359)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4528
  ‚Ä¢ Validation Loss: 0.5355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5355
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4123
  ‚Ä¢ Validation Loss: 0.5279
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5279
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3885
  ‚Ä¢ Validation Loss: 0.5385
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5385, best: 0.5279)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3700
  ‚Ä¢ Validation Loss: 0.5298
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5298, best: 0.5279)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4145
  ‚Ä¢ Validation Loss: 0.5277
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4205
  ‚Ä¢ Validation Loss: 0.5247
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5247
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3924
  ‚Ä¢ Validation Loss: 0.5244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5244
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4154
  ‚Ä¢ Validation Loss: 0.5225
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5225
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3405
  ‚Ä¢ Validation Loss: 0.5252
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5252, best: 0.5225)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3494
  ‚Ä¢ Validation Loss: 0.5195
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5195
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3533
  ‚Ä¢ Validation Loss: 0.5217
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5217, best: 0.5195)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3807
  ‚Ä¢ Validation Loss: 0.5206
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5206, best: 0.5195)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3610
  ‚Ä¢ Validation Loss: 0.5151
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5151
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3399
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5182, best: 0.5151)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2876
  ‚Ä¢ Validation Loss: 0.5227
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5227, best: 0.5151)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3275
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5182, best: 0.5151)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3533
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5161, best: 0.5151)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3637
  ‚Ä¢ Validation Loss: 0.5138
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5138
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.5090
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5090
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3114
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5056
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3684
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5094, best: 0.5056)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3539
  ‚Ä¢ Validation Loss: 0.5229
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5229, best: 0.5056)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3202
  ‚Ä¢ Validation Loss: 0.5144
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5144, best: 0.5056)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3165
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5094, best: 0.5056)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5076, best: 0.5056)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3850
  ‚Ä¢ Validation Loss: 0.5041
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5041
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3711
  ‚Ä¢ Validation Loss: 0.5119
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5119, best: 0.5041)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3477
  ‚Ä¢ Validation Loss: 0.5100
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5100, best: 0.5041)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3784
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5056, best: 0.5041)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3612
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5058, best: 0.5041)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2961
  ‚Ä¢ Validation Loss: 0.5029
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5029
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3685
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5007
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3271
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5010, best: 0.5007)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3507
  ‚Ä¢ Validation Loss: 0.5021
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5021, best: 0.5007)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3001
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5032, best: 0.5007)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3271
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4980
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3119
  ‚Ä¢ Validation Loss: 0.5002
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5002, best: 0.4980)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3351
  ‚Ä¢ Validation Loss: 0.5035
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5035, best: 0.4980)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3318
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4969
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3744
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4973, best: 0.4969)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3203
  ‚Ä¢ Validation Loss: 0.4949
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.4949
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3240
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4945
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2778
  ‚Ä¢ Validation Loss: 0.4991
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4991, best: 0.4945)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3809
  ‚Ä¢ Validation Loss: 0.4963
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4963, best: 0.4945)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3267
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5001, best: 0.4945)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3525
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3806
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4973, best: 0.4942)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3454
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4967, best: 0.4942)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3454
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4952, best: 0.4942)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3089
  ‚Ä¢ Validation Loss: 0.4948
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4948, best: 0.4942)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3477
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4940
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3587
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4960, best: 0.4940)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3412
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4933
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3169
  ‚Ä¢ Validation Loss: 0.4936
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4936, best: 0.4933)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2878
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4942, best: 0.4933)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2881
  ‚Ä¢ Validation Loss: 0.4923
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4923
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3488
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4909
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3848
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3360
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4901
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3457
  ‚Ä¢ Validation Loss: 0.4937
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4937, best: 0.4901)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3825
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4914, best: 0.4901)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3406
  ‚Ä¢ Validation Loss: 0.4905
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4905, best: 0.4901)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2636
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4921, best: 0.4901)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3456
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4914, best: 0.4901)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3288
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3717
  ‚Ä¢ Validation Loss: 0.4919
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4919, best: 0.4895)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3371
  ‚Ä¢ Validation Loss: 0.4905
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4905, best: 0.4895)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3487
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4922, best: 0.4895)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3365
  ‚Ä¢ Validation Loss: 0.4908
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4908, best: 0.4895)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3103
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4903, best: 0.4895)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3487
  ‚Ä¢ Validation Loss: 0.4899
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4899, best: 0.4895)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3191
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4884
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3384
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4881
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3780
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4903, best: 0.4881)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3352
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4883, best: 0.4881)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3240
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4903, best: 0.4881)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3600
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4890, best: 0.4881)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2126
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4894, best: 0.4881)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3066
  ‚Ä¢ Validation Loss: 0.4885
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4885, best: 0.4881)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4887, best: 0.4881)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2093
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4895, best: 0.4881)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2657
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4889, best: 0.4881)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2646
  ‚Ä¢ Validation Loss: 0.4897
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4897, best: 0.4881)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2611
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4886, best: 0.4881)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2880
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4883, best: 0.4881)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.4892
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4892, best: 0.4881)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2817
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4883, best: 0.4881)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2528
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4895, best: 0.4881)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2752
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4887, best: 0.4881)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4884, best: 0.4881)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1650
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4881
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2104
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4993, best: 0.4881)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3213
  ‚Ä¢ Validation Loss: 0.5053
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5053, best: 0.4881)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1835
  ‚Ä¢ Validation Loss: 0.4920
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4920, best: 0.4881)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3084
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4918, best: 0.4881)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3183
  ‚Ä¢ Validation Loss: 0.4977
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4977, best: 0.4881)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2100
  ‚Ä¢ Validation Loss: 0.5073
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5073, best: 0.4881)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2519
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4978, best: 0.4881)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1644
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4981, best: 0.4881)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2641
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4947, best: 0.4881)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2521
  ‚Ä¢ Validation Loss: 0.4962
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4962, best: 0.4881)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2333
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4954, best: 0.4881)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2411
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4960, best: 0.4881)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2024
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4969, best: 0.4881)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2358
  ‚Ä¢ Validation Loss: 0.4944
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4944, best: 0.4881)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1719
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4921, best: 0.4881)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2591
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4881)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2428
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4953, best: 0.4881)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2482
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4956, best: 0.4881)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2990
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4960, best: 0.4881)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2371
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2223
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4903, best: 0.4867)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2800
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4915, best: 0.4867)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2601
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4881, best: 0.4867)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2072
  ‚Ä¢ Validation Loss: 0.5043
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5043, best: 0.4867)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2659
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4927, best: 0.4867)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2621
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4927, best: 0.4867)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2671
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4856
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2456
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4890, best: 0.4856)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2678
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4981, best: 0.4856)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2575
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4846
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2006
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4855, best: 0.4846)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2358
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4993, best: 0.4846)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2033
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4894, best: 0.4846)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2095
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4909, best: 0.4846)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2146
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4866, best: 0.4846)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2714
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4886, best: 0.4846)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2224
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4832
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2477
  ‚Ä¢ Validation Loss: 0.4819
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4819
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2399
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4854, best: 0.4819)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2150
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4819)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1992
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4876, best: 0.4819)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2380
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4881, best: 0.4819)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2380
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4869, best: 0.4819)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1981
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4940, best: 0.4819)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2992
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4820, best: 0.4819)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2555
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4813
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2697
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4800
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.4822
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4822, best: 0.4800)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2289
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4883, best: 0.4800)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2837
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4809, best: 0.4800)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2167
  ‚Ä¢ Validation Loss: 0.4798
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4798
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2136
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4766
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2319
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4793, best: 0.4766)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2593
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4802, best: 0.4766)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2780
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4839, best: 0.4766)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1943
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4854, best: 0.4766)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2487
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4809, best: 0.4766)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1798
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4780, best: 0.4766)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3071
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4772, best: 0.4766)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2080
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4778, best: 0.4766)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2766
  ‚Ä¢ Validation Loss: 0.4783
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4783, best: 0.4766)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2471
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4815, best: 0.4766)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1858
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4768, best: 0.4766)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2346
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4782, best: 0.4766)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2672
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4771, best: 0.4766)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2817
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2456
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4785, best: 0.4741)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1891
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4797, best: 0.4741)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2457
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4764, best: 0.4741)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2558
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4808, best: 0.4741)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1910
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4803, best: 0.4741)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2552
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4789, best: 0.4741)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2243
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4802, best: 0.4741)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2202
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4750, best: 0.4741)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2312
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4827, best: 0.4741)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1179
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4815, best: 0.4741)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4814, best: 0.4741)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0820
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4809, best: 0.4741)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1165
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4754, best: 0.4741)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0741
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4765, best: 0.4741)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0668
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4768, best: 0.4741)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0443
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4778, best: 0.4741)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0583
  ‚Ä¢ Validation Loss: 0.4810
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4810, best: 0.4741)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0187
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4793, best: 0.4741)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4752, best: 0.4741)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1209
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4753, best: 0.4741)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1233
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4782, best: 0.4741)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0745
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4789, best: 0.4741)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0914
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4782, best: 0.4741)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4771, best: 0.4741)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0301
  ‚Ä¢ Validation Loss: 0.4791
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4791, best: 0.4741)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0315
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1013
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4745, best: 0.4718)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1000
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4757, best: 0.4718)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0739
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4803, best: 0.4718)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0613
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4760, best: 0.4718)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0769
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4729, best: 0.4718)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0817
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4711
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0792
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4726, best: 0.4711)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4730, best: 0.4711)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0406
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4733, best: 0.4711)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0491
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4773, best: 0.4711)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0705
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4772, best: 0.4711)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1023
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4742, best: 0.4711)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4732, best: 0.4711)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0537
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4745, best: 0.4711)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0741
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4782, best: 0.4711)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1323
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4754, best: 0.4711)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0688
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4733, best: 0.4711)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1136
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4745, best: 0.4711)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0964
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4711)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0614
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4711)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0973
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4729, best: 0.4711)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0747
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4706
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4706)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0878
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4732, best: 0.4706)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0542
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4739, best: 0.4706)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0650
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4763, best: 0.4706)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0469
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4744, best: 0.4706)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4742, best: 0.4706)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1022
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4747, best: 0.4706)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0678
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4714, best: 0.4706)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0352
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4702
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4731, best: 0.4702)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1167
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4724, best: 0.4702)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0750
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4723, best: 0.4702)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1152
  ‚Ä¢ Validation Loss: 0.4720
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4720, best: 0.4702)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0786
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4723, best: 0.4702)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1211
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4717, best: 0.4702)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0648
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4713, best: 0.4702)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0913
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4745, best: 0.4702)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1010
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4698
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0725
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1290
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4713, best: 0.4680)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0370
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4680)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0941
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4715, best: 0.4680)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1199
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4709, best: 0.4680)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0549
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4680)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1242
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4741, best: 0.4680)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0616
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4713, best: 0.4680)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0594
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4705, best: 0.4680)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0701
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4725, best: 0.4680)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0979
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4701, best: 0.4680)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0525
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4724, best: 0.4680)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0543
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4704, best: 0.4680)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4721, best: 0.4680)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1017
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4706, best: 0.4680)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0614
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4680)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0991
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4680)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0749
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4723, best: 0.4680)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4680
Total Epochs:   300
Models Saved:   ./Result/a1/Syr341
TensorBoard:    ./Result/a1/Syr341/tensorboard_logs
================================================================================

[11:07:08] Training completed. Best val loss: 0.4680

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING BASELINE + DEEP SUPERVISION + SMART: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: smart
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9683, Recall=0.9795, F1=0.9739, IoU=0.9491
Paratext            : Precision=0.4860, Recall=0.3799, F1=0.4264, IoU=0.2710
Decoration          : Precision=0.9284, Recall=0.6720, F1=0.7797, IoU=0.6389
Main Text           : Precision=0.8534, Recall=0.8380, F1=0.8456, IoU=0.7326
Title               : Precision=0.3412, Recall=0.1803, F1=0.2359, IoU=0.1337

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7155
Mean Recall:    0.6099
Mean F1-Score:  0.6523
Mean IoU:       0.5451
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8275
Mean Recall:    0.7691
Mean F1-Score:  0.7919
Mean IoU:       0.6913
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Results Location: ./Result/a1/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Wed Nov 19 11:12:03 AM CET 2025
= Job-ID             : 1394314 on tinygpu
= Job-Name           : baseline_ds_smart
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 06:42:14
= Total RAM usage    : 6.7 GiB of requested  GiB (%)   
= Node list          : tg06b
= Subm/Elig/Start/End: 2025-11-19T04:29:03 / 2025-11-19T04:29:03 / 2025-11-19T04:29:30 / 2025-11-19T11:11:44
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              84.7G   104.9G   209.7G        N/A     238K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3553774, 64 %, 50 %, 10816 MiB, 5892931 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3578860, 20 %, 13 %, 1218 MiB, 183415 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3579542, 68 %, 50 %, 10816 MiB, 5912055 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3600098, 21 %, 14 %, 1218 MiB, 160440 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3600143, 68 %, 53 %, 10816 MiB, 5526096 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3612290, 22 %, 14 %, 1218 MiB, 198658 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3612341, 69 %, 53 %, 10816 MiB, 5560343 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 3689516, 22 %, 14 %, 1218 MiB, 199636 ms
