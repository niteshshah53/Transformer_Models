### Starting TaskPrologue of job 1327314 on tg066 at Sat Nov 15 06:08:57 PM CET 2025
Running on cores 4-5,12-13,16,21,28-29 with governor ondemand
Sat Nov 15 18:08:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:86:00.0 Off |                  N/A |
| 27%   27C    P8             13W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE NETWORK MODEL
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)

Component Details:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: 2 Swin Transformer blocks (enabled)
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: simple (concatenation)
  âœ“ Adapter mode: streaming (integrated)
  âœ“ GroupNorm: enabled
  âœ— Deep Supervision: disabled (base model)
  âœ— Multi-Scale Aggregation: disabled (base model)
  âœ— Fourier Feature Fusion: disabled (using simple fusion)
  âœ— Smart Skip Connections: disabled (using simple fusion)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING CNN-TRANSFORMER BASE MODEL: Latin2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: simple
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Disabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ—
  â€¢ Fusion Method: SIMPLE
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin2
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

âœ“ Loss functions created: CE (weighted), Focal (Î³=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a1/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin2/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a1/Latin2/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âœ“ Loaded optimizer state
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 289
   âœ“ Best validation loss: 0.4094
   âœ“ Resuming from epoch 290

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
Resuming from epoch: 290
================================================================================


EPOCH 291/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0435
  â€¢ Validation Loss: 0.3708
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3708
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 292/300
--------------------------------------------------
  âš ï¸  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  â€¢ Train Loss: inf
  â€¢ Validation Loss: 0.3707
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3707
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 293/300
--------------------------------------------------
  âš ï¸  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  â€¢ Train Loss: inf
  â€¢ Validation Loss: 0.3703
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3703
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 294/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0173
  â€¢ Validation Loss: 0.3707
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3707, best: 0.3703)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 295/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0133
  â€¢ Validation Loss: 0.3709
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3709, best: 0.3703)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 296/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0142
  â€¢ Validation Loss: 0.3704
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3704, best: 0.3703)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 297/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0148
  â€¢ Validation Loss: 0.3701
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3701
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 298/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0522
  â€¢ Validation Loss: 0.3710
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3710, best: 0.3701)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 299/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0465
  â€¢ Validation Loss: 0.3711
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3711, best: 0.3701)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 300/300
--------------------------------------------------
  âš ï¸  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0337
  â€¢ Validation Loss: 0.3705
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   ğŸ’¾ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3705, best: 0.3701)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3701
Total Epochs:   300
Models Saved:   ./Result/a1/Latin2
TensorBoard:    ./Result/a1/Latin2/tensorboard_logs
================================================================================

[18:12:18] Training completed. Best val loss: 0.3701

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TRAINING COMPLETED: Latin2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proceeding to testing...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TESTING CNN-TRANSFORMER BASE MODEL: Latin2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Configuration:
  âœ“ Test-Time Augmentation (TTA): ENABLED
  âœ— CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): âœ“ ENABLED
  â†’ Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90Â°
  â†’ Averaging predictions across all augmentations
CRF POST-PROCESSING: âœ— DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
âœ“ Ground truth found for 076
âœ“ Completed: 076
Processing: 079 (54 patches)
âœ“ Ground truth found for 079
âœ“ Completed: 079
Processing: 082 (54 patches)
âœ“ Ground truth found for 082
âœ“ Completed: 082
Processing: 095 (54 patches)
âœ“ Ground truth found for 095
âœ“ Completed: 095
Processing: 106 (54 patches)
âœ“ Ground truth found for 106
âœ“ Completed: 106
Processing: 111 (54 patches)
âœ“ Ground truth found for 111
âœ“ Completed: 111
Processing: 115 (54 patches)
âœ“ Ground truth found for 115
âœ“ Completed: 115
Processing: 117 (54 patches)
âœ“ Ground truth found for 117
âœ“ Completed: 117
Processing: 128 (54 patches)
âœ“ Ground truth found for 128
âœ“ Completed: 128
Processing: 134 (54 patches)
âœ“ Ground truth found for 134
âœ“ Completed: 134
Processing: 138 (54 patches)
âœ“ Ground truth found for 138
âœ“ Completed: 138
Processing: 142 (54 patches)
âœ“ Ground truth found for 142
âœ“ Completed: 142
Processing: 159 (54 patches)
âœ“ Ground truth found for 159
âœ“ Completed: 159
Processing: 166 (54 patches)
âœ“ Ground truth found for 166
âœ“ Completed: 166
Processing: 185 (54 patches)
âœ“ Ground truth found for 185
âœ“ Completed: 185
Processing: 200 (54 patches)
âœ“ Ground truth found for 200
âœ“ Completed: 200
Processing: 203 (54 patches)
âœ“ Ground truth found for 203
âœ“ Completed: 203
Processing: 208 (54 patches)
âœ“ Ground truth found for 208
âœ“ Completed: 208
Processing: 229 (54 patches)
âœ“ Ground truth found for 229
âœ“ Completed: 229
Processing: 230 (54 patches)
âœ“ Ground truth found for 230
âœ“ Completed: 230
Processing: 235 (54 patches)
âœ“ Ground truth found for 235
âœ“ Completed: 235
Processing: 236 (54 patches)
âœ“ Ground truth found for 236
âœ“ Completed: 236
Processing: 248 (54 patches)
âœ“ Ground truth found for 248
âœ“ Completed: 248
Processing: 249 (54 patches)
âœ“ Ground truth found for 249
âœ“ Completed: 249
Processing: 250 (54 patches)
âœ“ Ground truth found for 250
âœ“ Completed: 250
Processing: 251 (54 patches)
âœ“ Ground truth found for 251
âœ“ Completed: 251
Processing: 252 (54 patches)
âœ“ Ground truth found for 252
âœ“ Completed: 252
Processing: 275 (54 patches)
âœ“ Ground truth found for 275
âœ“ Completed: 275
Processing: 277 (54 patches)
âœ“ Ground truth found for 277
âœ“ Completed: 277
Processing: 297 (54 patches)
âœ“ Ground truth found for 297
âœ“ Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9875, Recall=0.9922, F1=0.9898, IoU=0.9799
Paratext            : Precision=0.7008, Recall=0.6545, F1=0.6769, IoU=0.5116
Decoration          : Precision=0.8691, Recall=0.8983, F1=0.8834, IoU=0.7912
Main Text           : Precision=0.8124, Recall=0.8131, F1=0.8128, IoU=0.6846
Title               : Precision=0.8347, Recall=0.7899, F1=0.8117, IoU=0.6830
Chapter Headings    : Precision=0.5694, Recall=0.1104, F1=0.1849, IoU=0.1019

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7956
Mean Recall:    0.7097
Mean F1-Score:  0.7266
Mean IoU:       0.6254
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TESTING COMPLETED: Latin2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING CNN-TRANSFORMER BASE MODEL: Latin14396
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: simple
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Disabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ—
  â€¢ Fusion Method: SIMPLE
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin14396
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

âœ“ Loss functions created: CE (weighted), Focal (Î³=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a1/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin14396/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a1/Latin14396/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âœ“ Loaded optimizer state
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 276
   âœ“ Best validation loss: 0.4110
   âœ“ Resuming from epoch 277

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
Resuming from epoch: 277
================================================================================


EPOCH 278/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0590
  â€¢ Validation Loss: 0.3758
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3758
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 279/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0480
  â€¢ Validation Loss: 0.3755
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3755
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0554
  â€¢ Validation Loss: 0.3735
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3735
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 281/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0792
  â€¢ Validation Loss: 0.3733
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3733
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 282/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0145
  â€¢ Validation Loss: 0.3736
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3736, best: 0.3733)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 283/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0762
  â€¢ Validation Loss: 0.3747
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3747, best: 0.3733)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 284/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0470
  â€¢ Validation Loss: 0.3747
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3747, best: 0.3733)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 285/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0507
  â€¢ Validation Loss: 0.3746
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3746, best: 0.3733)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 286/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0859
  â€¢ Validation Loss: 0.3749
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3749, best: 0.3733)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 287/300
--------------------------------------------------
  âš ï¸  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0305
  â€¢ Validation Loss: 0.3746
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3746, best: 0.3733)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 288/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1172
  â€¢ Validation Loss: 0.3749
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3749, best: 0.3733)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 289/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0815
  â€¢ Validation Loss: 0.3769
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3769, best: 0.3733)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 290/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0698
  â€¢ Validation Loss: 0.3772
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3772, best: 0.3733)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 291/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0997
  â€¢ Validation Loss: 0.3780
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3780, best: 0.3733)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 292/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0681
  â€¢ Validation Loss: 0.3780
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3780, best: 0.3733)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 293/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0721
  â€¢ Validation Loss: 0.3777
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3777, best: 0.3733)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 294/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0141
  â€¢ Validation Loss: 0.3772
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3772, best: 0.3733)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 295/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0187
  â€¢ Validation Loss: 0.3765
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3765, best: 0.3733)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 296/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0803
  â€¢ Validation Loss: 0.3751
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3751, best: 0.3733)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 297/300
--------------------------------------------------
  âš ï¸  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0272
  â€¢ Validation Loss: 0.3747
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3747, best: 0.3733)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 298/300
--------------------------------------------------
  âš ï¸  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0166
  â€¢ Validation Loss: 0.3744
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3744, best: 0.3733)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 299/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0560
  â€¢ Validation Loss: 0.3737
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3737, best: 0.3733)
    âš  No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 300/300
--------------------------------------------------
  âš ï¸  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0457
  â€¢ Validation Loss: 0.3731
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   ğŸ’¾ Periodic checkpoint: epoch_300.pth
    âœ“ New best checkpoint saved! Val loss: 0.3731
    âœ“ Improvement detected! Resetting patience counter.

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3731
Total Epochs:   300
Models Saved:   ./Result/a1/Latin14396
TensorBoard:    ./Result/a1/Latin14396/tensorboard_logs
================================================================================

[18:19:22] Training completed. Best val loss: 0.3731

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TRAINING COMPLETED: Latin14396
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proceeding to testing...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TESTING CNN-TRANSFORMER BASE MODEL: Latin14396
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Configuration:
  âœ“ Test-Time Augmentation (TTA): ENABLED
  âœ— CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): âœ“ ENABLED
  â†’ Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90Â°
  â†’ Averaging predictions across all augmentations
CRF POST-PROCESSING: âœ— DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
âœ“ Ground truth found for 014
âœ“ Completed: 014
Processing: 032 (54 patches)
âœ“ Ground truth found for 032
âœ“ Completed: 032
Processing: 034 (54 patches)
âœ“ Ground truth found for 034
âœ“ Completed: 034
Processing: 036 (54 patches)
âœ“ Ground truth found for 036
âœ“ Completed: 036
Processing: 038 (54 patches)
âœ“ Ground truth found for 038
âœ“ Completed: 038
Processing: 047 (54 patches)
âœ“ Ground truth found for 047
âœ“ Completed: 047
Processing: 060 (54 patches)
âœ“ Ground truth found for 060
âœ“ Completed: 060
Processing: 085 (54 patches)
âœ“ Ground truth found for 085
âœ“ Completed: 085
Processing: 087 (54 patches)
âœ“ Ground truth found for 087
âœ“ Completed: 087
Processing: 104 (54 patches)
âœ“ Ground truth found for 104
âœ“ Completed: 104
Processing: 105 (54 patches)
âœ“ Ground truth found for 105
âœ“ Completed: 105
Processing: 108 (54 patches)
âœ“ Ground truth found for 108
âœ“ Completed: 108
Processing: 110 (54 patches)
âœ“ Ground truth found for 110
âœ“ Completed: 110
Processing: 136 (54 patches)
âœ“ Ground truth found for 136
âœ“ Completed: 136
Processing: 169 (54 patches)
âœ“ Ground truth found for 169
âœ“ Completed: 169
Processing: 195 (54 patches)
âœ“ Ground truth found for 195
âœ“ Completed: 195
Processing: 196 (54 patches)
âœ“ Ground truth found for 196
âœ“ Completed: 196
Processing: 198 (54 patches)
âœ“ Ground truth found for 198
âœ“ Completed: 198
Processing: 204 (54 patches)
âœ“ Ground truth found for 204
âœ“ Completed: 204
Processing: 223 (54 patches)
âœ“ Ground truth found for 223
âœ“ Completed: 223
Processing: 225 (54 patches)
âœ“ Ground truth found for 225
âœ“ Completed: 225
Processing: 227 (54 patches)
âœ“ Ground truth found for 227
âœ“ Completed: 227
Processing: 229 (54 patches)
âœ“ Ground truth found for 229
âœ“ Completed: 229
Processing: 251 (54 patches)
âœ“ Ground truth found for 251
âœ“ Completed: 251
Processing: 253 (54 patches)
âœ“ Ground truth found for 253
âœ“ Completed: 253
Processing: 255 (54 patches)
âœ“ Ground truth found for 255
âœ“ Completed: 255
Processing: 264 (54 patches)
âœ“ Ground truth found for 264
âœ“ Completed: 264
Processing: 270 (54 patches)
âœ“ Ground truth found for 270
âœ“ Completed: 270
Processing: 276 (54 patches)
âœ“ Ground truth found for 276
âœ“ Completed: 276
Processing: 325 (54 patches)
âœ“ Ground truth found for 325
âœ“ Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9852, Recall=0.9924, F1=0.9888, IoU=0.9778
Paratext            : Precision=0.6148, Recall=0.4970, F1=0.5497, IoU=0.3790
Decoration          : Precision=0.9147, Recall=0.9362, F1=0.9253, IoU=0.8611
Main Text           : Precision=0.9160, Recall=0.8397, F1=0.8762, IoU=0.7797
Title               : Precision=0.7636, Recall=0.8223, F1=0.7918, IoU=0.6554
Chapter Headings    : Precision=0.8398, Recall=0.7082, F1=0.7684, IoU=0.6239

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8390
Mean Recall:    0.7993
Mean F1-Score:  0.8167
Mean IoU:       0.7128
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TESTING COMPLETED: Latin14396
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING CNN-TRANSFORMER BASE MODEL: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: simple
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Disabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ—
  â€¢ Fusion Method: SIMPLE
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Latin16746
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

âœ“ Loss functions created: CE (weighted), Focal (Î³=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,496
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a1/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Latin16746/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a1/Latin16746/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âœ“ Loaded optimizer state
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 137
   âœ“ Best validation loss: 0.3906
   âœ“ Resuming from epoch 138

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
Resuming from epoch: 138
================================================================================


EPOCH 139/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1756
  â€¢ Validation Loss: 0.3546
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.3546
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 140/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1764
  â€¢ Validation Loss: 0.3548
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3548, best: 0.3546)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 141/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1533
  â€¢ Validation Loss: 0.3549
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3549, best: 0.3546)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 142/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1149
  â€¢ Validation Loss: 0.3547
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3547, best: 0.3546)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 143/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1556
  â€¢ Validation Loss: 0.3547
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3547, best: 0.3546)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 144/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1633
  â€¢ Validation Loss: 0.3548
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3548, best: 0.3546)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 145/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2020
  â€¢ Validation Loss: 0.3545
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.3545
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 146/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2285
  â€¢ Validation Loss: 0.3544
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.3544
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 147/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1373
  â€¢ Validation Loss: 0.3550
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3550, best: 0.3544)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 148/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1662
  â€¢ Validation Loss: 0.3547
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3547, best: 0.3544)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 149/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2128
  â€¢ Validation Loss: 0.3546
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3546, best: 0.3544)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 150/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2030
  â€¢ Validation Loss: 0.3545
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3545, best: 0.3544)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 151/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0977
  â€¢ Validation Loss: 0.3670
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3670, best: 0.3544)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 152/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1429
  â€¢ Validation Loss: 0.3590
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3590, best: 0.3544)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 153/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1251
  â€¢ Validation Loss: 0.3584
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3584, best: 0.3544)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 154/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0574
  â€¢ Validation Loss: 0.3775
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3775, best: 0.3544)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 155/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1109
  â€¢ Validation Loss: 0.3645
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3645, best: 0.3544)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 156/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1475
  â€¢ Validation Loss: 0.3606
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3606, best: 0.3544)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 157/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1007
  â€¢ Validation Loss: 0.3693
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3693, best: 0.3544)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 158/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1965
  â€¢ Validation Loss: 0.3585
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3585, best: 0.3544)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 159/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1490
  â€¢ Validation Loss: 0.3571
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3571, best: 0.3544)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 160/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2133
  â€¢ Validation Loss: 0.3566
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3566, best: 0.3544)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 161/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2000
  â€¢ Validation Loss: 0.3581
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3581, best: 0.3544)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 162/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1317
  â€¢ Validation Loss: 0.3592
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3592, best: 0.3544)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 163/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1486
  â€¢ Validation Loss: 0.3538
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.3538
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 164/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1768
  â€¢ Validation Loss: 0.3554
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3554, best: 0.3538)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 165/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1233
  â€¢ Validation Loss: 0.3656
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3656, best: 0.3538)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 166/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1723
  â€¢ Validation Loss: 0.3613
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3613, best: 0.3538)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 167/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1284
  â€¢ Validation Loss: 0.3619
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3619, best: 0.3538)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 168/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1346
  â€¢ Validation Loss: 0.3571
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3571, best: 0.3538)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 169/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1091
  â€¢ Validation Loss: 0.3642
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3642, best: 0.3538)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 170/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0946
  â€¢ Validation Loss: 0.3674
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3674, best: 0.3538)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 171/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1270
  â€¢ Validation Loss: 0.3532
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.3532
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 172/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1414
  â€¢ Validation Loss: 0.3634
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3634, best: 0.3532)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 173/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1861
  â€¢ Validation Loss: 0.3556
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3556, best: 0.3532)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 174/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1797
  â€¢ Validation Loss: 0.3561
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3561, best: 0.3532)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 175/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1463
  â€¢ Validation Loss: 0.3517
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.3517
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 176/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1738
  â€¢ Validation Loss: 0.3527
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3527, best: 0.3517)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 177/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1304
  â€¢ Validation Loss: 0.3567
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3567, best: 0.3517)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 178/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1312
  â€¢ Validation Loss: 0.3594
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3594, best: 0.3517)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 179/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1483
  â€¢ Validation Loss: 0.3566
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3566, best: 0.3517)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 180/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1036
  â€¢ Validation Loss: 0.3549
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3549, best: 0.3517)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 181/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1038
  â€¢ Validation Loss: 0.3611
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3611, best: 0.3517)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 182/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1072
  â€¢ Validation Loss: 0.3558
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3558, best: 0.3517)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 183/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1976
  â€¢ Validation Loss: 0.3538
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3538, best: 0.3517)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 184/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1641
  â€¢ Validation Loss: 0.3594
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3594, best: 0.3517)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 185/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1519
  â€¢ Validation Loss: 0.3578
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3578, best: 0.3517)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 186/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1620
  â€¢ Validation Loss: 0.3547
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3547, best: 0.3517)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 187/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1919
  â€¢ Validation Loss: 0.3556
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3556, best: 0.3517)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 188/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1175
  â€¢ Validation Loss: 0.3585
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3585, best: 0.3517)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 189/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1513
  â€¢ Validation Loss: 0.3621
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3621, best: 0.3517)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 190/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1319
  â€¢ Validation Loss: 0.3538
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3538, best: 0.3517)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 191/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1695
  â€¢ Validation Loss: 0.3541
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3541, best: 0.3517)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 192/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0935
  â€¢ Validation Loss: 0.3771
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3771, best: 0.3517)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 193/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1491
  â€¢ Validation Loss: 0.3660
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3660, best: 0.3517)
    âš  No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 194/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0963
  â€¢ Validation Loss: 0.3664
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3664, best: 0.3517)
    âš  No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 195/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0555
  â€¢ Validation Loss: 0.3563
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3563, best: 0.3517)
    âš  No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 196/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1549
  â€¢ Validation Loss: 0.3533
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3533, best: 0.3517)
    âš  No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 197/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1543
  â€¢ Validation Loss: 0.3551
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3551, best: 0.3517)
    âš  No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 198/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2180
  â€¢ Validation Loss: 0.3548
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3548, best: 0.3517)
    âš  No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 199/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1445
  â€¢ Validation Loss: 0.3510
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.3510
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 200/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1379
  â€¢ Validation Loss: 0.3510
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   ğŸ’¾ Periodic checkpoint: epoch_200.pth
    âœ“ New best checkpoint saved! Val loss: 0.3510
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 201/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1857
  â€¢ Validation Loss: 0.3519
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3519, best: 0.3510)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 202/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1839
  â€¢ Validation Loss: 0.3541
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3541, best: 0.3510)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 203/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1498
  â€¢ Validation Loss: 0.3492
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.3492
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 204/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1773
  â€¢ Validation Loss: 0.3585
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3585, best: 0.3492)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 205/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1430
  â€¢ Validation Loss: 0.3532
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3532, best: 0.3492)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 206/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1535
  â€¢ Validation Loss: 0.3532
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3532, best: 0.3492)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 207/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1850
  â€¢ Validation Loss: 0.3540
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3540, best: 0.3492)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 208/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1943
  â€¢ Validation Loss: 0.3553
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3553, best: 0.3492)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 209/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1947
  â€¢ Validation Loss: 0.3502
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3502, best: 0.3492)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 210/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1639
  â€¢ Validation Loss: 0.3531
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3531, best: 0.3492)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 211/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1698
  â€¢ Validation Loss: 0.3540
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3540, best: 0.3492)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 212/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1261
  â€¢ Validation Loss: 0.3538
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3538, best: 0.3492)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 213/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1420
  â€¢ Validation Loss: 0.3613
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3613, best: 0.3492)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 214/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1523
  â€¢ Validation Loss: 0.3627
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3627, best: 0.3492)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 215/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1544
  â€¢ Validation Loss: 0.3522
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3522, best: 0.3492)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 216/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2029
  â€¢ Validation Loss: 0.3494
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3494, best: 0.3492)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 217/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1376
  â€¢ Validation Loss: 0.3561
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3561, best: 0.3492)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 218/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1989
  â€¢ Validation Loss: 0.3552
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3552, best: 0.3492)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 219/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1875
  â€¢ Validation Loss: 0.3551
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3551, best: 0.3492)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 220/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1871
  â€¢ Validation Loss: 0.3533
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3533, best: 0.3492)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 221/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1829
  â€¢ Validation Loss: 0.3476
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.3476
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2211
  â€¢ Validation Loss: 0.3515
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3515, best: 0.3476)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1688
  â€¢ Validation Loss: 0.3505
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3505, best: 0.3476)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 224/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1170
  â€¢ Validation Loss: 0.3454
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.3454
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 225/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1186
  â€¢ Validation Loss: 0.3484
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3484, best: 0.3454)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 226/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1570
  â€¢ Validation Loss: 0.3519
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3519, best: 0.3454)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 227/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2232
  â€¢ Validation Loss: 0.3487
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3487, best: 0.3454)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 228/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1953
  â€¢ Validation Loss: 0.3457
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3457, best: 0.3454)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 229/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1481
  â€¢ Validation Loss: 0.3508
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3508, best: 0.3454)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 230/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1578
  â€¢ Validation Loss: 0.3472
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3472, best: 0.3454)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 231/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1270
  â€¢ Validation Loss: 0.3510
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3510, best: 0.3454)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 232/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1453
  â€¢ Validation Loss: 0.3510
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3510, best: 0.3454)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 233/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1871
  â€¢ Validation Loss: 0.3503
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3503, best: 0.3454)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 234/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1573
  â€¢ Validation Loss: 0.3455
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3455, best: 0.3454)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 235/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1626
  â€¢ Validation Loss: 0.3509
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3509, best: 0.3454)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 236/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1558
  â€¢ Validation Loss: 0.3432
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.3432
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 237/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1723
  â€¢ Validation Loss: 0.3462
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3462, best: 0.3432)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 238/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1433
  â€¢ Validation Loss: 0.3465
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3465, best: 0.3432)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 239/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2002
  â€¢ Validation Loss: 0.3473
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3473, best: 0.3432)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 240/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1773
  â€¢ Validation Loss: 0.3464
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3464, best: 0.3432)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 241/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1928
  â€¢ Validation Loss: 0.3435
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3435, best: 0.3432)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 242/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2032
  â€¢ Validation Loss: 0.3485
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3485, best: 0.3432)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 243/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1508
  â€¢ Validation Loss: 0.3443
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3443, best: 0.3432)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 244/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1937
  â€¢ Validation Loss: 0.3459
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3459, best: 0.3432)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 245/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1581
  â€¢ Validation Loss: 0.3470
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3470, best: 0.3432)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 246/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1474
  â€¢ Validation Loss: 0.3427
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.3427
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 247/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1698
  â€¢ Validation Loss: 0.3521
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3521, best: 0.3427)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 248/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1952
  â€¢ Validation Loss: 0.3436
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3436, best: 0.3427)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 249/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1685
  â€¢ Validation Loss: 0.3443
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3443, best: 0.3427)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 250/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1280
  â€¢ Validation Loss: 0.3443
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3443, best: 0.3427)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 251/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1494
  â€¢ Validation Loss: 0.3440
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3440, best: 0.3427)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 252/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0786
  â€¢ Validation Loss: 0.3437
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3437, best: 0.3427)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 253/300
--------------------------------------------------
  âš ï¸  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0260
  â€¢ Validation Loss: 0.3440
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3440, best: 0.3427)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 254/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1226
  â€¢ Validation Loss: 0.3419
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3419
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 255/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1127
  â€¢ Validation Loss: 0.3424
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3424, best: 0.3419)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 256/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1321
  â€¢ Validation Loss: 0.3439
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3439, best: 0.3419)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 257/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1089
  â€¢ Validation Loss: 0.3447
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3447, best: 0.3419)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 258/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1224
  â€¢ Validation Loss: 0.3443
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3443, best: 0.3419)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 259/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0697
  â€¢ Validation Loss: 0.3430
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3430, best: 0.3419)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 260/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1474
  â€¢ Validation Loss: 0.3485
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3485, best: 0.3419)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 261/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0834
  â€¢ Validation Loss: 0.3464
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3464, best: 0.3419)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 262/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0972
  â€¢ Validation Loss: 0.3442
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3442, best: 0.3419)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 263/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1643
  â€¢ Validation Loss: 0.3446
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3446, best: 0.3419)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 264/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1393
  â€¢ Validation Loss: 0.3455
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3455, best: 0.3419)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 265/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0778
  â€¢ Validation Loss: 0.3443
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3443, best: 0.3419)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 266/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1191
  â€¢ Validation Loss: 0.3436
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3436, best: 0.3419)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 267/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0774
  â€¢ Validation Loss: 0.3460
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3460, best: 0.3419)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 268/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1377
  â€¢ Validation Loss: 0.3454
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3454, best: 0.3419)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 269/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0841
  â€¢ Validation Loss: 0.3446
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3446, best: 0.3419)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 270/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0989
  â€¢ Validation Loss: 0.3432
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3432, best: 0.3419)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 271/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0961
  â€¢ Validation Loss: 0.3433
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3433, best: 0.3419)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 272/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1063
  â€¢ Validation Loss: 0.3433
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3433, best: 0.3419)
    âš  No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 273/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0902
  â€¢ Validation Loss: 0.3431
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3431, best: 0.3419)
    âš  No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 274/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1093
  â€¢ Validation Loss: 0.3434
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3434, best: 0.3419)
    âš  No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 275/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1255
  â€¢ Validation Loss: 0.3420
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3420, best: 0.3419)
    âš  No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 276/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1221
  â€¢ Validation Loss: 0.3411
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3411
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 277/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1145
  â€¢ Validation Loss: 0.3424
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3424, best: 0.3411)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 278/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1120
  â€¢ Validation Loss: 0.3444
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3444, best: 0.3411)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 279/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0947
  â€¢ Validation Loss: 0.3434
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3434, best: 0.3411)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 280/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1060
  â€¢ Validation Loss: 0.3426
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3426, best: 0.3411)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 281/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1180
  â€¢ Validation Loss: 0.3446
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3446, best: 0.3411)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 282/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1026
  â€¢ Validation Loss: 0.3451
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3451, best: 0.3411)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 283/300
--------------------------------------------------
  âš ï¸  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0817
  â€¢ Validation Loss: 0.3437
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3437, best: 0.3411)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 284/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1369
  â€¢ Validation Loss: 0.3434
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3434, best: 0.3411)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 285/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1118
  â€¢ Validation Loss: 0.3428
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3428, best: 0.3411)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 286/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1298
  â€¢ Validation Loss: 0.3436
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3436, best: 0.3411)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 287/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1236
  â€¢ Validation Loss: 0.3437
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3437, best: 0.3411)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 288/300
--------------------------------------------------
  âš ï¸  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0983
  â€¢ Validation Loss: 0.3427
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3427, best: 0.3411)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 289/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1442
  â€¢ Validation Loss: 0.3425
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3425, best: 0.3411)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 290/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1174
  â€¢ Validation Loss: 0.3424
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3424, best: 0.3411)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 291/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1210
  â€¢ Validation Loss: 0.3433
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3433, best: 0.3411)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 292/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1484
  â€¢ Validation Loss: 0.3411
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3411
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 293/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1751
  â€¢ Validation Loss: 0.3423
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3423, best: 0.3411)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 294/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0525
  â€¢ Validation Loss: 0.3424
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3424, best: 0.3411)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 295/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1116
  â€¢ Validation Loss: 0.3430
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3430, best: 0.3411)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 296/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1041
  â€¢ Validation Loss: 0.3425
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3425, best: 0.3411)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 297/300
--------------------------------------------------
  âš ï¸  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0683
  â€¢ Validation Loss: 0.3421
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3421, best: 0.3411)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 298/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1064
  â€¢ Validation Loss: 0.3423
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3423, best: 0.3411)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 299/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1125
  â€¢ Validation Loss: 0.3432
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3432, best: 0.3411)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 300/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1873
  â€¢ Validation Loss: 0.3424
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   ğŸ’¾ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3424, best: 0.3411)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3411
Total Epochs:   300
Models Saved:   ./Result/a1/Latin16746
TensorBoard:    ./Result/a1/Latin16746/tensorboard_logs
================================================================================

[18:49:43] Training completed. Best val loss: 0.3411

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TRAINING COMPLETED: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proceeding to testing...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TESTING CNN-TRANSFORMER BASE MODEL: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Configuration:
  âœ“ Test-Time Augmentation (TTA): ENABLED
  âœ— CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): âœ“ ENABLED
  â†’ Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90Â°
  â†’ Averaging predictions across all augmentations
CRF POST-PROCESSING: âœ— DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
âœ“ Ground truth found for 009
âœ“ Completed: 009
Processing: 020 (54 patches)
âœ“ Ground truth found for 020
âœ“ Completed: 020
Processing: 022 (54 patches)
âœ“ Ground truth found for 022
âœ“ Completed: 022
Processing: 029 (54 patches)
âœ“ Ground truth found for 029
âœ“ Completed: 029
Processing: 035 (54 patches)
âœ“ Ground truth found for 035
âœ“ Completed: 035
Processing: 048 (54 patches)
âœ“ Ground truth found for 048
âœ“ Completed: 048
Processing: 069 (54 patches)
âœ“ Ground truth found for 069
âœ“ Completed: 069
Processing: 082 (54 patches)
âœ“ Ground truth found for 082
âœ“ Completed: 082
Processing: 088 (54 patches)
âœ“ Ground truth found for 088
âœ“ Completed: 088
Processing: 089 (54 patches)
âœ“ Ground truth found for 089
âœ“ Completed: 089
Processing: 091 (54 patches)
âœ“ Ground truth found for 091
âœ“ Completed: 091
Processing: 100 (54 patches)
âœ“ Ground truth found for 100
âœ“ Completed: 100
Processing: 106 (54 patches)
âœ“ Ground truth found for 106
âœ“ Completed: 106
Processing: 117 (54 patches)
âœ“ Ground truth found for 117
âœ“ Completed: 117
Processing: 123 (54 patches)
âœ“ Ground truth found for 123
âœ“ Completed: 123
Processing: 125 (54 patches)
âœ“ Ground truth found for 125
âœ“ Completed: 125
Processing: 130 (54 patches)
âœ“ Ground truth found for 130
âœ“ Completed: 130
Processing: 133 (54 patches)
âœ“ Ground truth found for 133
âœ“ Completed: 133
Processing: 137 (54 patches)
âœ“ Ground truth found for 137
âœ“ Completed: 137
Processing: 146 (54 patches)
âœ“ Ground truth found for 146
âœ“ Completed: 146
Processing: 166 (54 patches)
âœ“ Ground truth found for 166
âœ“ Completed: 166
Processing: 184 (54 patches)
âœ“ Ground truth found for 184
âœ“ Completed: 184
Processing: 215 (54 patches)
âœ“ Ground truth found for 215
âœ“ Completed: 215
Processing: 237 (54 patches)
âœ“ Ground truth found for 237
âœ“ Completed: 237
Processing: 243 (54 patches)
âœ“ Ground truth found for 243
âœ“ Completed: 243
Processing: 255 (54 patches)
âœ“ Ground truth found for 255
âœ“ Completed: 255
Processing: 258 (54 patches)
âœ“ Ground truth found for 258
âœ“ Completed: 258
Processing: 284 (54 patches)
âœ“ Ground truth found for 284
âœ“ Completed: 284
Processing: 325 (54 patches)
âœ“ Ground truth found for 325
âœ“ Completed: 325
Processing: 357 (54 patches)
âœ“ Ground truth found for 357
âœ“ Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9836, Recall=0.9923, F1=0.9879, IoU=0.9762
Paratext            : Precision=0.7600, Recall=0.7799, F1=0.7698, IoU=0.6258
Decoration          : Precision=0.9734, Recall=0.9069, F1=0.9389, IoU=0.8849
Main Text           : Precision=0.9170, Recall=0.8711, F1=0.8935, IoU=0.8075
Title               : Precision=0.7524, Recall=0.7410, F1=0.7467, IoU=0.5957
Chapter Headings    : Precision=0.8917, Recall=0.7050, F1=0.7874, IoU=0.6494

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8797
Mean Recall:    0.8327
Mean F1-Score:  0.8540
Mean IoU:       0.7566
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TESTING COMPLETED: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING CNN-TRANSFORMER BASE MODEL: Syr341
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Output Directory: ./Result/a1/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: simple
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Disabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 32
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ—
  â€¢ Fusion Method: SIMPLE
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 32
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a1/Syr341
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 32
   - Steps per epoch: 17


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

âœ“ Loss functions created: CE (weighted), Focal (Î³=2.0, no weights), Dice

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,854,431
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a1/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a1/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 1.3949
  â€¢ Validation Loss: 0.8715
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.8715
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.8177
  â€¢ Validation Loss: 0.7792
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.7792
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.7206
  â€¢ Validation Loss: 0.7183
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.7183
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.6951
  â€¢ Validation Loss: 0.6703
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.6703
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.6507
  â€¢ Validation Loss: 0.5963
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.5963
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.6295
  â€¢ Validation Loss: 0.5501
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.5501
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.5667
  â€¢ Validation Loss: 0.5355
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.5355
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4744
  â€¢ Validation Loss: 0.5231
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.5231
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4965
  â€¢ Validation Loss: 0.5521
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5521, best: 0.5231)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3273
  â€¢ Validation Loss: 0.5132
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.5132
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4649
  â€¢ Validation Loss: 0.5059
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.5059
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4284
  â€¢ Validation Loss: 0.4992
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4992
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3076
  â€¢ Validation Loss: 0.5027
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5027, best: 0.4992)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4161
  â€¢ Validation Loss: 0.4967
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4967
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4447
  â€¢ Validation Loss: 0.4934
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4934
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3754
  â€¢ Validation Loss: 0.5044
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5044, best: 0.4934)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4512
  â€¢ Validation Loss: 0.4949
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4949, best: 0.4934)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4509
  â€¢ Validation Loss: 0.4867
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4867
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2966
  â€¢ Validation Loss: 0.4824
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4824
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3098
  â€¢ Validation Loss: 0.4884
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4884, best: 0.4824)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3917
  â€¢ Validation Loss: 0.4823
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4823
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4166
  â€¢ Validation Loss: 0.4807
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4807
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4316
  â€¢ Validation Loss: 0.4868
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4868, best: 0.4807)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3652
  â€¢ Validation Loss: 0.4808
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4808, best: 0.4807)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 25/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3767
  â€¢ Validation Loss: 0.4839
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4839, best: 0.4807)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 26/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4340
  â€¢ Validation Loss: 0.4795
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4795
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3745
  â€¢ Validation Loss: 0.4801
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4801, best: 0.4795)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4115
  â€¢ Validation Loss: 0.4833
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4833, best: 0.4795)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4032
  â€¢ Validation Loss: 0.4775
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4775
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4232
  â€¢ Validation Loss: 0.4766
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4766
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4269
  â€¢ Validation Loss: 0.4784
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4784, best: 0.4766)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.4362
  â€¢ Validation Loss: 0.4729
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4729
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3996
  â€¢ Validation Loss: 0.4760
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4760, best: 0.4729)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3372
  â€¢ Validation Loss: 0.4754
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4754, best: 0.4729)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 35/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4087
  â€¢ Validation Loss: 0.4763
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4763, best: 0.4729)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 36/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3607
  â€¢ Validation Loss: 0.4751
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4751, best: 0.4729)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 37/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.4385
  â€¢ Validation Loss: 0.4737
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4737, best: 0.4729)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 38/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4196
  â€¢ Validation Loss: 0.4719
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.4719
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.4401
  â€¢ Validation Loss: 0.4732
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4732, best: 0.4719)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4287
  â€¢ Validation Loss: 0.4721
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4721, best: 0.4719)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4018
  â€¢ Validation Loss: 0.4718
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.4718
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3337
  â€¢ Validation Loss: 0.4723
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4723, best: 0.4718)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4230
  â€¢ Validation Loss: 0.4713
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.4713
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3872
  â€¢ Validation Loss: 0.4718
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4718, best: 0.4713)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3624
  â€¢ Validation Loss: 0.4721
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4721, best: 0.4713)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4123
  â€¢ Validation Loss: 0.4725
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4713)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 47/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3454
  â€¢ Validation Loss: 0.4721
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4721, best: 0.4713)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 48/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3550
  â€¢ Validation Loss: 0.4725
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4713)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 49/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3589
  â€¢ Validation Loss: 0.4718
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4718, best: 0.4713)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 50/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4221
  â€¢ Validation Loss: 0.4718
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4713)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 51/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3567
  â€¢ Validation Loss: 0.4717
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4717, best: 0.4713)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 52/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3828
  â€¢ Validation Loss: 0.4743
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4713)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 53/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4223
  â€¢ Validation Loss: 0.4756
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4756, best: 0.4713)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 54/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3939
  â€¢ Validation Loss: 0.4754
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4754, best: 0.4713)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 55/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3957
  â€¢ Validation Loss: 0.4734
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4734, best: 0.4713)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 56/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3423
  â€¢ Validation Loss: 0.4673
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4673
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3888
  â€¢ Validation Loss: 0.4656
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4656
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3277
  â€¢ Validation Loss: 0.4676
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4676, best: 0.4656)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4058
  â€¢ Validation Loss: 0.4667
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4667, best: 0.4656)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3549
  â€¢ Validation Loss: 0.4684
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4684, best: 0.4656)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.4059
  â€¢ Validation Loss: 0.4651
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4651
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3180
  â€¢ Validation Loss: 0.4649
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4649
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3980
  â€¢ Validation Loss: 0.4618
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4618
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3606
  â€¢ Validation Loss: 0.4592
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4592
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2977
  â€¢ Validation Loss: 0.4653
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4653, best: 0.4592)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3429
  â€¢ Validation Loss: 0.4599
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4599, best: 0.4592)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 67/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3435
  â€¢ Validation Loss: 0.4594
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4594, best: 0.4592)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 68/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3680
  â€¢ Validation Loss: 0.4618
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4618, best: 0.4592)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 69/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3888
  â€¢ Validation Loss: 0.4609
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4609, best: 0.4592)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 70/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3152
  â€¢ Validation Loss: 0.4567
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4567
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3632
  â€¢ Validation Loss: 0.4547
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4547
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2808
  â€¢ Validation Loss: 0.4471
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4471
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3055
  â€¢ Validation Loss: 0.4537
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4537, best: 0.4471)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3543
  â€¢ Validation Loss: 0.4568
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4568, best: 0.4471)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3724
  â€¢ Validation Loss: 0.4554
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4554, best: 0.4471)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3545
  â€¢ Validation Loss: 0.4466
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4466
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2854
  â€¢ Validation Loss: 0.4585
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4466)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3054
  â€¢ Validation Loss: 0.4524
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4524, best: 0.4466)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3842
  â€¢ Validation Loss: 0.4510
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4510, best: 0.4466)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3225
  â€¢ Validation Loss: 0.4512
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4512, best: 0.4466)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 81/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3458
  â€¢ Validation Loss: 0.4509
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4509, best: 0.4466)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 82/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3715
  â€¢ Validation Loss: 0.4454
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4454
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2945
  â€¢ Validation Loss: 0.4482
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4482, best: 0.4454)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3555
  â€¢ Validation Loss: 0.4447
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4447
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2957
  â€¢ Validation Loss: 0.4418
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4418
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3170
  â€¢ Validation Loss: 0.4502
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4502, best: 0.4418)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3936
  â€¢ Validation Loss: 0.4405
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4405
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3663
  â€¢ Validation Loss: 0.4434
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4434, best: 0.4405)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 89/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3258
  â€¢ Validation Loss: 0.4402
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4402
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2867
  â€¢ Validation Loss: 0.4351
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4351
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3321
  â€¢ Validation Loss: 0.4379
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4379, best: 0.4351)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3780
  â€¢ Validation Loss: 0.4410
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4410, best: 0.4351)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 93/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2680
  â€¢ Validation Loss: 0.4395
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4395, best: 0.4351)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 94/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3367
  â€¢ Validation Loss: 0.4430
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4430, best: 0.4351)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 95/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3705
  â€¢ Validation Loss: 0.4502
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4502, best: 0.4351)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 96/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3564
  â€¢ Validation Loss: 0.4342
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4342
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3333
  â€¢ Validation Loss: 0.4387
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4387, best: 0.4342)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3447
  â€¢ Validation Loss: 0.4385
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4385, best: 0.4342)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 99/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3411
  â€¢ Validation Loss: 0.4320
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4320
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3144
  â€¢ Validation Loss: 0.4320
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   ğŸ’¾ Periodic checkpoint: epoch_100.pth
    âœ“ New best checkpoint saved! Val loss: 0.4320
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3289
  â€¢ Validation Loss: 0.4313
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4313
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3647
  â€¢ Validation Loss: 0.4320
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4320, best: 0.4313)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3702
  â€¢ Validation Loss: 0.4286
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4286
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2911
  â€¢ Validation Loss: 0.4335
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4335, best: 0.4286)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 105/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3203
  â€¢ Validation Loss: 0.4302
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4302, best: 0.4286)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 106/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1960
  â€¢ Validation Loss: 0.4293
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4293, best: 0.4286)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 107/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3754
  â€¢ Validation Loss: 0.4280
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4280
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2918
  â€¢ Validation Loss: 0.4290
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4290, best: 0.4280)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3384
  â€¢ Validation Loss: 0.4298
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4298, best: 0.4280)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3435
  â€¢ Validation Loss: 0.4277
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4277
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3639
  â€¢ Validation Loss: 0.4289
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4289, best: 0.4277)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3622
  â€¢ Validation Loss: 0.4326
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4326, best: 0.4277)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2997
  â€¢ Validation Loss: 0.4249
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.4249
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2942
  â€¢ Validation Loss: 0.4261
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4261, best: 0.4249)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3562
  â€¢ Validation Loss: 0.4256
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4256, best: 0.4249)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3323
  â€¢ Validation Loss: 0.4257
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4257, best: 0.4249)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 117/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3552
  â€¢ Validation Loss: 0.4249
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4249, best: 0.4249)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 118/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2666
  â€¢ Validation Loss: 0.4263
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4263, best: 0.4249)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 119/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3276
  â€¢ Validation Loss: 0.4270
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4270, best: 0.4249)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 120/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2903
  â€¢ Validation Loss: 0.4248
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.4248
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3344
  â€¢ Validation Loss: 0.4274
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4274, best: 0.4248)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3119
  â€¢ Validation Loss: 0.4236
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.4236
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3380
  â€¢ Validation Loss: 0.4246
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4246, best: 0.4236)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 124/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3163
  â€¢ Validation Loss: 0.4252
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4252, best: 0.4236)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 125/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2994
  â€¢ Validation Loss: 0.4267
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4267, best: 0.4236)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 126/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3270
  â€¢ Validation Loss: 0.4233
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.4233
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 127/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3406
  â€¢ Validation Loss: 0.4238
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4238, best: 0.4233)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 128/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3427
  â€¢ Validation Loss: 0.4216
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.4216
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3172
  â€¢ Validation Loss: 0.4230
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4230, best: 0.4216)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 130/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3292
  â€¢ Validation Loss: 0.4238
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4238, best: 0.4216)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 131/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3471
  â€¢ Validation Loss: 0.4236
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4236, best: 0.4216)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 132/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2492
  â€¢ Validation Loss: 0.4225
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4225, best: 0.4216)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 133/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2879
  â€¢ Validation Loss: 0.4230
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4230, best: 0.4216)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 134/300
--------------------------------------------------
Results:
  â€¢ Train Loss: 0.3469
  â€¢ Validation Loss: 0.4225
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4225, best: 0.4216)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 135/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2574
  â€¢ Validation Loss: 0.4221
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 136/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2594
  â€¢ Validation Loss: 0.4216
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4216)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 137/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2063
  â€¢ Validation Loss: 0.4221
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 138/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1931
  â€¢ Validation Loss: 0.4221
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4216)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 139/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2521
  â€¢ Validation Loss: 0.4236
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4236, best: 0.4216)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 140/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3036
  â€¢ Validation Loss: 0.4229
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4229, best: 0.4216)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 141/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2482
  â€¢ Validation Loss: 0.4220
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4220, best: 0.4216)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 142/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3241
  â€¢ Validation Loss: 0.4216
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4216, best: 0.4216)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 143/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2381
  â€¢ Validation Loss: 0.4215
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    âœ“ New best checkpoint saved! Val loss: 0.4215
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 144/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2154
  â€¢ Validation Loss: 0.4217
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4215)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 145/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2334
  â€¢ Validation Loss: 0.4219
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4219, best: 0.4215)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 146/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2374
  â€¢ Validation Loss: 0.4217
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4217, best: 0.4215)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 147/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2883
  â€¢ Validation Loss: 0.4223
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4223, best: 0.4215)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 148/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1970
  â€¢ Validation Loss: 0.4227
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4227, best: 0.4215)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 149/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2788
  â€¢ Validation Loss: 0.4221
  â€¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4221, best: 0.4215)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 150/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2813
  â€¢ Validation Loss: 0.4220
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4220, best: 0.4215)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 151/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2646
  â€¢ Validation Loss: 0.4343
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4343, best: 0.4215)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 152/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2183
  â€¢ Validation Loss: 0.4376
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4376, best: 0.4215)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 153/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2516
  â€¢ Validation Loss: 0.4344
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4344, best: 0.4215)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 154/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2586
  â€¢ Validation Loss: 0.4330
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4330, best: 0.4215)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 155/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2586
  â€¢ Validation Loss: 0.4355
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4355, best: 0.4215)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 156/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2803
  â€¢ Validation Loss: 0.4232
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4232, best: 0.4215)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 157/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2303
  â€¢ Validation Loss: 0.4252
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4252, best: 0.4215)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 158/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2928
  â€¢ Validation Loss: 0.4254
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4254, best: 0.4215)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 159/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1967
  â€¢ Validation Loss: 0.4397
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4397, best: 0.4215)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 160/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2470
  â€¢ Validation Loss: 0.4238
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4238, best: 0.4215)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 161/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2300
  â€¢ Validation Loss: 0.4215
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4215
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 162/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2894
  â€¢ Validation Loss: 0.4238
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4238, best: 0.4215)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 163/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2360
  â€¢ Validation Loss: 0.4231
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4231, best: 0.4215)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 164/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2627
  â€¢ Validation Loss: 0.4215
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4215, best: 0.4215)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 165/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2168
  â€¢ Validation Loss: 0.4265
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4265, best: 0.4215)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 166/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2557
  â€¢ Validation Loss: 0.4248
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4248, best: 0.4215)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 167/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2441
  â€¢ Validation Loss: 0.4269
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4269, best: 0.4215)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 168/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1992
  â€¢ Validation Loss: 0.4182
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4182
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2318
  â€¢ Validation Loss: 0.4235
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4182)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1979
  â€¢ Validation Loss: 0.4171
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4171
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2548
  â€¢ Validation Loss: 0.4202
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4202, best: 0.4171)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2368
  â€¢ Validation Loss: 0.4171
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4171, best: 0.4171)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2134
  â€¢ Validation Loss: 0.4243
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4243, best: 0.4171)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2031
  â€¢ Validation Loss: 0.4293
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4293, best: 0.4171)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3048
  â€¢ Validation Loss: 0.4170
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4170
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 176/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2612
  â€¢ Validation Loss: 0.4151
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4151
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2204
  â€¢ Validation Loss: 0.4235
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4235, best: 0.4151)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2355
  â€¢ Validation Loss: 0.4146
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4146
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2516
  â€¢ Validation Loss: 0.4204
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4204, best: 0.4146)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1784
  â€¢ Validation Loss: 0.4107
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    âœ“ New best checkpoint saved! Val loss: 0.4107
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2063
  â€¢ Validation Loss: 0.4108
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4108, best: 0.4107)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 182/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2562
  â€¢ Validation Loss: 0.4154
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4154, best: 0.4107)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 183/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2846
  â€¢ Validation Loss: 0.4162
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4162, best: 0.4107)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 184/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1841
  â€¢ Validation Loss: 0.4155
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4155, best: 0.4107)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 185/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2385
  â€¢ Validation Loss: 0.4163
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4163, best: 0.4107)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 186/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1596
  â€¢ Validation Loss: 0.4112
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4112, best: 0.4107)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 187/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2084
  â€¢ Validation Loss: 0.4153
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4153, best: 0.4107)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 188/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2486
  â€¢ Validation Loss: 0.4192
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4192, best: 0.4107)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 189/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2621
  â€¢ Validation Loss: 0.4207
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4207, best: 0.4107)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 190/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2443
  â€¢ Validation Loss: 0.4117
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4117, best: 0.4107)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 191/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1790
  â€¢ Validation Loss: 0.4117
  â€¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4117, best: 0.4107)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 192/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1854
  â€¢ Validation Loss: 0.4107
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4107
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2016
  â€¢ Validation Loss: 0.4103
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4103
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.3100
  â€¢ Validation Loss: 0.4101
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4101
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 195/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2215
  â€¢ Validation Loss: 0.4113
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4113, best: 0.4101)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 196/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2276
  â€¢ Validation Loss: 0.4183
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4183, best: 0.4101)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 197/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2410
  â€¢ Validation Loss: 0.4111
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4101)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 198/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1995
  â€¢ Validation Loss: 0.4143
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4143, best: 0.4101)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 199/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1699
  â€¢ Validation Loss: 0.4299
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4299, best: 0.4101)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 200/300
--------------------------------------------------
  âš ï¸  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.0872
  â€¢ Validation Loss: 0.4319
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   ğŸ’¾ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4319, best: 0.4101)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 201/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2030
  â€¢ Validation Loss: 0.4121
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4121, best: 0.4101)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 202/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1608
  â€¢ Validation Loss: 0.4130
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4130, best: 0.4101)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 203/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2732
  â€¢ Validation Loss: 0.4105
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4105, best: 0.4101)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 204/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2562
  â€¢ Validation Loss: 0.4120
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4120, best: 0.4101)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 205/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2429
  â€¢ Validation Loss: 0.4068
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4068
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 206/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1371
  â€¢ Validation Loss: 0.4176
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4176, best: 0.4068)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 207/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2299
  â€¢ Validation Loss: 0.4116
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4116, best: 0.4068)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 208/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1573
  â€¢ Validation Loss: 0.4087
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4087, best: 0.4068)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 209/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1898
  â€¢ Validation Loss: 0.4163
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4163, best: 0.4068)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 210/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1445
  â€¢ Validation Loss: 0.4111
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4068)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 211/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1845
  â€¢ Validation Loss: 0.4147
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4147, best: 0.4068)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 212/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1800
  â€¢ Validation Loss: 0.4111
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4111, best: 0.4068)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 213/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1374
  â€¢ Validation Loss: 0.4104
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4104, best: 0.4068)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 214/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1999
  â€¢ Validation Loss: 0.4066
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4066
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 215/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2226
  â€¢ Validation Loss: 0.4056
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4056
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 216/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2152
  â€¢ Validation Loss: 0.4048
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    âœ“ New best checkpoint saved! Val loss: 0.4048
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 217/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1902
  â€¢ Validation Loss: 0.4077
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4077, best: 0.4048)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 218/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2026
  â€¢ Validation Loss: 0.4099
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4099, best: 0.4048)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 219/300
--------------------------------------------------
  âš ï¸  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1665
  â€¢ Validation Loss: 0.4073
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4073, best: 0.4048)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 220/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1730
  â€¢ Validation Loss: 0.4126
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4126, best: 0.4048)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 221/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1993
  â€¢ Validation Loss: 0.4056
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4056, best: 0.4048)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 222/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1891
  â€¢ Validation Loss: 0.4205
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4205, best: 0.4048)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 223/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1808
  â€¢ Validation Loss: 0.4162
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4162, best: 0.4048)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 224/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2308
  â€¢ Validation Loss: 0.4082
  â€¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4082, best: 0.4048)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 225/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2179
  â€¢ Validation Loss: 0.4127
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4127, best: 0.4048)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 226/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2222
  â€¢ Validation Loss: 0.4071
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4071, best: 0.4048)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 227/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1866
  â€¢ Validation Loss: 0.4073
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4073, best: 0.4048)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 228/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2010
  â€¢ Validation Loss: 0.4032
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4032
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 229/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1919
  â€¢ Validation Loss: 0.4125
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4125, best: 0.4032)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 230/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2418
  â€¢ Validation Loss: 0.4077
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4077, best: 0.4032)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 231/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1416
  â€¢ Validation Loss: 0.4031
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.4031
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2110
  â€¢ Validation Loss: 0.4065
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4065, best: 0.4031)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2061
  â€¢ Validation Loss: 0.4053
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4053, best: 0.4031)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2714
  â€¢ Validation Loss: 0.4051
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4051, best: 0.4031)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2310
  â€¢ Validation Loss: 0.4153
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4153, best: 0.4031)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1980
  â€¢ Validation Loss: 0.4047
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4047, best: 0.4031)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2127
  â€¢ Validation Loss: 0.3987
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.3987
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2227
  â€¢ Validation Loss: 0.4059
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4059, best: 0.3987)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2384
  â€¢ Validation Loss: 0.4035
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4035, best: 0.3987)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 240/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2294
  â€¢ Validation Loss: 0.4018
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4018, best: 0.3987)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 241/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2236
  â€¢ Validation Loss: 0.4047
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4047, best: 0.3987)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 242/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2505
  â€¢ Validation Loss: 0.3979
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    âœ“ New best checkpoint saved! Val loss: 0.3979
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2139
  â€¢ Validation Loss: 0.4035
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4035, best: 0.3979)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2112
  â€¢ Validation Loss: 0.4034
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4034, best: 0.3979)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 245/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2132
  â€¢ Validation Loss: 0.4006
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4006, best: 0.3979)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 246/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2364
  â€¢ Validation Loss: 0.4042
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4042, best: 0.3979)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 247/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2144
  â€¢ Validation Loss: 0.4065
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4065, best: 0.3979)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 248/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2193
  â€¢ Validation Loss: 0.4026
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4026, best: 0.3979)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 249/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2511
  â€¢ Validation Loss: 0.4039
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4039, best: 0.3979)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 250/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1855
  â€¢ Validation Loss: 0.4058
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4058, best: 0.3979)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 251/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1892
  â€¢ Validation Loss: 0.4015
  â€¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4015, best: 0.3979)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 252/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1948
  â€¢ Validation Loss: 0.4024
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4024, best: 0.3979)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 253/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2176
  â€¢ Validation Loss: 0.4035
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4035, best: 0.3979)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 254/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2134
  â€¢ Validation Loss: 0.4054
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4054, best: 0.3979)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 255/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2064
  â€¢ Validation Loss: 0.4038
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4038, best: 0.3979)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 256/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1666
  â€¢ Validation Loss: 0.4087
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4087, best: 0.3979)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 257/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2170
  â€¢ Validation Loss: 0.4050
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4050, best: 0.3979)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 258/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2198
  â€¢ Validation Loss: 0.3984
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3984, best: 0.3979)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 259/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2205
  â€¢ Validation Loss: 0.3974
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3974
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2540
  â€¢ Validation Loss: 0.4062
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4062, best: 0.3974)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1699
  â€¢ Validation Loss: 0.3997
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3997, best: 0.3974)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2530
  â€¢ Validation Loss: 0.3995
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3995, best: 0.3974)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2239
  â€¢ Validation Loss: 0.3997
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3997, best: 0.3974)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 264/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2152
  â€¢ Validation Loss: 0.4004
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4004, best: 0.3974)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 265/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2119
  â€¢ Validation Loss: 0.3965
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3965
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 266/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2254
  â€¢ Validation Loss: 0.4000
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4000, best: 0.3965)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 267/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2187
  â€¢ Validation Loss: 0.4025
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4025, best: 0.3965)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 268/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2156
  â€¢ Validation Loss: 0.4034
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4034, best: 0.3965)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 269/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1883
  â€¢ Validation Loss: 0.3986
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3986, best: 0.3965)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 270/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1929
  â€¢ Validation Loss: 0.3970
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3970, best: 0.3965)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 271/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2108
  â€¢ Validation Loss: 0.3995
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3995, best: 0.3965)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 272/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1676
  â€¢ Validation Loss: 0.4005
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4005, best: 0.3965)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 273/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2247
  â€¢ Validation Loss: 0.3977
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3977, best: 0.3965)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 274/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2091
  â€¢ Validation Loss: 0.4006
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4006, best: 0.3965)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 275/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2271
  â€¢ Validation Loss: 0.3993
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3993, best: 0.3965)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 276/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1981
  â€¢ Validation Loss: 0.3984
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3984, best: 0.3965)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 277/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2047
  â€¢ Validation Loss: 0.3964
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    âœ“ New best checkpoint saved! Val loss: 0.3964
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 278/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2338
  â€¢ Validation Loss: 0.3965
  â€¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3965, best: 0.3964)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 279/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2095
  â€¢ Validation Loss: 0.3943
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3943
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 280/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1819
  â€¢ Validation Loss: 0.3997
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3997, best: 0.3943)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 281/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2084
  â€¢ Validation Loss: 0.3936
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3936
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 282/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2356
  â€¢ Validation Loss: 0.3921
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    âœ“ New best checkpoint saved! Val loss: 0.3921
    âœ“ Improvement detected! Resetting patience counter.

EPOCH 283/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2238
  â€¢ Validation Loss: 0.3962
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3962, best: 0.3921)
    âš  No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 284/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2017
  â€¢ Validation Loss: 0.3968
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3968, best: 0.3921)
    âš  No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 285/300
--------------------------------------------------
  âš ï¸  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2027
  â€¢ Validation Loss: 0.4012
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4012, best: 0.3921)
    âš  No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 286/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2235
  â€¢ Validation Loss: 0.3975
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3975, best: 0.3921)
    âš  No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 287/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1790
  â€¢ Validation Loss: 0.3951
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3951, best: 0.3921)
    âš  No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 288/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1643
  â€¢ Validation Loss: 0.3964
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3964, best: 0.3921)
    âš  No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 289/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2511
  â€¢ Validation Loss: 0.3986
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3986, best: 0.3921)
    âš  No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 290/300
--------------------------------------------------
  âš ï¸  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2470
  â€¢ Validation Loss: 0.4044
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4044, best: 0.3921)
    âš  No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 291/300
--------------------------------------------------
  âš ï¸  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1377
  â€¢ Validation Loss: 0.3992
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3992, best: 0.3921)
    âš  No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 292/300
--------------------------------------------------
  âš ï¸  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2623
  â€¢ Validation Loss: 0.4009
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4009, best: 0.3921)
    âš  No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 293/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1653
  â€¢ Validation Loss: 0.4000
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4000, best: 0.3921)
    âš  No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 294/300
--------------------------------------------------
  âš ï¸  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.2424
  â€¢ Validation Loss: 0.3983
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3983, best: 0.3921)
    âš  No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 295/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1689
  â€¢ Validation Loss: 0.4018
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4018, best: 0.3921)
    âš  No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 296/300
--------------------------------------------------
  âš ï¸  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1706
  â€¢ Validation Loss: 0.4023
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4023, best: 0.3921)
    âš  No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 297/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1956
  â€¢ Validation Loss: 0.3988
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3988, best: 0.3921)
    âš  No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 298/300
--------------------------------------------------
  âš ï¸  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1916
  â€¢ Validation Loss: 0.3980
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3980, best: 0.3921)
    âš  No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 299/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1903
  â€¢ Validation Loss: 0.3995
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3995, best: 0.3921)
    âš  No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 300/300
--------------------------------------------------
  âš ï¸  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  â€¢ Train Loss: 0.1849
  â€¢ Validation Loss: 0.3972
  â€¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   ğŸ’¾ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3972, best: 0.3921)
    âš  No improvement for 18 epochs (patience: 150, remaining: 132)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3921
Total Epochs:   300
Models Saved:   ./Result/a1/Syr341
TensorBoard:    ./Result/a1/Syr341/tensorboard_logs
================================================================================

[19:48:27] Training completed. Best val loss: 0.3921

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TRAINING COMPLETED: Syr341
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proceeding to testing...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TESTING CNN-TRANSFORMER BASE MODEL: Syr341
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Configuration:
  âœ“ Test-Time Augmentation (TTA): ENABLED
  âœ— CRF Post-processing: DISABLED
  - Batch Size: 8 (reduced for TTA memory efficiency)

WARNING:root:Component flags ['use_groupnorm'] are typically used with --use_baseline flag
WARNING:root:Consider using --use_baseline for baseline configuration
WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): âœ“ ENABLED
  â†’ Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90Â°
  â†’ Averaging predictions across all augmentations
CRF POST-PROCESSING: âœ— DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
âœ“ Ground truth found for 031
âœ“ Completed: 031
Processing: 053 (54 patches)
âœ“ Ground truth found for 053
âœ“ Completed: 053
Processing: 054 (54 patches)
âœ“ Ground truth found for 054
âœ“ Completed: 054
Processing: 071 (54 patches)
âœ“ Ground truth found for 071
âœ“ Completed: 071
Processing: 073 (54 patches)
âœ“ Ground truth found for 073
âœ“ Completed: 073
Processing: 075 (54 patches)
âœ“ Ground truth found for 075
âœ“ Completed: 075
Processing: 100 (54 patches)
âœ“ Ground truth found for 100
âœ“ Completed: 100
Processing: 137 (54 patches)
âœ“ Ground truth found for 137
âœ“ Completed: 137
Processing: 150 (54 patches)
âœ“ Ground truth found for 150
âœ“ Completed: 150
Processing: 160 (54 patches)
âœ“ Ground truth found for 160
âœ“ Completed: 160
Processing: 167 (54 patches)
âœ“ Ground truth found for 167
âœ“ Completed: 167
Processing: 184 (54 patches)
âœ“ Ground truth found for 184
âœ“ Completed: 184
Processing: 190 (54 patches)
âœ“ Ground truth found for 190
âœ“ Completed: 190
Processing: 201 (54 patches)
âœ“ Ground truth found for 201
âœ“ Completed: 201
Processing: 210 (54 patches)
âœ“ Ground truth found for 210
âœ“ Completed: 210
Processing: 222 (54 patches)
âœ“ Ground truth found for 222
âœ“ Completed: 222
Processing: 224 (54 patches)
âœ“ Ground truth found for 224
âœ“ Completed: 224
Processing: 231 (54 patches)
âœ“ Ground truth found for 231
âœ“ Completed: 231
Processing: 241 (54 patches)
âœ“ Ground truth found for 241
âœ“ Completed: 241
Processing: 249 (54 patches)
âœ“ Ground truth found for 249
âœ“ Completed: 249
Processing: 252 (54 patches)
âœ“ Ground truth found for 252
âœ“ Completed: 252
Processing: 267 (54 patches)
âœ“ Ground truth found for 267
âœ“ Completed: 267
Processing: 281 (54 patches)
âœ“ Ground truth found for 281
âœ“ Completed: 281
Processing: 286 (54 patches)
âœ“ Ground truth found for 286
âœ“ Completed: 286
Processing: 290 (54 patches)
âœ“ Ground truth found for 290
âœ“ Completed: 290
Processing: 313 (54 patches)
âœ“ Ground truth found for 313
âœ“ Completed: 313
Processing: 362 (54 patches)
âœ“ Ground truth found for 362
âœ“ Completed: 362
Processing: 368 (54 patches)
âœ“ Ground truth found for 368
âœ“ Completed: 368
Processing: 376 (54 patches)
âœ“ Ground truth found for 376
âœ“ Completed: 376
Processing: 446 (54 patches)
âœ“ Ground truth found for 446
âœ“ Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9690, Recall=0.9784, F1=0.9737, IoU=0.9487
Paratext            : Precision=0.5845, Recall=0.4471, F1=0.5066, IoU=0.3393
Decoration          : Precision=0.9641, Recall=0.6972, F1=0.8092, IoU=0.6795
Main Text           : Precision=0.8450, Recall=0.8414, F1=0.8432, IoU=0.7289
Title               : Precision=0.5165, Recall=0.2491, F1=0.3361, IoU=0.2020

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7758
Mean Recall:    0.6426
Mean F1-Score:  0.6938
Mean IoU:       0.5797
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8225
Mean Recall:    0.7461
Mean F1-Score:  0.7728
Mean IoU:       0.6686
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TESTING COMPLETED: Syr341
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL (No Extra Components)
Results Location: ./Result/a1/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Sat Nov 15 07:51:04 PM CET 2025
= Job-ID             : 1327314 on tinygpu
= Job-Name           : 1st
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 01:42:07
= Total RAM usage    : 8.4 GiB of requested  GiB (%)   
= Node list          : tg066
= Subm/Elig/Start/End: 2025-11-15T18:08:29 / 2025-11-15T18:08:29 / 2025-11-15T18:08:30 / 2025-11-15T19:50:37
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              71.1G   104.9G   209.7G        N/A     236K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1780020, 42 %, 31 %, 10624 MiB, 144435 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1780432, 16 %, 12 %, 1870 MiB, 142558 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1780469, 64 %, 48 %, 10624 MiB, 259798 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1781350, 18 %, 13 %, 1870 MiB, 127684 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1781390, 68 %, 51 %, 10624 MiB, 1677539 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1787477, 16 %, 12 %, 1870 MiB, 144668 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1787521, 64 %, 47 %, 10490 MiB, 3363052 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:86:00.0, 1798717, 16 %, 12 %, 1870 MiB, 145812 ms
