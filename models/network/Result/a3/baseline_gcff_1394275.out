### Starting TaskPrologue of job 1394275 on tg082 at Wed Nov 19 01:04:03 AM CET 2025
Running on cores 12-13,28-29,44-45,60-61 with governor powersave
Wed Nov 19 01:04:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:DA:00.0 Off |                  N/A |
| 30%   32C    P8             23W /  300W |       1MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + GCFF FUSION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff (Global Context Feature Fusion from MSAGHNet)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì GCFF Components:
    - Global Context Block (attention pooling + MLP)
    - Channel Attention Module (max/avg pool + MLP)
    - Applied at 3 skip connection stages
  ‚úó Deep Supervision: disabled (baseline with GCFF only)
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Fourier Feature Fusion: disabled (using GCFF fusion)
  ‚úó Smart Skip Connections: disabled (using GCFF fusion)
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
  - Loss: CE (weighted) + Focal (Œ≥=2.0) + Dice
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)
Output Directory: ./Result/a3/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF, SE-MSFE, or MSFA+MCT bottleneck)
   ‚Üí Encoder LR reduced to 0.01x (from 0.05x) for better stability
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up enabled (first 10 epochs)

Encoder     : LR=0.000001, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,954,879
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
   ‚Üí Warm-up: Manual LR control for first 10 epochs
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  üî• LR Warm-up: 10.0% complete (factor: 0.190x)
Results:
  ‚Ä¢ Train Loss: 0.9086
  ‚Ä¢ Validation Loss: 0.7337
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.7337
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7163
  ‚Ä¢ Validation Loss: 0.6774
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6774
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6697
  ‚Ä¢ Validation Loss: 0.6348
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6348
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6653
  ‚Ä¢ Validation Loss: 0.6132
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6132
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  üî• LR Warm-up: 50.0% complete (factor: 0.550x)
Results:
  ‚Ä¢ Train Loss: 0.6404
  ‚Ä¢ Validation Loss: 0.6067
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6067
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6203
  ‚Ä¢ Validation Loss: 0.5987
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5987
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6166
  ‚Ä¢ Validation Loss: 0.5857
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5857
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5957
  ‚Ä¢ Validation Loss: 0.5784
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5784
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5814
  ‚Ä¢ Validation Loss: 0.5771
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5771
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  üî• LR Warm-up: 100.0% complete (factor: 1.000x)
Results:
  ‚Ä¢ Train Loss: 0.5754
  ‚Ä¢ Validation Loss: 0.5650
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5650
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5572
  ‚Ä¢ Validation Loss: 0.5640
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5640
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5492
  ‚Ä¢ Validation Loss: 0.5608
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5608
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5479
  ‚Ä¢ Validation Loss: 0.5528
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5528
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5372
  ‚Ä¢ Validation Loss: 0.5442
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5442
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5297
  ‚Ä¢ Validation Loss: 0.5387
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5387
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5292
  ‚Ä¢ Validation Loss: 0.5396
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5396, best: 0.5387)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5200
  ‚Ä¢ Validation Loss: 0.5300
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5300
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5053
  ‚Ä¢ Validation Loss: 0.5241
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5241
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5076
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5199
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4927
  ‚Ä¢ Validation Loss: 0.5164
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5164
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4889
  ‚Ä¢ Validation Loss: 0.5108
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5108
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4836
  ‚Ä¢ Validation Loss: 0.5089
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5089
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4725
  ‚Ä¢ Validation Loss: 0.5073
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5073
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4773
  ‚Ä¢ Validation Loss: 0.4950
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4950
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4380
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4655
  ‚Ä¢ Validation Loss: 0.4858
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4858
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4545
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4846
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4393
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4826
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4409
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4773
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4366
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4743
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4262
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4701
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4361
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4705, best: 0.4701)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4326
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4701)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4065
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4677
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4075
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4681, best: 0.4677)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3982
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4655
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4166
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4639
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4080
  ‚Ä¢ Validation Loss: 0.4613
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4613
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4069
  ‚Ä¢ Validation Loss: 0.4643
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4643, best: 0.4613)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4052
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4602
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3992
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4595
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4096
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4072
  ‚Ä¢ Validation Loss: 0.4564
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4564
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4126
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4575, best: 0.4564)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3841
  ‚Ä¢ Validation Loss: 0.4547
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4078
  ‚Ä¢ Validation Loss: 0.4552
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4552, best: 0.4547)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4099
  ‚Ä¢ Validation Loss: 0.4550
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4550, best: 0.4547)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4098
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4532
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4051
  ‚Ä¢ Validation Loss: 0.4550
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4550, best: 0.4532)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4101
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4537, best: 0.4532)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4035
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4524
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4050
  ‚Ä¢ Validation Loss: 0.4533
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4533, best: 0.4524)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3973
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4530, best: 0.4524)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4004
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4526, best: 0.4524)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4012
  ‚Ä¢ Validation Loss: 0.4534
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4534, best: 0.4524)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4522
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4522
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4074
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4517
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4071
  ‚Ä¢ Validation Loss: 0.4525
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4525, best: 0.4517)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3967
  ‚Ä¢ Validation Loss: 0.4533
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4533, best: 0.4517)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4081
  ‚Ä¢ Validation Loss: 0.4521
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4521, best: 0.4517)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3809
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4568, best: 0.4517)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3911
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4573, best: 0.4517)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4014
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4527, best: 0.4517)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3850
  ‚Ä¢ Validation Loss: 0.4549
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4549, best: 0.4517)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3948
  ‚Ä¢ Validation Loss: 0.4538
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4538, best: 0.4517)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3941
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4492
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3921
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4469
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3891
  ‚Ä¢ Validation Loss: 0.4560
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4560, best: 0.4469)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3925
  ‚Ä¢ Validation Loss: 0.4457
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4457
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3973
  ‚Ä¢ Validation Loss: 0.4450
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4450
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3891
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4439
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3931
  ‚Ä¢ Validation Loss: 0.4415
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4415
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3809
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4441, best: 0.4415)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3825
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4372
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3738
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4367
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3791
  ‚Ä¢ Validation Loss: 0.4412
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4412, best: 0.4367)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3635
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4308
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3608
  ‚Ä¢ Validation Loss: 0.4318
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4318, best: 0.4308)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3632
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4345, best: 0.4308)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3769
  ‚Ä¢ Validation Loss: 0.4350
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4350, best: 0.4308)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3652
  ‚Ä¢ Validation Loss: 0.4303
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4303
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3667
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4290
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3592
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4291, best: 0.4290)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3187
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3674
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4254
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3696
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4277, best: 0.4254)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3679
  ‚Ä¢ Validation Loss: 0.4320
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4320, best: 0.4254)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3492
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4247
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3163
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4238
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3133
  ‚Ä¢ Validation Loss: 0.4294
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4294, best: 0.4238)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3222
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4288, best: 0.4238)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2928
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4242, best: 0.4238)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3061
  ‚Ä¢ Validation Loss: 0.4230
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4230
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3214
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4212
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2979
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4259, best: 0.4212)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3242
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4331, best: 0.4212)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3257
  ‚Ä¢ Validation Loss: 0.4246
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4246, best: 0.4212)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 98/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3526
  ‚Ä¢ Validation Loss: 0.4209
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4209
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3215
  ‚Ä¢ Validation Loss: 0.4291
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4291, best: 0.4209)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3357
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4223, best: 0.4209)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2551
  ‚Ä¢ Validation Loss: 0.4186
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4186
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2483
  ‚Ä¢ Validation Loss: 0.4213
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4213, best: 0.4186)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3228
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2873
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4220, best: 0.4177)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3167
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4189, best: 0.4177)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3157
  ‚Ä¢ Validation Loss: 0.4241
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4241, best: 0.4177)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2817
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4196, best: 0.4177)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3116
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4148
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2941
  ‚Ä¢ Validation Loss: 0.4129
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4129
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2740
  ‚Ä¢ Validation Loss: 0.4119
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4119
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3378
  ‚Ä¢ Validation Loss: 0.4136
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4136, best: 0.4119)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2993
  ‚Ä¢ Validation Loss: 0.4138
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4138, best: 0.4119)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3076
  ‚Ä¢ Validation Loss: 0.4181
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4181, best: 0.4119)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2966
  ‚Ä¢ Validation Loss: 0.4179
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4179, best: 0.4119)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.4170
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4170, best: 0.4119)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2788
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4142, best: 0.4119)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2971
  ‚Ä¢ Validation Loss: 0.4106
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4106
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3131
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4116, best: 0.4106)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3261
  ‚Ä¢ Validation Loss: 0.4162
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4162, best: 0.4106)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2837
  ‚Ä¢ Validation Loss: 0.4119
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4119, best: 0.4106)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2757
  ‚Ä¢ Validation Loss: 0.4102
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4102
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.4113
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4113, best: 0.4102)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2873
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4163, best: 0.4102)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3086
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4126, best: 0.4102)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2883
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4112, best: 0.4102)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2863
  ‚Ä¢ Validation Loss: 0.4143
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4143, best: 0.4102)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2755
  ‚Ä¢ Validation Loss: 0.4081
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4081
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3025
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4091, best: 0.4081)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2939
  ‚Ä¢ Validation Loss: 0.4092
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4092, best: 0.4081)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2944
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4095, best: 0.4081)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2737
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4084, best: 0.4081)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2820
  ‚Ä¢ Validation Loss: 0.4085
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4085, best: 0.4081)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2648
  ‚Ä¢ Validation Loss: 0.4082
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4082, best: 0.4081)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2902
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4061
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3017
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4079, best: 0.4061)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2622
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4071, best: 0.4061)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4096
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4096, best: 0.4061)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3180
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4061, best: 0.4061)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3188
  ‚Ä¢ Validation Loss: 0.4081
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4081, best: 0.4061)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2935
  ‚Ä¢ Validation Loss: 0.4074
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4074, best: 0.4061)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2893
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4073, best: 0.4061)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2977
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4068, best: 0.4061)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4066, best: 0.4061)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2705
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4067, best: 0.4061)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2545
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4071, best: 0.4061)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2394
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4052
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4063, best: 0.4052)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3006
  ‚Ä¢ Validation Loss: 0.4083
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4083, best: 0.4052)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3060
  ‚Ä¢ Validation Loss: 0.4069
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4069, best: 0.4052)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2773
  ‚Ä¢ Validation Loss: 0.4080
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4080, best: 0.4052)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3165
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4068, best: 0.4052)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2930
  ‚Ä¢ Validation Loss: 0.4057
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4057, best: 0.4052)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2831
  ‚Ä¢ Validation Loss: 0.4071
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4071, best: 0.4052)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2973
  ‚Ä¢ Validation Loss: 0.4087
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4087, best: 0.4052)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2994
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4067, best: 0.4052)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3102
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4055, best: 0.4052)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2579
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4065, best: 0.4052)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3043
  ‚Ä¢ Validation Loss: 0.4066
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4066, best: 0.4052)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2760
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4065, best: 0.4052)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2747
  ‚Ä¢ Validation Loss: 0.4060
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4060, best: 0.4052)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3225
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4101, best: 0.4052)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2763
  ‚Ä¢ Validation Loss: 0.4180
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4180, best: 0.4052)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2521
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4063, best: 0.4052)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3067
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4059, best: 0.4052)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2910
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4101, best: 0.4052)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2516
  ‚Ä¢ Validation Loss: 0.4086
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4086, best: 0.4052)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3226
  ‚Ä¢ Validation Loss: 0.4136
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4136, best: 0.4052)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2898
  ‚Ä¢ Validation Loss: 0.4033
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4033
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3062
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4146, best: 0.4033)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2464
  ‚Ä¢ Validation Loss: 0.4015
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4015
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2572
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4127, best: 0.4015)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2828
  ‚Ä¢ Validation Loss: 0.4055
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4055, best: 0.4015)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2296
  ‚Ä¢ Validation Loss: 0.4043
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4043, best: 0.4015)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3110
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4239, best: 0.4015)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2526
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4112, best: 0.4015)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2765
  ‚Ä¢ Validation Loss: 0.4079
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4079, best: 0.4015)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2771
  ‚Ä¢ Validation Loss: 0.4026
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4026, best: 0.4015)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2780
  ‚Ä¢ Validation Loss: 0.4037
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4037, best: 0.4015)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2582
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4045, best: 0.4015)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2586
  ‚Ä¢ Validation Loss: 0.3996
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3996
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2545
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4028, best: 0.3996)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2694
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4045, best: 0.3996)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2990
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4171, best: 0.3996)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2356
  ‚Ä¢ Validation Loss: 0.4129
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4129, best: 0.3996)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3164
  ‚Ä¢ Validation Loss: 0.4058
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4058, best: 0.3996)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2924
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4044, best: 0.3996)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2630
  ‚Ä¢ Validation Loss: 0.3978
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2855
  ‚Ä¢ Validation Loss: 0.3994
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3994, best: 0.3978)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2472
  ‚Ä¢ Validation Loss: 0.4001
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4001, best: 0.3978)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1749
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3980, best: 0.3978)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1194
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4094, best: 0.3978)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1884
  ‚Ä¢ Validation Loss: 0.4108
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4108, best: 0.3978)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1224
  ‚Ä¢ Validation Loss: 0.4000
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4000, best: 0.3978)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0922
  ‚Ä¢ Validation Loss: 0.4049
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4049, best: 0.3978)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0529
  ‚Ä¢ Validation Loss: 0.4056
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4056, best: 0.3978)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0572
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3958
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0570
  ‚Ä¢ Validation Loss: 0.3977
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3977, best: 0.3958)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1105
  ‚Ä¢ Validation Loss: 0.3952
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3952
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1761
  ‚Ä¢ Validation Loss: 0.3992
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3992, best: 0.3952)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1126
  ‚Ä¢ Validation Loss: 0.4001
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4001, best: 0.3952)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1184
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4068, best: 0.3952)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1622
  ‚Ä¢ Validation Loss: 0.4027
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4027, best: 0.3952)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1204
  ‚Ä¢ Validation Loss: 0.4021
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4021, best: 0.3952)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1512
  ‚Ä¢ Validation Loss: 0.4038
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4038, best: 0.3952)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0962
  ‚Ä¢ Validation Loss: 0.4025
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4025, best: 0.3952)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1514
  ‚Ä¢ Validation Loss: 0.3962
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3962, best: 0.3952)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1524
  ‚Ä¢ Validation Loss: 0.3966
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3966, best: 0.3952)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.3945
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3945
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1156
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4032, best: 0.3945)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1098
  ‚Ä¢ Validation Loss: 0.3988
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3988, best: 0.3945)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1331
  ‚Ä¢ Validation Loss: 0.4020
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4020, best: 0.3945)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0535
  ‚Ä¢ Validation Loss: 0.4129
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4129, best: 0.3945)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1184
  ‚Ä¢ Validation Loss: 0.3969
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3969, best: 0.3945)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1302
  ‚Ä¢ Validation Loss: 0.3976
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3976, best: 0.3945)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0906
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4035, best: 0.3945)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1217
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3937
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0903
  ‚Ä¢ Validation Loss: 0.3984
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3984, best: 0.3937)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1457
  ‚Ä¢ Validation Loss: 0.3947
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3947, best: 0.3937)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0675
  ‚Ä¢ Validation Loss: 0.3969
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3969, best: 0.3937)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1354
  ‚Ä¢ Validation Loss: 0.3930
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3930
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1455
  ‚Ä¢ Validation Loss: 0.4015
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4015, best: 0.3930)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0657
  ‚Ä¢ Validation Loss: 0.4049
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4049, best: 0.3930)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0784
  ‚Ä¢ Validation Loss: 0.3974
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3974, best: 0.3930)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1087
  ‚Ä¢ Validation Loss: 0.3997
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3997, best: 0.3930)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1580
  ‚Ä¢ Validation Loss: 0.3958
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3958, best: 0.3930)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1156
  ‚Ä¢ Validation Loss: 0.3988
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3988, best: 0.3930)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0672
  ‚Ä¢ Validation Loss: 0.3967
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3967, best: 0.3930)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0819
  ‚Ä¢ Validation Loss: 0.3938
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3938, best: 0.3930)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1169
  ‚Ä¢ Validation Loss: 0.3965
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3965, best: 0.3930)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.3953
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3953, best: 0.3930)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1023
  ‚Ä¢ Validation Loss: 0.3927
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3927
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.3975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3975, best: 0.3927)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1323
  ‚Ä¢ Validation Loss: 0.3932
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3932, best: 0.3927)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1600
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1301
  ‚Ä¢ Validation Loss: 0.3960
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3960, best: 0.3921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1725
  ‚Ä¢ Validation Loss: 0.4022
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4022, best: 0.3921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1340
  ‚Ä¢ Validation Loss: 0.3963
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3963, best: 0.3921)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.3956
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3956, best: 0.3921)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1372
  ‚Ä¢ Validation Loss: 0.3944
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3944, best: 0.3921)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1193
  ‚Ä¢ Validation Loss: 0.3906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3906
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1283
  ‚Ä¢ Validation Loss: 0.3946
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3946, best: 0.3906)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0748
  ‚Ä¢ Validation Loss: 0.3924
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3924, best: 0.3906)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1562
  ‚Ä¢ Validation Loss: 0.3976
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3976, best: 0.3906)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0822
  ‚Ä¢ Validation Loss: 0.3956
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3956, best: 0.3906)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1350
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3937, best: 0.3906)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1674
  ‚Ä¢ Validation Loss: 0.3942
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3942, best: 0.3906)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1186
  ‚Ä¢ Validation Loss: 0.3906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3906, best: 0.3906)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1496
  ‚Ä¢ Validation Loss: 0.3976
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3976, best: 0.3906)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1427
  ‚Ä¢ Validation Loss: 0.3932
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3932, best: 0.3906)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1462
  ‚Ä¢ Validation Loss: 0.3957
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3957, best: 0.3906)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1017
  ‚Ä¢ Validation Loss: 0.3901
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3901
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1254
  ‚Ä¢ Validation Loss: 0.3967
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3967, best: 0.3901)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1084
  ‚Ä¢ Validation Loss: 0.3913
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3913, best: 0.3901)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1321
  ‚Ä¢ Validation Loss: 0.3911
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3911, best: 0.3901)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1001
  ‚Ä¢ Validation Loss: 0.3974
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3974, best: 0.3901)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0906
  ‚Ä¢ Validation Loss: 0.3995
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3995, best: 0.3901)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1222
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3937, best: 0.3901)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1231
  ‚Ä¢ Validation Loss: 0.3919
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3919, best: 0.3901)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1158
  ‚Ä¢ Validation Loss: 0.3891
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3891
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1078
  ‚Ä¢ Validation Loss: 0.3927
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3927, best: 0.3891)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1582
  ‚Ä¢ Validation Loss: 0.3907
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3907, best: 0.3891)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.3915
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3915, best: 0.3891)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1374
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3885
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1654
  ‚Ä¢ Validation Loss: 0.3909
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3909, best: 0.3885)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.3953
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3953, best: 0.3885)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1180
  ‚Ä¢ Validation Loss: 0.3870
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3870
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1023
  ‚Ä¢ Validation Loss: 0.3899
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3899, best: 0.3870)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1552
  ‚Ä¢ Validation Loss: 0.3920
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3920, best: 0.3870)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1598
  ‚Ä¢ Validation Loss: 0.3912
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3912, best: 0.3870)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1145
  ‚Ä¢ Validation Loss: 0.4023
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4023, best: 0.3870)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0630
  ‚Ä¢ Validation Loss: 0.3915
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3915, best: 0.3870)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1801
  ‚Ä¢ Validation Loss: 0.3917
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3917, best: 0.3870)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1374
  ‚Ä¢ Validation Loss: 0.3895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3895, best: 0.3870)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1002
  ‚Ä¢ Validation Loss: 0.3894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3894, best: 0.3870)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.3935
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3935, best: 0.3870)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1442
  ‚Ä¢ Validation Loss: 0.3916
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3916, best: 0.3870)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1387
  ‚Ä¢ Validation Loss: 0.3859
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3859
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1288
  ‚Ä¢ Validation Loss: 0.3933
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3933, best: 0.3859)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1178
  ‚Ä¢ Validation Loss: 0.3889
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3889, best: 0.3859)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1555
  ‚Ä¢ Validation Loss: 0.3926
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3926, best: 0.3859)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1221
  ‚Ä¢ Validation Loss: 0.3937
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3937, best: 0.3859)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1118
  ‚Ä¢ Validation Loss: 0.3880
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3880, best: 0.3859)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0906
  ‚Ä¢ Validation Loss: 0.3886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3886, best: 0.3859)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1545
  ‚Ä¢ Validation Loss: 0.3928
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3928, best: 0.3859)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1120
  ‚Ä¢ Validation Loss: 0.3886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3886, best: 0.3859)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1355
  ‚Ä¢ Validation Loss: 0.3924
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3924, best: 0.3859)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1893
  ‚Ä¢ Validation Loss: 0.3896
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3896, best: 0.3859)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1234
  ‚Ä¢ Validation Loss: 0.3904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3904, best: 0.3859)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0921
  ‚Ä¢ Validation Loss: 0.3929
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3929, best: 0.3859)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1270
  ‚Ä¢ Validation Loss: 0.3900
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3900, best: 0.3859)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1398
  ‚Ä¢ Validation Loss: 0.3883
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3883, best: 0.3859)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.3864
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3864, best: 0.3859)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1239
  ‚Ä¢ Validation Loss: 0.3889
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3889, best: 0.3859)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1011
  ‚Ä¢ Validation Loss: 0.3884
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3884, best: 0.3859)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0905
  ‚Ä¢ Validation Loss: 0.3930
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3930, best: 0.3859)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0880
  ‚Ä¢ Validation Loss: 0.3925
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3925, best: 0.3859)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0881
  ‚Ä¢ Validation Loss: 0.3888
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3888, best: 0.3859)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1130
  ‚Ä¢ Validation Loss: 0.3866
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3866, best: 0.3859)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1493
  ‚Ä¢ Validation Loss: 0.3875
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3875, best: 0.3859)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.3872
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.3872, best: 0.3859)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3859
Total Epochs:   300
Models Saved:   ./Result/a3/Latin2
TensorBoard:    ./Result/a3/Latin2/tensorboard_logs
================================================================================

[01:56:05] Training completed. Best val loss: 0.3859

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9886, Recall=0.9894, F1=0.9890, IoU=0.9783
Paratext            : Precision=0.6746, Recall=0.6277, F1=0.6503, IoU=0.4818
Decoration          : Precision=0.8410, Recall=0.8775, F1=0.8589, IoU=0.7527
Main Text           : Precision=0.7992, Recall=0.8406, F1=0.8194, IoU=0.6940
Title               : Precision=0.8171, Recall=0.7446, F1=0.7792, IoU=0.6382
Chapter Headings    : Precision=0.6933, Recall=0.2681, F1=0.3866, IoU=0.2396

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8023
Mean Recall:    0.7246
Mean F1-Score:  0.7472
Mean IoU:       0.6308
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)
Output Directory: ./Result/a3/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF, SE-MSFE, or MSFA+MCT bottleneck)
   ‚Üí Encoder LR reduced to 0.01x (from 0.05x) for better stability
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up enabled (first 10 epochs)

Encoder     : LR=0.000001, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,954,879
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
   ‚Üí Warm-up: Manual LR control for first 10 epochs
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  üî• LR Warm-up: 10.0% complete (factor: 0.190x)
Results:
  ‚Ä¢ Train Loss: 1.0294
  ‚Ä¢ Validation Loss: 0.7397
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.7397
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7661
  ‚Ä¢ Validation Loss: 0.6778
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6778
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7527
  ‚Ä¢ Validation Loss: 0.6221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6221
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7003
  ‚Ä¢ Validation Loss: 0.6095
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.6095
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  üî• LR Warm-up: 50.0% complete (factor: 0.550x)
Results:
  ‚Ä¢ Train Loss: 0.6795
  ‚Ä¢ Validation Loss: 0.5985
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5985
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6694
  ‚Ä¢ Validation Loss: 0.5910
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6449
  ‚Ä¢ Validation Loss: 0.5843
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5843
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6362
  ‚Ä¢ Validation Loss: 0.5773
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5773
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6199
  ‚Ä¢ Validation Loss: 0.5707
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5707
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  üî• LR Warm-up: 100.0% complete (factor: 1.000x)
Results:
  ‚Ä¢ Train Loss: 0.6105
  ‚Ä¢ Validation Loss: 0.5639
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5639
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5965
  ‚Ä¢ Validation Loss: 0.5601
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5601
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5943
  ‚Ä¢ Validation Loss: 0.5574
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5574
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5761
  ‚Ä¢ Validation Loss: 0.5467
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5467
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5648
  ‚Ä¢ Validation Loss: 0.5391
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5391
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5576
  ‚Ä¢ Validation Loss: 0.5334
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5334
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5468
  ‚Ä¢ Validation Loss: 0.5304
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5304
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5392
  ‚Ä¢ Validation Loss: 0.5215
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5215
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5222
  ‚Ä¢ Validation Loss: 0.5165
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5165
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5141
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5098
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5145
  ‚Ä¢ Validation Loss: 0.5048
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5048
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4987
  ‚Ä¢ Validation Loss: 0.5022
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5022
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5061
  ‚Ä¢ Validation Loss: 0.5037
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5037, best: 0.5022)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4907
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4990
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4921
  ‚Ä¢ Validation Loss: 0.4955
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4955
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4951
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4980, best: 0.4955)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4772
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4947
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4795
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4906
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4712
  ‚Ä¢ Validation Loss: 0.4898
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4898
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4503
  ‚Ä¢ Validation Loss: 0.4871
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4871
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4737
  ‚Ä¢ Validation Loss: 0.4859
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4859
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4482
  ‚Ä¢ Validation Loss: 0.4831
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4831
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4562
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4805
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4645
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4827, best: 0.4805)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4475
  ‚Ä¢ Validation Loss: 0.4788
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4788
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4519
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4761
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4462
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4766, best: 0.4761)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4392
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4748
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4464
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4749, best: 0.4748)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4353
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4711
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4460
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4745, best: 0.4711)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4159
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4742, best: 0.4711)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4431
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4719, best: 0.4711)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4057
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4682
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4299
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4672
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4305
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4676, best: 0.4672)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3959
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4663
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3862
  ‚Ä¢ Validation Loss: 0.4664
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4664, best: 0.4663)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4228
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4676, best: 0.4663)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4340
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4658
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4000
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4647
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4169
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4654, best: 0.4647)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4248
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4647
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4221
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4660, best: 0.4647)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4337
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4638
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4264
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4639, best: 0.4638)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3854
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4641, best: 0.4638)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4071
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4634
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4211
  ‚Ä¢ Validation Loss: 0.4642
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4642, best: 0.4634)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4019
  ‚Ä¢ Validation Loss: 0.4643
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4643, best: 0.4634)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4129
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4646, best: 0.4634)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4365
  ‚Ä¢ Validation Loss: 0.4650
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4650, best: 0.4634)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4020
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4700, best: 0.4634)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3978
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4606
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4117
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4571
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4168
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4571
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4228
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4617, best: 0.4571)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4103
  ‚Ä¢ Validation Loss: 0.4555
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4555
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4186
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4658, best: 0.4555)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3916
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4554
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3807
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4523
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3458
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4512
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3842
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4512
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3803
  ‚Ä¢ Validation Loss: 0.4487
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4487
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4020
  ‚Ä¢ Validation Loss: 0.4459
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4459
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3869
  ‚Ä¢ Validation Loss: 0.4417
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4417
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3532
  ‚Ä¢ Validation Loss: 0.4465
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4465, best: 0.4417)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3868
  ‚Ä¢ Validation Loss: 0.4446
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4446, best: 0.4417)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3630
  ‚Ä¢ Validation Loss: 0.4476
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4476, best: 0.4417)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3810
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4479, best: 0.4417)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3760
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4478, best: 0.4417)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3675
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4474, best: 0.4417)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3829
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4455, best: 0.4417)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3871
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4466, best: 0.4417)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3643
  ‚Ä¢ Validation Loss: 0.4445
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4445, best: 0.4417)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3599
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4496, best: 0.4417)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3708
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4399
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3496
  ‚Ä¢ Validation Loss: 0.4424
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4424, best: 0.4399)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3685
  ‚Ä¢ Validation Loss: 0.4420
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4420, best: 0.4399)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3050
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4403, best: 0.4399)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3047
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4403, best: 0.4399)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3385
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4442, best: 0.4399)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3033
  ‚Ä¢ Validation Loss: 0.4420
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4420, best: 0.4399)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2782
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4419, best: 0.4399)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3210
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4359
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2902
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4353
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3033
  ‚Ä¢ Validation Loss: 0.4368
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4368, best: 0.4353)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3130
  ‚Ä¢ Validation Loss: 0.4410
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4410, best: 0.4353)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2340
  ‚Ä¢ Validation Loss: 0.4406
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4406, best: 0.4353)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2909
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4367, best: 0.4353)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2731
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.4345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2600
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4358, best: 0.4345)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2904
  ‚Ä¢ Validation Loss: 0.4364
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4364, best: 0.4345)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2661
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2540
  ‚Ä¢ Validation Loss: 0.4335
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4335
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2554
  ‚Ä¢ Validation Loss: 0.4326
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4326
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2430
  ‚Ä¢ Validation Loss: 0.4355
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4355, best: 0.4326)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3268
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4337, best: 0.4326)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.4329
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4329, best: 0.4326)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1988
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4322
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3184
  ‚Ä¢ Validation Loss: 0.4295
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4295
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2388
  ‚Ä¢ Validation Loss: 0.4316
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4316, best: 0.4295)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2722
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4315, best: 0.4295)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2381
  ‚Ä¢ Validation Loss: 0.4336
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4336, best: 0.4295)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2571
  ‚Ä¢ Validation Loss: 0.4321
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4321, best: 0.4295)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3054
  ‚Ä¢ Validation Loss: 0.4335
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4335, best: 0.4295)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3419
  ‚Ä¢ Validation Loss: 0.4322
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4322, best: 0.4295)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2527
  ‚Ä¢ Validation Loss: 0.4331
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4331, best: 0.4295)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2849
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4308, best: 0.4295)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2974
  ‚Ä¢ Validation Loss: 0.4312
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4312, best: 0.4295)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2858
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4302, best: 0.4295)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2841
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4311, best: 0.4295)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3065
  ‚Ä¢ Validation Loss: 0.4293
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4293
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3137
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4298, best: 0.4293)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3051
  ‚Ä¢ Validation Loss: 0.4281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4281
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3413
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2916
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4296, best: 0.4277)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2666
  ‚Ä¢ Validation Loss: 0.4281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4281, best: 0.4277)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3029
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4290, best: 0.4277)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2497
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4267
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2649
  ‚Ä¢ Validation Loss: 0.4269
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4269, best: 0.4267)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2567
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4252
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2804
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4284, best: 0.4252)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3007
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4261, best: 0.4252)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2613
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4267, best: 0.4252)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2378
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4256, best: 0.4252)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2796
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4265, best: 0.4252)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2938
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4264, best: 0.4252)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2692
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4260, best: 0.4252)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2468
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4254, best: 0.4252)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3239
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4253, best: 0.4252)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2733
  ‚Ä¢ Validation Loss: 0.4246
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4246
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2437
  ‚Ä¢ Validation Loss: 0.4248
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4248, best: 0.4246)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3186
  ‚Ä¢ Validation Loss: 0.4257
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4257, best: 0.4246)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2911
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4250, best: 0.4246)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2543
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4245
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2836
  ‚Ä¢ Validation Loss: 0.4251
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4251, best: 0.4245)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3025
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4256, best: 0.4245)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2861
  ‚Ä¢ Validation Loss: 0.4257
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4257, best: 0.4245)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2844
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4261, best: 0.4245)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2241
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4265, best: 0.4245)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2948
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4256, best: 0.4245)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2895
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4249, best: 0.4245)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2687
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4247, best: 0.4245)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3042
  ‚Ä¢ Validation Loss: 0.4235
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4235
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2354
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4250, best: 0.4235)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2740
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4242, best: 0.4235)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2555
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4243, best: 0.4235)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2443
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4259, best: 0.4235)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2656
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4242, best: 0.4235)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2931
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4253, best: 0.4235)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2045
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4290, best: 0.4235)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2509
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4276, best: 0.4235)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2547
  ‚Ä¢ Validation Loss: 0.4273
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4273, best: 0.4235)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2207
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4290, best: 0.4235)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3084
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4256, best: 0.4235)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2636
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4232
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2384
  ‚Ä¢ Validation Loss: 0.4311
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4311, best: 0.4232)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2993
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4315, best: 0.4232)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2759
  ‚Ä¢ Validation Loss: 0.4262
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4262, best: 0.4232)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2001
  ‚Ä¢ Validation Loss: 0.4240
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4240, best: 0.4232)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2854
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4261, best: 0.4232)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2613
  ‚Ä¢ Validation Loss: 0.4267
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4267, best: 0.4232)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1876
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4265, best: 0.4232)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2647
  ‚Ä¢ Validation Loss: 0.4255
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4255, best: 0.4232)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2546
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4259, best: 0.4232)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1564
  ‚Ä¢ Validation Loss: 0.4244
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4244, best: 0.4232)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2358
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4242, best: 0.4232)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2818
  ‚Ä¢ Validation Loss: 0.4224
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4224
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2278
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4264, best: 0.4224)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2715
  ‚Ä¢ Validation Loss: 0.4224
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4224
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2422
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4219
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2733
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4228, best: 0.4219)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2746
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2450
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4252, best: 0.4202)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2718
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4225, best: 0.4202)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2716
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4176
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2376
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4190, best: 0.4176)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2688
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4204, best: 0.4176)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4207, best: 0.4176)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2127
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4207, best: 0.4176)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2585
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4169
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2074
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4204, best: 0.4169)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2297
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4302, best: 0.4169)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2236
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4169)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2218
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2447
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4206, best: 0.4146)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2470
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4173, best: 0.4146)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2965
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4146)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2006
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4214, best: 0.4146)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1396
  ‚Ä¢ Validation Loss: 0.4180
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4180, best: 0.4146)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1193
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4212, best: 0.4146)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1217
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4149, best: 0.4146)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0893
  ‚Ä¢ Validation Loss: 0.4165
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4165, best: 0.4146)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1333
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4193, best: 0.4146)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0686
  ‚Ä¢ Validation Loss: 0.4164
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4164, best: 0.4146)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0993
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4112
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.4116
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4116, best: 0.4112)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.4168
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4168, best: 0.4112)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0956
  ‚Ä¢ Validation Loss: 0.4167
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4167, best: 0.4112)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0825
  ‚Ä¢ Validation Loss: 0.4145
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4145, best: 0.4112)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1698
  ‚Ä¢ Validation Loss: 0.4180
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4180, best: 0.4112)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1329
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4193, best: 0.4112)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0719
  ‚Ä¢ Validation Loss: 0.4224
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4224, best: 0.4112)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1192
  ‚Ä¢ Validation Loss: 0.4159
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4159, best: 0.4112)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1194
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4158, best: 0.4112)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1925
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4171, best: 0.4112)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1820
  ‚Ä¢ Validation Loss: 0.4080
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4080
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1219
  ‚Ä¢ Validation Loss: 0.4122
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4122, best: 0.4080)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1461
  ‚Ä¢ Validation Loss: 0.4139
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4139, best: 0.4080)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1346
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4147, best: 0.4080)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1511
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4158, best: 0.4080)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.4115
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4115, best: 0.4080)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1328
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4127, best: 0.4080)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1432
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4148, best: 0.4080)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1592
  ‚Ä¢ Validation Loss: 0.4111
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4111, best: 0.4080)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1697
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4135, best: 0.4080)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1342
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4132, best: 0.4080)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4094, best: 0.4080)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1338
  ‚Ä¢ Validation Loss: 0.4148
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4148, best: 0.4080)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1093
  ‚Ä¢ Validation Loss: 0.4094
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4094, best: 0.4080)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1183
  ‚Ä¢ Validation Loss: 0.4123
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4123, best: 0.4080)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1326
  ‚Ä¢ Validation Loss: 0.4126
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4126, best: 0.4080)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1827
  ‚Ä¢ Validation Loss: 0.4127
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4127, best: 0.4080)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1558
  ‚Ä¢ Validation Loss: 0.4078
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1463
  ‚Ä¢ Validation Loss: 0.4096
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4096, best: 0.4078)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1055
  ‚Ä¢ Validation Loss: 0.4105
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4105, best: 0.4078)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1420
  ‚Ä¢ Validation Loss: 0.4095
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4095, best: 0.4078)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1592
  ‚Ä¢ Validation Loss: 0.4112
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4112, best: 0.4078)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1125
  ‚Ä¢ Validation Loss: 0.4091
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4091, best: 0.4078)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1319
  ‚Ä¢ Validation Loss: 0.4099
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4099, best: 0.4078)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1054
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4135, best: 0.4078)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1192
  ‚Ä¢ Validation Loss: 0.4081
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4081, best: 0.4078)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1187
  ‚Ä¢ Validation Loss: 0.4069
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4069
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1423
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4073, best: 0.4069)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1004
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4073, best: 0.4069)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1080
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4072, best: 0.4069)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1266
  ‚Ä¢ Validation Loss: 0.4132
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4132, best: 0.4069)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1042
  ‚Ä¢ Validation Loss: 0.4070
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4070, best: 0.4069)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1667
  ‚Ä¢ Validation Loss: 0.4083
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4083, best: 0.4069)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1740
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4084, best: 0.4069)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4069
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4069
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1900
  ‚Ä¢ Validation Loss: 0.4076
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4076, best: 0.4069)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1580
  ‚Ä¢ Validation Loss: 0.4104
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4104, best: 0.4069)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1149
  ‚Ä¢ Validation Loss: 0.4082
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4082, best: 0.4069)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1435
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4084, best: 0.4069)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1003
  ‚Ä¢ Validation Loss: 0.4073
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4073, best: 0.4069)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1514
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4084, best: 0.4069)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1360
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4061
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0778
  ‚Ä¢ Validation Loss: 0.4065
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4065, best: 0.4061)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1308
  ‚Ä¢ Validation Loss: 0.4083
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4083, best: 0.4061)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1129
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4044
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1670
  ‚Ä¢ Validation Loss: 0.4075
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4075, best: 0.4044)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0646
  ‚Ä¢ Validation Loss: 0.4101
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4101, best: 0.4044)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.4078
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4078, best: 0.4044)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1346
  ‚Ä¢ Validation Loss: 0.4030
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4030
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1314
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4068, best: 0.4030)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4067, best: 0.4030)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1026
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4044, best: 0.4030)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4061, best: 0.4030)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0980
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4059, best: 0.4030)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1506
  ‚Ä¢ Validation Loss: 0.4061
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4061, best: 0.4030)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1167
  ‚Ä¢ Validation Loss: 0.4028
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4028
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0883
  ‚Ä¢ Validation Loss: 0.4052
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4052, best: 0.4028)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1663
  ‚Ä¢ Validation Loss: 0.4064
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4064, best: 0.4028)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1023
  ‚Ä¢ Validation Loss: 0.4072
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4072, best: 0.4028)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1820
  ‚Ä¢ Validation Loss: 0.4063
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4063, best: 0.4028)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1204
  ‚Ä¢ Validation Loss: 0.4100
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4100, best: 0.4028)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1791
  ‚Ä¢ Validation Loss: 0.4033
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4033, best: 0.4028)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1452
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4046, best: 0.4028)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1934
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4044, best: 0.4028)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1618
  ‚Ä¢ Validation Loss: 0.4067
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4067, best: 0.4028)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1282
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4036, best: 0.4028)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0990
  ‚Ä¢ Validation Loss: 0.4023
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4023
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1142
  ‚Ä¢ Validation Loss: 0.4035
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4035, best: 0.4023)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1122
  ‚Ä¢ Validation Loss: 0.4024
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4024, best: 0.4023)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1791
  ‚Ä¢ Validation Loss: 0.4009
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4009
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1612
  ‚Ä¢ Validation Loss: 0.4036
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4036, best: 0.4009)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1738
  ‚Ä¢ Validation Loss: 0.4024
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4024, best: 0.4009)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1101
  ‚Ä¢ Validation Loss: 0.4040
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4040, best: 0.4009)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1445
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4044, best: 0.4009)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2096
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4047, best: 0.4009)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1295
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4047, best: 0.4009)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1484
  ‚Ä¢ Validation Loss: 0.4070
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4070, best: 0.4009)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1634
  ‚Ä¢ Validation Loss: 0.4013
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4013, best: 0.4009)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1247
  ‚Ä¢ Validation Loss: 0.4045
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4045, best: 0.4009)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1224
  ‚Ä¢ Validation Loss: 0.4059
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4059, best: 0.4009)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1248
  ‚Ä¢ Validation Loss: 0.4048
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4048, best: 0.4009)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1407
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4044, best: 0.4009)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1888
  ‚Ä¢ Validation Loss: 0.4009
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4009, best: 0.4009)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1368
  ‚Ä¢ Validation Loss: 0.4011
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4011, best: 0.4009)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4009
Total Epochs:   300
Models Saved:   ./Result/a3/Latin14396
TensorBoard:    ./Result/a3/Latin14396/tensorboard_logs
================================================================================

[02:54:01] Training completed. Best val loss: 0.4009

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9870, Recall=0.9889, F1=0.9880, IoU=0.9762
Paratext            : Precision=0.6949, Recall=0.4622, F1=0.5551, IoU=0.3842
Decoration          : Precision=0.9229, Recall=0.9210, F1=0.9220, IoU=0.8552
Main Text           : Precision=0.8613, Recall=0.8738, F1=0.8675, IoU=0.7661
Title               : Precision=0.7730, Recall=0.8205, F1=0.7961, IoU=0.6612
Chapter Headings    : Precision=0.8764, Recall=0.4528, F1=0.5971, IoU=0.4256

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8526
Mean Recall:    0.7532
Mean F1-Score:  0.7876
Mean IoU:       0.6781
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)
Output Directory: ./Result/a3/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF or MSFA+MCT bottleneck)
   ‚Üí Encoder LR: 0.05x (default)
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up: DISABLED (only for SE-MSFE)

Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,954,879
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0629
  ‚Ä¢ Validation Loss: 0.7371
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7371
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7379
  ‚Ä¢ Validation Loss: 0.6337
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6337
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6811
  ‚Ä¢ Validation Loss: 0.5789
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5789
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6582
  ‚Ä¢ Validation Loss: 0.5719
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5719
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6368
  ‚Ä¢ Validation Loss: 0.5657
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5657
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6163
  ‚Ä¢ Validation Loss: 0.5595
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5595
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6179
  ‚Ä¢ Validation Loss: 0.5476
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5476
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5916
  ‚Ä¢ Validation Loss: 0.5476
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5476
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5809
  ‚Ä¢ Validation Loss: 0.5346
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5346
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5562
  ‚Ä¢ Validation Loss: 0.5236
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5236
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5427
  ‚Ä¢ Validation Loss: 0.5146
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5334
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5080
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5204
  ‚Ä¢ Validation Loss: 0.4951
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4951
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5131
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4938
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4968
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4870
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4942
  ‚Ä¢ Validation Loss: 0.4831
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4831
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4844
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4765
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4564
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4751
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4697
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4699
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4587
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4520
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4699, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4528
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4638
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4293
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4574
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4532
  ‚Ä¢ Validation Loss: 0.4564
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4564
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4434
  ‚Ä¢ Validation Loss: 0.4542
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4542
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4476
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4537
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4323
  ‚Ä¢ Validation Loss: 0.4515
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4515
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4314
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4494
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4337
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4506, best: 0.4494)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4263
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4479
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4085
  ‚Ä¢ Validation Loss: 0.4433
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4433
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4079
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4444, best: 0.4433)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4047
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4005
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4437, best: 0.4427)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4195
  ‚Ä¢ Validation Loss: 0.4418
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4418
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4178
  ‚Ä¢ Validation Loss: 0.4405
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4405
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4152
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4389
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4171
  ‚Ä¢ Validation Loss: 0.4395
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4395, best: 0.4389)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3952
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4403, best: 0.4389)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4166
  ‚Ä¢ Validation Loss: 0.4377
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4377
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4151
  ‚Ä¢ Validation Loss: 0.4350
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4350
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4144
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4365, best: 0.4350)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4145
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4363, best: 0.4350)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4194
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4352, best: 0.4350)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4046
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4357, best: 0.4350)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4127
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4363, best: 0.4350)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3805
  ‚Ä¢ Validation Loss: 0.4350
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4350
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4182
  ‚Ä¢ Validation Loss: 0.4360
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4360, best: 0.4350)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4199
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4359, best: 0.4350)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4207
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4369, best: 0.4350)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4080
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4359, best: 0.4350)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4127
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4369, best: 0.4350)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4249
  ‚Ä¢ Validation Loss: 0.4377
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4377, best: 0.4350)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3921
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4042
  ‚Ä¢ Validation Loss: 0.4263
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4263
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3687
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4266, best: 0.4263)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4013
  ‚Ä¢ Validation Loss: 0.4279
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4279, best: 0.4263)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3711
  ‚Ä¢ Validation Loss: 0.4262
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4262
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3806
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4239
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3918
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4229
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3833
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4211
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3883
  ‚Ä¢ Validation Loss: 0.4140
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4140
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3874
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4142, best: 0.4140)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3757
  ‚Ä¢ Validation Loss: 0.4123
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4123
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3785
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4152, best: 0.4123)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3728
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4147, best: 0.4123)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3940
  ‚Ä¢ Validation Loss: 0.4117
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4117
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3695
  ‚Ä¢ Validation Loss: 0.4084
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4084
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3618
  ‚Ä¢ Validation Loss: 0.4050
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4050
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3625
  ‚Ä¢ Validation Loss: 0.4088
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4088, best: 0.4050)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3583
  ‚Ä¢ Validation Loss: 0.4098
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4098, best: 0.4050)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3610
  ‚Ä¢ Validation Loss: 0.4047
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4047
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3605
  ‚Ä¢ Validation Loss: 0.4027
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4027
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3580
  ‚Ä¢ Validation Loss: 0.4044
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4044, best: 0.4027)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3474
  ‚Ä¢ Validation Loss: 0.4089
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4089, best: 0.4027)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3572
  ‚Ä¢ Validation Loss: 0.4001
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4001
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3464
  ‚Ä¢ Validation Loss: 0.4068
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4068, best: 0.4001)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3403
  ‚Ä¢ Validation Loss: 0.4046
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4046, best: 0.4001)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3439
  ‚Ä¢ Validation Loss: 0.4003
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4003, best: 0.4001)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3376
  ‚Ä¢ Validation Loss: 0.3996
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3996
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3391
  ‚Ä¢ Validation Loss: 0.4032
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4032, best: 0.3996)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3483
  ‚Ä¢ Validation Loss: 0.3976
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3976
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3280
  ‚Ä¢ Validation Loss: 0.3989
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3989, best: 0.3976)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3255
  ‚Ä¢ Validation Loss: 0.3944
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3944
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3106
  ‚Ä¢ Validation Loss: 0.3980
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3980, best: 0.3944)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3314
  ‚Ä¢ Validation Loss: 0.3924
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3924
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3302
  ‚Ä¢ Validation Loss: 0.3954
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3954, best: 0.3924)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3277
  ‚Ä¢ Validation Loss: 0.3921
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3171
  ‚Ä¢ Validation Loss: 0.3927
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3927, best: 0.3921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.3923
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3923, best: 0.3921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3127
  ‚Ä¢ Validation Loss: 0.3936
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3936, best: 0.3921)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3136
  ‚Ä¢ Validation Loss: 0.3902
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2998
  ‚Ä¢ Validation Loss: 0.3908
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3908, best: 0.3902)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3075
  ‚Ä¢ Validation Loss: 0.3890
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3890
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2972
  ‚Ä¢ Validation Loss: 0.3893
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3893, best: 0.3890)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3031
  ‚Ä¢ Validation Loss: 0.3893
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3893, best: 0.3890)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3071
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3885
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.3885
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3885, best: 0.3885)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2737
  ‚Ä¢ Validation Loss: 0.3861
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3861
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3167
  ‚Ä¢ Validation Loss: 0.3872
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.3872, best: 0.3861)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2920
  ‚Ä¢ Validation Loss: 0.3915
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3915, best: 0.3861)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2948
  ‚Ä¢ Validation Loss: 0.3879
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3879, best: 0.3861)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2906
  ‚Ä¢ Validation Loss: 0.3887
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3887, best: 0.3861)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2953
  ‚Ä¢ Validation Loss: 0.3889
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3889, best: 0.3861)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2891
  ‚Ä¢ Validation Loss: 0.3887
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3887, best: 0.3861)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2728
  ‚Ä¢ Validation Loss: 0.3852
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3852
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2855
  ‚Ä¢ Validation Loss: 0.3863
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3863, best: 0.3852)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2464
  ‚Ä¢ Validation Loss: 0.3880
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3880, best: 0.3852)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2811
  ‚Ä¢ Validation Loss: 0.3845
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3845
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2857
  ‚Ä¢ Validation Loss: 0.3863
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3863, best: 0.3845)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2805
  ‚Ä¢ Validation Loss: 0.3839
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3839
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2815
  ‚Ä¢ Validation Loss: 0.3837
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3837
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.3846
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3846, best: 0.3837)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2693
  ‚Ä¢ Validation Loss: 0.3843
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3843, best: 0.3837)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2874
  ‚Ä¢ Validation Loss: 0.3834
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3834
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2504
  ‚Ä¢ Validation Loss: 0.3832
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3832
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2665
  ‚Ä¢ Validation Loss: 0.3825
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3825
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2815
  ‚Ä¢ Validation Loss: 0.3859
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3859, best: 0.3825)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2904
  ‚Ä¢ Validation Loss: 0.3813
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3813
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2588
  ‚Ä¢ Validation Loss: 0.3838
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3838, best: 0.3813)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2531
  ‚Ä¢ Validation Loss: 0.3822
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3822, best: 0.3813)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.3826
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3826, best: 0.3813)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2474
  ‚Ä¢ Validation Loss: 0.3831
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3831, best: 0.3813)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2850
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3821, best: 0.3813)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2806
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3818, best: 0.3813)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2635
  ‚Ä¢ Validation Loss: 0.3842
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3842, best: 0.3813)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2496
  ‚Ä¢ Validation Loss: 0.3813
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3813, best: 0.3813)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.3821
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3821, best: 0.3813)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2628
  ‚Ä¢ Validation Loss: 0.3817
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3817, best: 0.3813)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2852
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3810
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2237
  ‚Ä¢ Validation Loss: 0.3824
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3824, best: 0.3810)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3042
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3814, best: 0.3810)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2991
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3810, best: 0.3810)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2677
  ‚Ä¢ Validation Loss: 0.3810
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3810, best: 0.3810)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2727
  ‚Ä¢ Validation Loss: 0.3823
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3823, best: 0.3810)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2477
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3818, best: 0.3810)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3028
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3814, best: 0.3810)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2631
  ‚Ä¢ Validation Loss: 0.3834
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3834, best: 0.3810)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2606
  ‚Ä¢ Validation Loss: 0.3816
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3816, best: 0.3810)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2862
  ‚Ä¢ Validation Loss: 0.3809
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.3809
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2370
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3814, best: 0.3809)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2480
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3814, best: 0.3809)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2699
  ‚Ä¢ Validation Loss: 0.3816
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3816, best: 0.3809)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2645
  ‚Ä¢ Validation Loss: 0.3824
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3824, best: 0.3809)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2712
  ‚Ä¢ Validation Loss: 0.3812
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3812, best: 0.3809)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2127
  ‚Ä¢ Validation Loss: 0.3820
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3820, best: 0.3809)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 147/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3145
  ‚Ä¢ Validation Loss: 0.3817
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3817, best: 0.3809)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2643
  ‚Ä¢ Validation Loss: 0.3814
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3814, best: 0.3809)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2580
  ‚Ä¢ Validation Loss: 0.3818
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.3818, best: 0.3809)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2880
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3830, best: 0.3809)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2570
  ‚Ä¢ Validation Loss: 0.3853
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3853, best: 0.3809)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2392
  ‚Ä¢ Validation Loss: 0.3863
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3863, best: 0.3809)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2252
  ‚Ä¢ Validation Loss: 0.3857
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3857, best: 0.3809)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2410
  ‚Ä¢ Validation Loss: 0.3849
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3849, best: 0.3809)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2448
  ‚Ä¢ Validation Loss: 0.3862
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3862, best: 0.3809)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2922
  ‚Ä¢ Validation Loss: 0.3825
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3825, best: 0.3809)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2356
  ‚Ä¢ Validation Loss: 0.3857
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3857, best: 0.3809)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2656
  ‚Ä¢ Validation Loss: 0.3801
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3801
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2821
  ‚Ä¢ Validation Loss: 0.3782
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3782
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2532
  ‚Ä¢ Validation Loss: 0.3793
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3793, best: 0.3782)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2353
  ‚Ä¢ Validation Loss: 0.3778
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3778
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2345
  ‚Ä¢ Validation Loss: 0.3776
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3776
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 163/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.2981
  ‚Ä¢ Validation Loss: 0.3790
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3790, best: 0.3776)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 164/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3013
  ‚Ä¢ Validation Loss: 0.3802
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3802, best: 0.3776)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2520
  ‚Ä¢ Validation Loss: 0.3762
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3762
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2582
  ‚Ä¢ Validation Loss: 0.3767
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3767, best: 0.3762)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2740
  ‚Ä¢ Validation Loss: 0.3765
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3765, best: 0.3762)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2661
  ‚Ä¢ Validation Loss: 0.3793
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3793, best: 0.3762)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2714
  ‚Ä¢ Validation Loss: 0.3788
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3788, best: 0.3762)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2759
  ‚Ä¢ Validation Loss: 0.3753
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3753
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2579
  ‚Ä¢ Validation Loss: 0.3763
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3763, best: 0.3753)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2575
  ‚Ä¢ Validation Loss: 0.3742
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3742
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2738
  ‚Ä¢ Validation Loss: 0.3729
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3729
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2375
  ‚Ä¢ Validation Loss: 0.3717
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3717
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1978
  ‚Ä¢ Validation Loss: 0.3706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3706
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.3762
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3762, best: 0.3706)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2767
  ‚Ä¢ Validation Loss: 0.3753
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3753, best: 0.3706)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2740
  ‚Ä¢ Validation Loss: 0.3713
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3713, best: 0.3706)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2556
  ‚Ä¢ Validation Loss: 0.3720
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3720, best: 0.3706)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2782
  ‚Ä¢ Validation Loss: 0.3710
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3710, best: 0.3706)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2462
  ‚Ä¢ Validation Loss: 0.3706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3706
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2736
  ‚Ä¢ Validation Loss: 0.3830
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3830, best: 0.3706)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2555
  ‚Ä¢ Validation Loss: 0.3706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3706, best: 0.3706)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2302
  ‚Ä¢ Validation Loss: 0.3695
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3695
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2522
  ‚Ä¢ Validation Loss: 0.3697
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3697, best: 0.3695)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2632
  ‚Ä¢ Validation Loss: 0.3685
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3685
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1423
  ‚Ä¢ Validation Loss: 0.3722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3722, best: 0.3685)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1570
  ‚Ä¢ Validation Loss: 0.3681
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.3681
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1959
  ‚Ä¢ Validation Loss: 0.3699
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3699, best: 0.3681)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1319
  ‚Ä¢ Validation Loss: 0.3726
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3726, best: 0.3681)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1553
  ‚Ä¢ Validation Loss: 0.3692
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.3692, best: 0.3681)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1935
  ‚Ä¢ Validation Loss: 0.3695
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3695, best: 0.3681)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1765
  ‚Ä¢ Validation Loss: 0.3677
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3677
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1408
  ‚Ä¢ Validation Loss: 0.3715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3715, best: 0.3677)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2037
  ‚Ä¢ Validation Loss: 0.3661
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3661
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1881
  ‚Ä¢ Validation Loss: 0.3654
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1744
  ‚Ä¢ Validation Loss: 0.3678
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3678, best: 0.3654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1414
  ‚Ä¢ Validation Loss: 0.3700
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3700, best: 0.3654)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1894
  ‚Ä¢ Validation Loss: 0.3670
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3670, best: 0.3654)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1416
  ‚Ä¢ Validation Loss: 0.3656
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.3656, best: 0.3654)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1253
  ‚Ä¢ Validation Loss: 0.3706
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3706, best: 0.3654)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1709
  ‚Ä¢ Validation Loss: 0.3690
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3690, best: 0.3654)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1619
  ‚Ä¢ Validation Loss: 0.3634
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3634
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1974
  ‚Ä¢ Validation Loss: 0.3655
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3655, best: 0.3634)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1767
  ‚Ä¢ Validation Loss: 0.3669
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3669, best: 0.3634)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1547
  ‚Ä¢ Validation Loss: 0.3689
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3689, best: 0.3634)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1557
  ‚Ä¢ Validation Loss: 0.3664
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3664, best: 0.3634)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1191
  ‚Ä¢ Validation Loss: 0.3652
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3652, best: 0.3634)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1532
  ‚Ä¢ Validation Loss: 0.3677
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3677, best: 0.3634)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1576
  ‚Ä¢ Validation Loss: 0.3686
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3686, best: 0.3634)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1027
  ‚Ä¢ Validation Loss: 0.3653
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3653, best: 0.3634)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2123
  ‚Ä¢ Validation Loss: 0.3646
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3646, best: 0.3634)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1748
  ‚Ä¢ Validation Loss: 0.3637
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3637, best: 0.3634)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1378
  ‚Ä¢ Validation Loss: 0.3642
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3642, best: 0.3634)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1690
  ‚Ä¢ Validation Loss: 0.3632
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3632
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1409
  ‚Ä¢ Validation Loss: 0.3713
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3713, best: 0.3632)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1366
  ‚Ä¢ Validation Loss: 0.3626
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3626
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1418
  ‚Ä¢ Validation Loss: 0.3644
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3644, best: 0.3626)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.3650
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3650, best: 0.3626)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1822
  ‚Ä¢ Validation Loss: 0.3626
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3626, best: 0.3626)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1649
  ‚Ä¢ Validation Loss: 0.3619
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3619
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1600
  ‚Ä¢ Validation Loss: 0.3669
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3669, best: 0.3619)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1525
  ‚Ä¢ Validation Loss: 0.3657
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.3657, best: 0.3619)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1944
  ‚Ä¢ Validation Loss: 0.3604
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.3604
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1941
  ‚Ä¢ Validation Loss: 0.3620
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3620, best: 0.3604)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1835
  ‚Ä¢ Validation Loss: 0.3661
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3661, best: 0.3604)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1773
  ‚Ä¢ Validation Loss: 0.3613
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3613, best: 0.3604)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1476
  ‚Ä¢ Validation Loss: 0.3629
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3629, best: 0.3604)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1677
  ‚Ä¢ Validation Loss: 0.3650
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3650, best: 0.3604)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1470
  ‚Ä¢ Validation Loss: 0.3640
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3640, best: 0.3604)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1802
  ‚Ä¢ Validation Loss: 0.3597
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3597
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1379
  ‚Ä¢ Validation Loss: 0.3659
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3659, best: 0.3597)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1779
  ‚Ä¢ Validation Loss: 0.3621
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3621, best: 0.3597)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1678
  ‚Ä¢ Validation Loss: 0.3635
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3635, best: 0.3597)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1242
  ‚Ä¢ Validation Loss: 0.3661
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3661, best: 0.3597)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0914
  ‚Ä¢ Validation Loss: 0.3608
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3608, best: 0.3597)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2133
  ‚Ä¢ Validation Loss: 0.3584
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3584
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1214
  ‚Ä¢ Validation Loss: 0.3615
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3615, best: 0.3584)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1665
  ‚Ä¢ Validation Loss: 0.3588
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3588, best: 0.3584)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2076
  ‚Ä¢ Validation Loss: 0.3579
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.3579
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1991
  ‚Ä¢ Validation Loss: 0.3594
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3594, best: 0.3579)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1437
  ‚Ä¢ Validation Loss: 0.3609
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3609, best: 0.3579)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1361
  ‚Ä¢ Validation Loss: 0.3620
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3620, best: 0.3579)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1118
  ‚Ä¢ Validation Loss: 0.3602
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3602, best: 0.3579)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1486
  ‚Ä¢ Validation Loss: 0.3594
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3594, best: 0.3579)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1524
  ‚Ä¢ Validation Loss: 0.3632
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3632, best: 0.3579)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1360
  ‚Ä¢ Validation Loss: 0.3680
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3680, best: 0.3579)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1133
  ‚Ä¢ Validation Loss: 0.3684
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3684, best: 0.3579)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1489
  ‚Ä¢ Validation Loss: 0.3611
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3611, best: 0.3579)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1884
  ‚Ä¢ Validation Loss: 0.3599
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3599, best: 0.3579)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1777
  ‚Ä¢ Validation Loss: 0.3618
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.3618, best: 0.3579)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1187
  ‚Ä¢ Validation Loss: 0.3616
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3616, best: 0.3579)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1652
  ‚Ä¢ Validation Loss: 0.3646
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3646, best: 0.3579)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1446
  ‚Ä¢ Validation Loss: 0.3578
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3578
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2175
  ‚Ä¢ Validation Loss: 0.3600
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3600, best: 0.3578)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1666
  ‚Ä¢ Validation Loss: 0.3620
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3620, best: 0.3578)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1761
  ‚Ä¢ Validation Loss: 0.3585
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3585, best: 0.3578)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1871
  ‚Ä¢ Validation Loss: 0.3630
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3630, best: 0.3578)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1677
  ‚Ä¢ Validation Loss: 0.3580
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3580, best: 0.3578)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1001
  ‚Ä¢ Validation Loss: 0.3617
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3617, best: 0.3578)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1460
  ‚Ä¢ Validation Loss: 0.3584
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3584, best: 0.3578)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1641
  ‚Ä¢ Validation Loss: 0.3577
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3577
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1974
  ‚Ä¢ Validation Loss: 0.3572
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3572
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1402
  ‚Ä¢ Validation Loss: 0.3575
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3575, best: 0.3572)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1325
  ‚Ä¢ Validation Loss: 0.3587
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3587, best: 0.3572)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1797
  ‚Ä¢ Validation Loss: 0.3593
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3593, best: 0.3572)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1741
  ‚Ä¢ Validation Loss: 0.3603
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3603, best: 0.3572)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1796
  ‚Ä¢ Validation Loss: 0.3565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3565
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1500
  ‚Ä¢ Validation Loss: 0.3594
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3594, best: 0.3565)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1862
  ‚Ä¢ Validation Loss: 0.3584
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3584, best: 0.3565)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1935
  ‚Ä¢ Validation Loss: 0.3604
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3604, best: 0.3565)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1662
  ‚Ä¢ Validation Loss: 0.3568
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3568, best: 0.3565)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1526
  ‚Ä¢ Validation Loss: 0.3579
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3579, best: 0.3565)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1337
  ‚Ä¢ Validation Loss: 0.3558
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.3558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1890
  ‚Ä¢ Validation Loss: 0.3565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3565, best: 0.3558)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1183
  ‚Ä¢ Validation Loss: 0.3565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3565, best: 0.3558)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1421
  ‚Ä¢ Validation Loss: 0.3574
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3574, best: 0.3558)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1979
  ‚Ä¢ Validation Loss: 0.3598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.3598, best: 0.3558)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2107
  ‚Ä¢ Validation Loss: 0.3585
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3585, best: 0.3558)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1604
  ‚Ä¢ Validation Loss: 0.3570
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3570, best: 0.3558)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1547
  ‚Ä¢ Validation Loss: 0.3586
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3586, best: 0.3558)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1436
  ‚Ä¢ Validation Loss: 0.3565
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3565, best: 0.3558)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1953
  ‚Ä¢ Validation Loss: 0.3571
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3571, best: 0.3558)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2174
  ‚Ä¢ Validation Loss: 0.3578
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3578, best: 0.3558)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2125
  ‚Ä¢ Validation Loss: 0.3550
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.3550
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2129
  ‚Ä¢ Validation Loss: 0.3560
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3560, best: 0.3550)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1604
  ‚Ä¢ Validation Loss: 0.3589
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3589, best: 0.3550)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1504
  ‚Ä¢ Validation Loss: 0.3559
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3559, best: 0.3550)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1845
  ‚Ä¢ Validation Loss: 0.3587
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3587, best: 0.3550)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.3554
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3554, best: 0.3550)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1649
  ‚Ä¢ Validation Loss: 0.3565
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3565, best: 0.3550)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1504
  ‚Ä¢ Validation Loss: 0.3578
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3578, best: 0.3550)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1291
  ‚Ä¢ Validation Loss: 0.3571
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3571, best: 0.3550)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1219
  ‚Ä¢ Validation Loss: 0.3569
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3569, best: 0.3550)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2295
  ‚Ä¢ Validation Loss: 0.3566
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3566, best: 0.3550)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1616
  ‚Ä¢ Validation Loss: 0.3572
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3572, best: 0.3550)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1751
  ‚Ä¢ Validation Loss: 0.3561
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3561, best: 0.3550)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1648
  ‚Ä¢ Validation Loss: 0.3564
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3564, best: 0.3550)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1838
  ‚Ä¢ Validation Loss: 0.3551
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.3551, best: 0.3550)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1915
  ‚Ä¢ Validation Loss: 0.3543
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    ‚úì New best checkpoint saved! Val loss: 0.3543
    ‚úì Improvement detected! Resetting patience counter.

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3543
Total Epochs:   300
Models Saved:   ./Result/a3/Latin16746
TensorBoard:    ./Result/a3/Latin16746/tensorboard_logs
================================================================================

[03:54:12] Training completed. Best val loss: 0.3543

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9867, Recall=0.9910, F1=0.9888, IoU=0.9779
Paratext            : Precision=0.7480, Recall=0.8024, F1=0.7742, IoU=0.6316
Decoration          : Precision=0.9740, Recall=0.9021, F1=0.9367, IoU=0.8809
Main Text           : Precision=0.9043, Recall=0.9097, F1=0.9070, IoU=0.8298
Title               : Precision=0.7763, Recall=0.7215, F1=0.7479, IoU=0.5973
Chapter Headings    : Precision=0.9243, Recall=0.6970, F1=0.7947, IoU=0.6593

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8856
Mean Recall:    0.8373
Mean F1-Score:  0.8582
Mean IoU:       0.7628
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)
Output Directory: ./Result/a3/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF or MSFA+MCT bottleneck)
   ‚Üí Encoder LR: 0.05x (default)
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up: DISABLED (only for SE-MSFE)

Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,954,814
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.3345
  ‚Ä¢ Validation Loss: 0.8840
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8840
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8114
  ‚Ä¢ Validation Loss: 0.6421
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6421
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6948
  ‚Ä¢ Validation Loss: 0.5732
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5732
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6779
  ‚Ä¢ Validation Loss: 0.5676
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6383
  ‚Ä¢ Validation Loss: 0.5552
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5552
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6093
  ‚Ä¢ Validation Loss: 0.5533
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5533
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6229
  ‚Ä¢ Validation Loss: 0.5385
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5385
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5931
  ‚Ä¢ Validation Loss: 0.5344
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5344
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5670
  ‚Ä¢ Validation Loss: 0.5355
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5355, best: 0.5344)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5513
  ‚Ä¢ Validation Loss: 0.5222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5222
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4860
  ‚Ä¢ Validation Loss: 0.5240
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5240, best: 0.5222)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4420
  ‚Ä¢ Validation Loss: 0.5218
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5218
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4713
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5157
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4720
  ‚Ä¢ Validation Loss: 0.5159
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5159, best: 0.5157)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4270
  ‚Ä¢ Validation Loss: 0.5089
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5089
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4271
  ‚Ä¢ Validation Loss: 0.5092
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5092, best: 0.5089)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4841
  ‚Ä¢ Validation Loss: 0.5129
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5129, best: 0.5089)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4896
  ‚Ä¢ Validation Loss: 0.5193
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5193, best: 0.5089)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4726
  ‚Ä¢ Validation Loss: 0.5064
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5064
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3970
  ‚Ä¢ Validation Loss: 0.5066
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5066, best: 0.5064)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3979
  ‚Ä¢ Validation Loss: 0.5068
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5068, best: 0.5064)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3693
  ‚Ä¢ Validation Loss: 0.5077
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5077, best: 0.5064)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4465
  ‚Ä¢ Validation Loss: 0.5037
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5037
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4033
  ‚Ä¢ Validation Loss: 0.5014
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5014
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3629
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5036, best: 0.5014)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4688
  ‚Ä¢ Validation Loss: 0.5006
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5006
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3899
  ‚Ä¢ Validation Loss: 0.5009
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5009, best: 0.5006)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4178
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4975
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4488
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4969
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4621
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4960
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4140
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4974, best: 0.4960)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4463
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4960, best: 0.4960)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4553
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4957
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4028
  ‚Ä¢ Validation Loss: 0.4943
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4943
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4534
  ‚Ä¢ Validation Loss: 0.4941
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4941
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4193
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4933
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4449
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4254
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4942, best: 0.4902)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4356
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4906, best: 0.4902)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4409
  ‚Ä¢ Validation Loss: 0.4912
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4912, best: 0.4902)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4457
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3861
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4927, best: 0.4895)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3869
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4909, best: 0.4895)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4321
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4911, best: 0.4895)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4484
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4916, best: 0.4895)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4597
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4903, best: 0.4895)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4398
  ‚Ä¢ Validation Loss: 0.4923
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4923, best: 0.4895)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4067
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4907, best: 0.4895)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3921
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4925, best: 0.4895)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3863
  ‚Ä¢ Validation Loss: 0.4920
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4920, best: 0.4895)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3859
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4903, best: 0.4895)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4278
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4869
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3851
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4872, best: 0.4869)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3987
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4846
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3982
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4856, best: 0.4846)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4406
  ‚Ä¢ Validation Loss: 0.4818
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4818
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4130
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4801
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3888
  ‚Ä¢ Validation Loss: 0.4831
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4831, best: 0.4801)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4002
  ‚Ä¢ Validation Loss: 0.4893
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4893, best: 0.4801)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4161
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4856, best: 0.4801)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3996
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4759
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4304
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4837, best: 0.4759)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3743
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4807, best: 0.4759)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4308
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4767, best: 0.4759)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3456
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4756
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3932
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4710
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3893
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4729, best: 0.4710)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3958
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4722, best: 0.4710)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3423
  ‚Ä¢ Validation Loss: 0.4720
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4720, best: 0.4710)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3550
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4673
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3873
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4655
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3557
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4713, best: 0.4655)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3686
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4678, best: 0.4655)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3802
  ‚Ä¢ Validation Loss: 0.4619
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4619
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3415
  ‚Ä¢ Validation Loss: 0.4635
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4635, best: 0.4619)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3834
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4640, best: 0.4619)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3794
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4663, best: 0.4619)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3787
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4647, best: 0.4619)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3086
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4639, best: 0.4619)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3569
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4586
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3760
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4590, best: 0.4586)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3331
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4603, best: 0.4586)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3539
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4568
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3037
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4592, best: 0.4568)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3366
  ‚Ä¢ Validation Loss: 0.4553
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4553
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3271
  ‚Ä¢ Validation Loss: 0.4563
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4563, best: 0.4553)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3656
  ‚Ä¢ Validation Loss: 0.4560
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4560, best: 0.4553)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3560
  ‚Ä¢ Validation Loss: 0.4543
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4543
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3595
  ‚Ä¢ Validation Loss: 0.4540
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4540
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3209
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4532
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3735
  ‚Ä¢ Validation Loss: 0.4552
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4552, best: 0.4532)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3512
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4527
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3545
  ‚Ä¢ Validation Loss: 0.4534
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4534, best: 0.4527)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3032
  ‚Ä¢ Validation Loss: 0.4539
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4539, best: 0.4527)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3298
  ‚Ä¢ Validation Loss: 0.4564
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4564, best: 0.4527)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3358
  ‚Ä¢ Validation Loss: 0.4536
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4536, best: 0.4527)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2820
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4554, best: 0.4527)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3003
  ‚Ä¢ Validation Loss: 0.4522
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4522
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3065
  ‚Ä¢ Validation Loss: 0.4520
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4520
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2868
  ‚Ä¢ Validation Loss: 0.4544
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4544, best: 0.4520)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2885
  ‚Ä¢ Validation Loss: 0.4503
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4503
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2958
  ‚Ä¢ Validation Loss: 0.4496
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4496
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2765
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4510, best: 0.4496)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3103
  ‚Ä¢ Validation Loss: 0.4513
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4513, best: 0.4496)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3221
  ‚Ä¢ Validation Loss: 0.4502
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4502, best: 0.4496)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2785
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4504, best: 0.4496)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2828
  ‚Ä¢ Validation Loss: 0.4488
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4488
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2652
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4460
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3194
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4472, best: 0.4460)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3152
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4474, best: 0.4460)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3141
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4493, best: 0.4460)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3053
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4481, best: 0.4460)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2625
  ‚Ä¢ Validation Loss: 0.4495
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4495, best: 0.4460)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2705
  ‚Ä¢ Validation Loss: 0.4476
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4476, best: 0.4460)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2874
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4466, best: 0.4460)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3212
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4455
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3157
  ‚Ä¢ Validation Loss: 0.4453
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4453
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2405
  ‚Ä¢ Validation Loss: 0.4465
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4465, best: 0.4453)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3120
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4467, best: 0.4453)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2493
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4462, best: 0.4453)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2542
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4458, best: 0.4453)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2941
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4471, best: 0.4453)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2847
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4461, best: 0.4453)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2804
  ‚Ä¢ Validation Loss: 0.4446
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4446
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2497
  ‚Ä¢ Validation Loss: 0.4473
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4473, best: 0.4446)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2795
  ‚Ä¢ Validation Loss: 0.4450
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4450, best: 0.4446)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2871
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4439
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2034
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4441, best: 0.4439)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2755
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4466, best: 0.4439)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3365
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4447, best: 0.4439)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2769
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4458, best: 0.4439)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2982
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4436
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3432
  ‚Ä¢ Validation Loss: 0.4456
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4456, best: 0.4436)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2824
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4455, best: 0.4436)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3075
  ‚Ä¢ Validation Loss: 0.4450
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4450, best: 0.4436)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2448
  ‚Ä¢ Validation Loss: 0.4449
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4449, best: 0.4436)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2746
  ‚Ä¢ Validation Loss: 0.4451
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4451, best: 0.4436)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3117
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4444, best: 0.4436)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2895
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4455, best: 0.4436)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2976
  ‚Ä¢ Validation Loss: 0.4450
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4450, best: 0.4436)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3109
  ‚Ä¢ Validation Loss: 0.4445
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4445, best: 0.4436)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2741
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4444, best: 0.4436)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2866
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4440, best: 0.4436)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2911
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4439, best: 0.4436)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3182
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4439, best: 0.4436)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3116
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4440, best: 0.4436)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3164
  ‚Ä¢ Validation Loss: 0.4435
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4435
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2551
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4437, best: 0.4435)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3140
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4436, best: 0.4435)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3165
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4440, best: 0.4435)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3133
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4461, best: 0.4435)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2881
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4430
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2960
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4440, best: 0.4430)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2899
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4455, best: 0.4430)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2927
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4462, best: 0.4430)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2728
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4436, best: 0.4430)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2508
  ‚Ä¢ Validation Loss: 0.4434
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4434, best: 0.4430)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3117
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2454
  ‚Ä¢ Validation Loss: 0.4465
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4465, best: 0.4427)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2179
  ‚Ä¢ Validation Loss: 0.4444
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4444, best: 0.4427)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3053
  ‚Ä¢ Validation Loss: 0.4446
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4446, best: 0.4427)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2509
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4431, best: 0.4427)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2853
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4441, best: 0.4427)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2999
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4439, best: 0.4427)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2556
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4427, best: 0.4427)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4386
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2447
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4389, best: 0.4386)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2974
  ‚Ä¢ Validation Loss: 0.4400
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4400, best: 0.4386)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2992
  ‚Ä¢ Validation Loss: 0.4404
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4404, best: 0.4386)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2830
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4390, best: 0.4386)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2501
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4399, best: 0.4386)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2969
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4370
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3052
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4354
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3232
  ‚Ä¢ Validation Loss: 0.4378
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4378, best: 0.4354)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2835
  ‚Ä¢ Validation Loss: 0.4334
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4334
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2617
  ‚Ä¢ Validation Loss: 0.4324
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4324
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2517
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4300
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2402
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4333, best: 0.4300)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2923
  ‚Ä¢ Validation Loss: 0.4315
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4315, best: 0.4300)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3020
  ‚Ä¢ Validation Loss: 0.4337
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4337, best: 0.4300)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2718
  ‚Ä¢ Validation Loss: 0.4309
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4309, best: 0.4300)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2370
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4363, best: 0.4300)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2982
  ‚Ä¢ Validation Loss: 0.4312
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4312, best: 0.4300)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2367
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4276
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2648
  ‚Ä¢ Validation Loss: 0.4312
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4312, best: 0.4276)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2843
  ‚Ä¢ Validation Loss: 0.4302
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4302, best: 0.4276)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2999
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4288, best: 0.4276)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2685
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4274
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2926
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4301, best: 0.4274)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2906
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4289, best: 0.4274)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2722
  ‚Ä¢ Validation Loss: 0.4277
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4277, best: 0.4274)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4296, best: 0.4274)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 193/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3027
  ‚Ä¢ Validation Loss: 0.4298
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4298, best: 0.4274)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2428
  ‚Ä¢ Validation Loss: 0.4289
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4289, best: 0.4274)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2092
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4266
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1457
  ‚Ä¢ Validation Loss: 0.4282
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4282, best: 0.4266)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1549
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4274, best: 0.4266)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1845
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4299, best: 0.4266)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1530
  ‚Ä¢ Validation Loss: 0.4265
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4265
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1390
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    ‚úì New best checkpoint saved! Val loss: 0.4259
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1832
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4276, best: 0.4259)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2237
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4239
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1377
  ‚Ä¢ Validation Loss: 0.4286
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4286, best: 0.4239)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1281
  ‚Ä¢ Validation Loss: 0.4323
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4323, best: 0.4239)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1972
  ‚Ä¢ Validation Loss: 0.4241
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4241, best: 0.4239)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1596
  ‚Ä¢ Validation Loss: 0.4245
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4245, best: 0.4239)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4237
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1647
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4288, best: 0.4237)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1791
  ‚Ä¢ Validation Loss: 0.4254
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4254, best: 0.4237)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1443
  ‚Ä¢ Validation Loss: 0.4253
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4253, best: 0.4237)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1972
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4280, best: 0.4237)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1394
  ‚Ä¢ Validation Loss: 0.4278
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4278, best: 0.4237)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1514
  ‚Ä¢ Validation Loss: 0.4264
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4264, best: 0.4237)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2194
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4256, best: 0.4237)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1715
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4247, best: 0.4237)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1903
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4238, best: 0.4237)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1523
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4252, best: 0.4237)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1553
  ‚Ä¢ Validation Loss: 0.4251
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4251, best: 0.4237)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1883
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4225
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1823
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4219
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1523
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4234, best: 0.4219)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2178
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4234, best: 0.4219)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1701
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4232, best: 0.4219)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1495
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4229, best: 0.4219)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1323
  ‚Ä¢ Validation Loss: 0.4260
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4260, best: 0.4219)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2320
  ‚Ä¢ Validation Loss: 0.4249
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4249, best: 0.4219)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2257
  ‚Ä¢ Validation Loss: 0.4241
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4241, best: 0.4219)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1444
  ‚Ä¢ Validation Loss: 0.4234
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4234, best: 0.4219)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1461
  ‚Ä¢ Validation Loss: 0.4208
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4208
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.4219
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4219, best: 0.4208)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1431
  ‚Ä¢ Validation Loss: 0.4227
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4227, best: 0.4208)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1496
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4214, best: 0.4208)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2216
  ‚Ä¢ Validation Loss: 0.4237
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4237, best: 0.4208)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2181
  ‚Ä¢ Validation Loss: 0.4250
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4250, best: 0.4208)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1256
  ‚Ä¢ Validation Loss: 0.4229
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4229, best: 0.4208)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.4283
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4283, best: 0.4208)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1810
  ‚Ä¢ Validation Loss: 0.4198
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4198
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1307
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4202, best: 0.4198)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1962
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4243, best: 0.4198)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1572
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4236, best: 0.4198)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1337
  ‚Ä¢ Validation Loss: 0.4211
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4211, best: 0.4198)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1913
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4214, best: 0.4198)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1955
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4242, best: 0.4198)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0969
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4220, best: 0.4198)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1887
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4193
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1214
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4226, best: 0.4193)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1793
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4212, best: 0.4193)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1452
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4217, best: 0.4193)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2042
  ‚Ä¢ Validation Loss: 0.4226
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4226, best: 0.4193)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1539
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4247, best: 0.4193)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1526
  ‚Ä¢ Validation Loss: 0.4222
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4222, best: 0.4193)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1908
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4236, best: 0.4193)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1298
  ‚Ä¢ Validation Loss: 0.4220
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4220, best: 0.4193)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4217, best: 0.4193)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1749
  ‚Ä¢ Validation Loss: 0.4247
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4247, best: 0.4193)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1980
  ‚Ä¢ Validation Loss: 0.4214
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4214, best: 0.4193)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1910
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4199, best: 0.4193)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2291
  ‚Ä¢ Validation Loss: 0.4196
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4196, best: 0.4193)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1961
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4193
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1542
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4199, best: 0.4193)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1885
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4221, best: 0.4193)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2283
  ‚Ä¢ Validation Loss: 0.4194
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4194, best: 0.4193)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1443
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4191
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1524
  ‚Ä¢ Validation Loss: 0.4252
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4252, best: 0.4191)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1454
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4193, best: 0.4191)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2335
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4199, best: 0.4191)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1423
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4207, best: 0.4191)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1607
  ‚Ä¢ Validation Loss: 0.4205
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4205, best: 0.4191)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2018
  ‚Ä¢ Validation Loss: 0.4201
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4201, best: 0.4191)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1708
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4216, best: 0.4191)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2013
  ‚Ä¢ Validation Loss: 0.4204
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4204, best: 0.4191)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1632
  ‚Ä¢ Validation Loss: 0.4233
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4233, best: 0.4191)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1842
  ‚Ä¢ Validation Loss: 0.4187
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4187
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1556
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4212, best: 0.4187)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1973
  ‚Ä¢ Validation Loss: 0.4223
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4223, best: 0.4187)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1657
  ‚Ä¢ Validation Loss: 0.4215
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4215, best: 0.4187)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1656
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4195, best: 0.4187)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1941
  ‚Ä¢ Validation Loss: 0.4199
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4199, best: 0.4187)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1900
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4195, best: 0.4187)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2074
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4195, best: 0.4187)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2080
  ‚Ä¢ Validation Loss: 0.4197
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4197, best: 0.4187)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1694
  ‚Ä¢ Validation Loss: 0.4186
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4186
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1783
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4193, best: 0.4186)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1887
  ‚Ä¢ Validation Loss: 0.4193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4193, best: 0.4186)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1273
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4190, best: 0.4186)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1652
  ‚Ä¢ Validation Loss: 0.4184
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4184
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2182
  ‚Ä¢ Validation Loss: 0.4203
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4203, best: 0.4184)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1732
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4202, best: 0.4184)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1970
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4195, best: 0.4184)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1845
  ‚Ä¢ Validation Loss: 0.4205
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4205, best: 0.4184)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2063
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4176
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1073
  ‚Ä¢ Validation Loss: 0.4186
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4186, best: 0.4176)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1977
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4176)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1862
  ‚Ä¢ Validation Loss: 0.4191
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4191, best: 0.4176)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1820
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4176, best: 0.4176)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1496
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4206, best: 0.4176)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1704
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4195, best: 0.4176)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1922
  ‚Ä¢ Validation Loss: 0.4186
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4186, best: 0.4176)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1616
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4190, best: 0.4176)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1785
  ‚Ä¢ Validation Loss: 0.4200
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4200, best: 0.4176)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4176
Total Epochs:   300
Models Saved:   ./Result/a3/Syr341
TensorBoard:    ./Result/a3/Syr341/tensorboard_logs
================================================================================

[04:53:49] Training completed. Best val loss: 0.4176

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9668, Recall=0.9799, F1=0.9733, IoU=0.9480
Paratext            : Precision=0.5063, Recall=0.3224, F1=0.3939, IoU=0.2453
Decoration          : Precision=0.9391, Recall=0.6784, F1=0.7877, IoU=0.6498
Main Text           : Precision=0.8489, Recall=0.8236, F1=0.8360, IoU=0.7183
Title               : Precision=0.3068, Recall=0.1339, F1=0.1865, IoU=0.1028

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7136
Mean Recall:    0.5876
Mean F1-Score:  0.6355
Mean IoU:       0.5328
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8135
Mean Recall:    0.7257
Mean F1-Score:  0.7571
Mean IoU:       0.6511
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL + GCFF (Global Context Feature Fusion)
Results Location: ./Result/a3/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Wed Nov 19 04:59:28 AM CET 2025
= Job-ID             : 1394275 on tinygpu
= Job-Name           : 3rd
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run3.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 03:55:25
= Total RAM usage    : 4.6 GiB of requested  GiB (%)   
= Node list          : tg082
= Subm/Elig/Start/End: 2025-11-19T00:40:17 / 2025-11-19T00:40:17 / 2025-11-19T01:04:03 / 2025-11-19T04:59:28
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody             0.0K  1000.0G  1500.0G        N/A       1    5,000K   7,500K        N/A    
    /home/hpc              63.6G   104.9G   209.7G        N/A     238K     500K   1,000K        N/A    
    /home/vault             0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3028171, 63 %, 45 %, 8290 MiB, 2954243 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3170973, 13 %, 6 %, 892 MiB, 197046 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3191550, 61 %, 44 %, 8290 MiB, 3014466 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3385619, 13 %, 6 %, 892 MiB, 203203 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3403936, 60 %, 43 %, 8290 MiB, 3144996 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3594386, 14 %, 6 %, 892 MiB, 187503 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3612176, 59 %, 42 %, 8290 MiB, 3122884 ms
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 3802197, 13 %, 6 %, 892 MiB, 203082 ms
