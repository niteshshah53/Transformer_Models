### Starting TaskPrologue of job 1391401 on tg065 at Sun Nov 16 11:45:10 PM CET 2025
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Sun Nov 16 23:45:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:AF:00.0 Off |                  N/A |
| 27%   28C    P8              8W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple (concatenation)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Deep Supervision: enabled (auxiliary outputs for better gradient flow)
  ‚úó Multi-Scale Aggregation: disabled (base model)
  ‚úó Fourier Feature Fusion: disabled (using simple fusion)
  ‚úó Smart Skip Connections: disabled (using simple fusion)
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
  - Loss: CE (weighted) + Focal (Œ≥=2.0) + Dice
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a3/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=9,601,714
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8906
  ‚Ä¢ Validation Loss: 0.7641
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7641
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7224
  ‚Ä¢ Validation Loss: 0.6826
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6826
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6817
  ‚Ä¢ Validation Loss: 0.6741
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6731
  ‚Ä¢ Validation Loss: 0.6576
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6576
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6538
  ‚Ä¢ Validation Loss: 0.6568
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6568
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6449
  ‚Ä¢ Validation Loss: 0.6398
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6398
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6200
  ‚Ä¢ Validation Loss: 0.6142
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6142
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5893
  ‚Ä¢ Validation Loss: 0.6055
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6055
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5769
  ‚Ä¢ Validation Loss: 0.5882
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5882
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5485
  ‚Ä¢ Validation Loss: 0.5776
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5776
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5287
  ‚Ä¢ Validation Loss: 0.5756
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5756
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5227
  ‚Ä¢ Validation Loss: 0.5702
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5702
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5151
  ‚Ä¢ Validation Loss: 0.5680
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5680
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5054
  ‚Ä¢ Validation Loss: 0.5570
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4948
  ‚Ä¢ Validation Loss: 0.5532
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5532
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4999
  ‚Ä¢ Validation Loss: 0.5531
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5531
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4876
  ‚Ä¢ Validation Loss: 0.5520
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5520
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4779
  ‚Ä¢ Validation Loss: 0.5466
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5466
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4774
  ‚Ä¢ Validation Loss: 0.5440
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5440
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4616
  ‚Ä¢ Validation Loss: 0.5410
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5410
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4633
  ‚Ä¢ Validation Loss: 0.5323
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5323
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4708
  ‚Ä¢ Validation Loss: 0.5333
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5333, best: 0.5323)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4559
  ‚Ä¢ Validation Loss: 0.5330
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5330, best: 0.5323)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4619
  ‚Ä¢ Validation Loss: 0.5309
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5309
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4513
  ‚Ä¢ Validation Loss: 0.5357
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5357, best: 0.5309)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4490
  ‚Ä¢ Validation Loss: 0.5294
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5294
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4496
  ‚Ä¢ Validation Loss: 0.5265
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5265
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4448
  ‚Ä¢ Validation Loss: 0.5248
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5248
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4428
  ‚Ä¢ Validation Loss: 0.5226
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5226
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4424
  ‚Ä¢ Validation Loss: 0.5253
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5253, best: 0.5226)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4413
  ‚Ä¢ Validation Loss: 0.5224
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5224
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4465
  ‚Ä¢ Validation Loss: 0.5200
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5200
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4463
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5199
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4348
  ‚Ä¢ Validation Loss: 0.5196
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5196
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4278
  ‚Ä¢ Validation Loss: 0.5173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5173
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4369
  ‚Ä¢ Validation Loss: 0.5181
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5181, best: 0.5173)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4318
  ‚Ä¢ Validation Loss: 0.5190
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5190, best: 0.5173)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4311
  ‚Ä¢ Validation Loss: 0.5160
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5160
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4379
  ‚Ä¢ Validation Loss: 0.5171
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5171, best: 0.5160)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4233
  ‚Ä¢ Validation Loss: 0.5160
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5160, best: 0.5160)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4221
  ‚Ä¢ Validation Loss: 0.5175
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5175, best: 0.5160)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4282
  ‚Ä¢ Validation Loss: 0.5169
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5169, best: 0.5160)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4320
  ‚Ä¢ Validation Loss: 0.5141
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5141
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4233
  ‚Ä¢ Validation Loss: 0.5137
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5137
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4232
  ‚Ä¢ Validation Loss: 0.5143
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5143, best: 0.5137)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4278
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5142, best: 0.5137)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4199
  ‚Ä¢ Validation Loss: 0.5154
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5154, best: 0.5137)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4159
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5142, best: 0.5137)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4116
  ‚Ä¢ Validation Loss: 0.5150
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5150, best: 0.5137)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4145
  ‚Ä¢ Validation Loss: 0.5137
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5137, best: 0.5137)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4298
  ‚Ä¢ Validation Loss: 0.5173
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5173, best: 0.5137)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4322
  ‚Ä¢ Validation Loss: 0.5222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5222, best: 0.5137)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4349
  ‚Ä¢ Validation Loss: 0.5173
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5173, best: 0.5137)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4331
  ‚Ä¢ Validation Loss: 0.5256
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5256, best: 0.5137)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4118
  ‚Ä¢ Validation Loss: 0.5162
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5162, best: 0.5137)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4269
  ‚Ä¢ Validation Loss: 0.5191
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5191, best: 0.5137)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4298
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5161, best: 0.5137)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4038
  ‚Ä¢ Validation Loss: 0.5093
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5093
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3858
  ‚Ä¢ Validation Loss: 0.5215
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5215, best: 0.5093)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4184
  ‚Ä¢ Validation Loss: 0.5075
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5075
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4201
  ‚Ä¢ Validation Loss: 0.5082
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5082, best: 0.5075)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4046
  ‚Ä¢ Validation Loss: 0.5113
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5113, best: 0.5075)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4144
  ‚Ä¢ Validation Loss: 0.5149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5149, best: 0.5075)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4122
  ‚Ä¢ Validation Loss: 0.5124
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5124, best: 0.5075)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4082
  ‚Ä¢ Validation Loss: 0.5057
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5057
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4051
  ‚Ä¢ Validation Loss: 0.5149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5149, best: 0.5057)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3890
  ‚Ä¢ Validation Loss: 0.5029
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5029
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4069
  ‚Ä¢ Validation Loss: 0.5019
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5019
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3887
  ‚Ä¢ Validation Loss: 0.5091
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5091, best: 0.5019)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3732
  ‚Ä¢ Validation Loss: 0.5051
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5051, best: 0.5019)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3735
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4981
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3857
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5058, best: 0.4981)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3937
  ‚Ä¢ Validation Loss: 0.4985
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4985, best: 0.4981)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3907
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4960
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3443
  ‚Ä¢ Validation Loss: 0.4982
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4982, best: 0.4960)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3655
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4957
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3710
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4965, best: 0.4957)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3351
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4976, best: 0.4957)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3637
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3826
  ‚Ä¢ Validation Loss: 0.4970
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4970, best: 0.4942)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3707
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4922
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3693
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4895
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3509
  ‚Ä¢ Validation Loss: 0.4888
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4888
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3778
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4880
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3336
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4915, best: 0.4880)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3253
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4954, best: 0.4880)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3424
  ‚Ä¢ Validation Loss: 0.4874
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4874
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3599
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4850
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3663
  ‚Ä¢ Validation Loss: 0.4900
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4900, best: 0.4850)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3455
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4878, best: 0.4850)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2939
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4902, best: 0.4850)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4849
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4849
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2161
  ‚Ä¢ Validation Loss: 0.4844
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4844
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1910
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4878, best: 0.4844)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2336
  ‚Ä¢ Validation Loss: 0.4859
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4859, best: 0.4844)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2058
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4807
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1792
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4804
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1830
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4881, best: 0.4804)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2588
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4829, best: 0.4804)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2238
  ‚Ä¢ Validation Loss: 0.4851
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4851, best: 0.4804)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2639
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4894, best: 0.4804)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2336
  ‚Ä¢ Validation Loss: 0.4898
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4898, best: 0.4804)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2929
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4804, best: 0.4804)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2737
  ‚Ä¢ Validation Loss: 0.4791
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4791
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2345
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4827, best: 0.4791)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2720
  ‚Ä¢ Validation Loss: 0.4810
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4810, best: 0.4791)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2404
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4836, best: 0.4791)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2435
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4826, best: 0.4791)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2272
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4802, best: 0.4791)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2354
  ‚Ä¢ Validation Loss: 0.4796
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4796, best: 0.4791)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2694
  ‚Ä¢ Validation Loss: 0.4816
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4816, best: 0.4791)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2113
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4790
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2376
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4782
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2721
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4801, best: 0.4782)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2164
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4805, best: 0.4782)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2427
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4804, best: 0.4782)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2320
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4760
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1899
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4808, best: 0.4760)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2190
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4823, best: 0.4760)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2457
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4812, best: 0.4760)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2067
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4779, best: 0.4760)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2116
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4766, best: 0.4760)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2006
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4785, best: 0.4760)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2133
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4755
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2254
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4769, best: 0.4755)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2135
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4797, best: 0.4755)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2873
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4753
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2345
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4750
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2101
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4763, best: 0.4750)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2508
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4749
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2513
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4763, best: 0.4749)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2534
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4767, best: 0.4749)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2045
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4754, best: 0.4749)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1914
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4745
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4759, best: 0.4745)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2928
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4738
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2902
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4745, best: 0.4738)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2417
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4745, best: 0.4738)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2265
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4772, best: 0.4738)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2152
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4758, best: 0.4738)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2130
  ‚Ä¢ Validation Loss: 0.4762
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4762, best: 0.4738)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2662
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4743, best: 0.4738)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2161
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4771, best: 0.4738)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2532
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4739, best: 0.4738)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2284
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4760, best: 0.4738)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2107
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4751, best: 0.4738)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2304
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4755, best: 0.4738)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2449
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4763, best: 0.4738)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2533
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4754, best: 0.4738)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2262
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4754, best: 0.4738)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2200
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4860, best: 0.4738)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2299
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4846, best: 0.4738)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1989
  ‚Ä¢ Validation Loss: 0.4864
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4864, best: 0.4738)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2015
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4855, best: 0.4738)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2358
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4785, best: 0.4738)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1946
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4807, best: 0.4738)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1529
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4795, best: 0.4738)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0322
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4868, best: 0.4738)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0382
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4854, best: 0.4738)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0869
  ‚Ä¢ Validation Loss: 0.4857
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4857, best: 0.4738)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0549
  ‚Ä¢ Validation Loss: 0.4819
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4819, best: 0.4738)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0398
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4846, best: 0.4738)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0671
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4809, best: 0.4738)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0610
  ‚Ä¢ Validation Loss: 0.4810
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4810, best: 0.4738)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0555
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4837, best: 0.4738)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1112
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4787, best: 0.4738)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0732
  ‚Ä¢ Validation Loss: 0.4792
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4792, best: 0.4738)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1294
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4764, best: 0.4738)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0624
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4755, best: 0.4738)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0476
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4771, best: 0.4738)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0988
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4758, best: 0.4738)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0880
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4869, best: 0.4738)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0528
  ‚Ä¢ Validation Loss: 0.4842
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4842, best: 0.4738)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0696
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4767, best: 0.4738)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0784
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4769, best: 0.4738)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0861
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4787, best: 0.4738)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0589
  ‚Ä¢ Validation Loss: 0.4822
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4822, best: 0.4738)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0819
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4728
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0984
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4748, best: 0.4728)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0740
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4737, best: 0.4728)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0583
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4846, best: 0.4728)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0369
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4833, best: 0.4728)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0632
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4780, best: 0.4728)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0443
  ‚Ä¢ Validation Loss: 0.4841
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4841, best: 0.4728)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0588
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4763, best: 0.4728)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0856
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4741, best: 0.4728)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0678
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4780, best: 0.4728)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0737
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4767, best: 0.4728)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0440
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4836, best: 0.4728)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0340
  ‚Ä¢ Validation Loss: 0.4830
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4830, best: 0.4728)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4733, best: 0.4728)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0609
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4782, best: 0.4728)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1299
  ‚Ä¢ Validation Loss: 0.4828
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4828, best: 0.4728)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0780
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4751, best: 0.4728)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0791
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4722
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0501
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4759, best: 0.4722)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0752
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4764, best: 0.4722)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0723
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4736, best: 0.4722)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0978
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4802, best: 0.4722)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0758
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4763, best: 0.4722)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0595
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4767, best: 0.4722)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0763
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4775, best: 0.4722)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4755, best: 0.4722)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0643
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4737, best: 0.4722)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0676
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4726, best: 0.4722)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1148
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4746, best: 0.4722)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0706
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4754, best: 0.4722)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0443
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4745, best: 0.4722)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1037
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4784, best: 0.4722)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0211
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4846, best: 0.4722)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0325
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4727, best: 0.4722)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0944
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4717
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4714
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0694
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4761, best: 0.4714)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0303
  ‚Ä¢ Validation Loss: 0.4792
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4792, best: 0.4714)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4750, best: 0.4714)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0553
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4761, best: 0.4714)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0380
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4744, best: 0.4714)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4812, best: 0.4714)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0812
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4702
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0571
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4710, best: 0.4702)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0708
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4713, best: 0.4702)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1093
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4714, best: 0.4702)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0754
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4685
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4754, best: 0.4685)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0327
  ‚Ä¢ Validation Loss: 0.4774
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4774, best: 0.4685)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0591
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4699, best: 0.4685)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0523
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4795, best: 0.4685)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0636
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4737, best: 0.4685)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0434
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4761, best: 0.4685)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0460
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4758, best: 0.4685)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0622
  ‚Ä¢ Validation Loss: 0.4776
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4776, best: 0.4685)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1030
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4724, best: 0.4685)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4826, best: 0.4685)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0345
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4784, best: 0.4685)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0920
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4771, best: 0.4685)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0594
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4756, best: 0.4685)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0575
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4717, best: 0.4685)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1148
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4714, best: 0.4685)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0933
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4688, best: 0.4685)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0788
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4726, best: 0.4685)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0475
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4730, best: 0.4685)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0924
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4681
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0514
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4681)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0205
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4669
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1004
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4693, best: 0.4669)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1007
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4723, best: 0.4669)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0893
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4687, best: 0.4669)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0858
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4693, best: 0.4669)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0636
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4698, best: 0.4669)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4693, best: 0.4669)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1113
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4724, best: 0.4669)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0575
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4752, best: 0.4669)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0656
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4711, best: 0.4669)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0655
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4708, best: 0.4669)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0648
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4749, best: 0.4669)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0611
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4747, best: 0.4669)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0938
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4705, best: 0.4669)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1135
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4698, best: 0.4669)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0757
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4682, best: 0.4669)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0865
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4700, best: 0.4669)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4683, best: 0.4669)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1012
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4686, best: 0.4669)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0652
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4701, best: 0.4669)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0870
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4688, best: 0.4669)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0765
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4690, best: 0.4669)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0662
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4685, best: 0.4669)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0943
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4673, best: 0.4669)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0856
  ‚Ä¢ Validation Loss: 0.4664
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4664
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1004
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4735, best: 0.4664)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0654
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4715, best: 0.4664)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0671
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4675, best: 0.4664)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4693, best: 0.4664)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0902
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4675, best: 0.4664)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1132
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4689, best: 0.4664)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1298
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4674, best: 0.4664)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0704
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4687, best: 0.4664)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1719
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4674, best: 0.4664)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0702
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4670, best: 0.4664)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0692
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4692, best: 0.4664)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1057
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4673, best: 0.4664)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0590
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4683, best: 0.4664)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0927
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4685, best: 0.4664)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0925
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4673, best: 0.4664)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0857
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4668, best: 0.4664)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0870
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4673, best: 0.4664)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1215
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4665, best: 0.4664)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0812
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4659
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0865
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4660, best: 0.4659)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1043
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4659)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1076
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4655
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1207
  ‚Ä¢ Validation Loss: 0.4644
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4644
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0926
  ‚Ä¢ Validation Loss: 0.4664
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4664, best: 0.4644)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4655, best: 0.4644)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1088
  ‚Ä¢ Validation Loss: 0.4666
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4666, best: 0.4644)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1308
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4692, best: 0.4644)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4700, best: 0.4644)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1456
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4663, best: 0.4644)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1316
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4659, best: 0.4644)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1181
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4683, best: 0.4644)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4644
Total Epochs:   300
Models Saved:   ./Result/a3/Latin2
TensorBoard:    ./Result/a3/Latin2/tensorboard_logs
================================================================================

[01:00:58] Training completed. Best val loss: 0.4644

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9880, Recall=0.9921, F1=0.9901, IoU=0.9803
Paratext            : Precision=0.6911, Recall=0.6979, F1=0.6945, IoU=0.5320
Decoration          : Precision=0.8750, Recall=0.9009, F1=0.8877, IoU=0.7981
Main Text           : Precision=0.8204, Recall=0.8096, F1=0.8149, IoU=0.6877
Title               : Precision=0.8002, Recall=0.8382, F1=0.8187, IoU=0.6931
Chapter Headings    : Precision=0.5908, Recall=0.2167, F1=0.3171, IoU=0.1884

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7943
Mean Recall:    0.7425
Mean F1-Score:  0.7538
Mean IoU:       0.6466
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a3/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=9,601,714
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.9834
  ‚Ä¢ Validation Loss: 0.7776
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7776
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7652
  ‚Ä¢ Validation Loss: 0.6669
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6669
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7414
  ‚Ä¢ Validation Loss: 0.6522
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6522
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6942
  ‚Ä¢ Validation Loss: 0.6362
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6362
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6701
  ‚Ä¢ Validation Loss: 0.6247
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6247
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6600
  ‚Ä¢ Validation Loss: 0.6123
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6123
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6303
  ‚Ä¢ Validation Loss: 0.6061
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6061
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6102
  ‚Ä¢ Validation Loss: 0.5899
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5899
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6008
  ‚Ä¢ Validation Loss: 0.5826
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5826
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5845
  ‚Ä¢ Validation Loss: 0.5742
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5742
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5665
  ‚Ä¢ Validation Loss: 0.5779
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5779, best: 0.5742)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5620
  ‚Ä¢ Validation Loss: 0.5685
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5685
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5551
  ‚Ä¢ Validation Loss: 0.5658
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5658
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5370
  ‚Ä¢ Validation Loss: 0.5558
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5237
  ‚Ä¢ Validation Loss: 0.5524
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5524
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5360
  ‚Ä¢ Validation Loss: 0.5591
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5591, best: 0.5524)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5251
  ‚Ä¢ Validation Loss: 0.5515
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5515
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4945
  ‚Ä¢ Validation Loss: 0.5436
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5436
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5143
  ‚Ä¢ Validation Loss: 0.5483
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5483, best: 0.5436)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5071
  ‚Ä¢ Validation Loss: 0.5387
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5387
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4839
  ‚Ä¢ Validation Loss: 0.5359
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5359
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4898
  ‚Ä¢ Validation Loss: 0.5366
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5366, best: 0.5359)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4771
  ‚Ä¢ Validation Loss: 0.5315
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5315
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4827
  ‚Ä¢ Validation Loss: 0.5321
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5321, best: 0.5315)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4606
  ‚Ä¢ Validation Loss: 0.5343
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5343, best: 0.5315)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4701
  ‚Ä¢ Validation Loss: 0.5275
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5275
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4648
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5283, best: 0.5275)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4538
  ‚Ä¢ Validation Loss: 0.5313
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5313, best: 0.5275)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4629
  ‚Ä¢ Validation Loss: 0.5291
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5291, best: 0.5275)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4522
  ‚Ä¢ Validation Loss: 0.5284
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5284, best: 0.5275)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4594
  ‚Ä¢ Validation Loss: 0.5247
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5247
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4581
  ‚Ä¢ Validation Loss: 0.5254
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5254, best: 0.5247)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4605
  ‚Ä¢ Validation Loss: 0.5240
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5240
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4459
  ‚Ä¢ Validation Loss: 0.5285
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5285, best: 0.5240)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4421
  ‚Ä¢ Validation Loss: 0.5227
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5227
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4390
  ‚Ä¢ Validation Loss: 0.5226
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5226
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4385
  ‚Ä¢ Validation Loss: 0.5193
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5193
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4230
  ‚Ä¢ Validation Loss: 0.5187
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5187
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4432
  ‚Ä¢ Validation Loss: 0.5187
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5187
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4457
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5199, best: 0.5187)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4271
  ‚Ä¢ Validation Loss: 0.5171
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5171
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4222
  ‚Ä¢ Validation Loss: 0.5189
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5189, best: 0.5171)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4258
  ‚Ä¢ Validation Loss: 0.5199
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5199, best: 0.5171)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4321
  ‚Ä¢ Validation Loss: 0.5179
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5179, best: 0.5171)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4418
  ‚Ä¢ Validation Loss: 0.5170
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5170
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4349
  ‚Ä¢ Validation Loss: 0.5158
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5158
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3928
  ‚Ä¢ Validation Loss: 0.5178
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5178, best: 0.5158)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4429
  ‚Ä¢ Validation Loss: 0.5172
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5172, best: 0.5158)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4362
  ‚Ä¢ Validation Loss: 0.5163
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5163, best: 0.5158)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4158
  ‚Ä¢ Validation Loss: 0.5175
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5175, best: 0.5158)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4314
  ‚Ä¢ Validation Loss: 0.5244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5244, best: 0.5158)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4377
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5182, best: 0.5158)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4393
  ‚Ä¢ Validation Loss: 0.5233
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5233, best: 0.5158)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4279
  ‚Ä¢ Validation Loss: 0.5256
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5256, best: 0.5158)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4099
  ‚Ä¢ Validation Loss: 0.5227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5227, best: 0.5158)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3930
  ‚Ä¢ Validation Loss: 0.5225
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5225, best: 0.5158)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4417
  ‚Ä¢ Validation Loss: 0.5137
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5137
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4411
  ‚Ä¢ Validation Loss: 0.5179
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5179, best: 0.5137)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4049
  ‚Ä¢ Validation Loss: 0.5091
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5091
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4188
  ‚Ä¢ Validation Loss: 0.5132
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5132, best: 0.5091)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4051
  ‚Ä¢ Validation Loss: 0.5122
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5122, best: 0.5091)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3711
  ‚Ä¢ Validation Loss: 0.5105
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5105, best: 0.5091)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3959
  ‚Ä¢ Validation Loss: 0.5016
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5016
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3849
  ‚Ä¢ Validation Loss: 0.5108
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5108, best: 0.5016)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3667
  ‚Ä¢ Validation Loss: 0.5121
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5121, best: 0.5016)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3971
  ‚Ä¢ Validation Loss: 0.5225
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5225, best: 0.5016)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4002
  ‚Ä¢ Validation Loss: 0.5086
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5086, best: 0.5016)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4132
  ‚Ä¢ Validation Loss: 0.5039
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5039, best: 0.5016)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3816
  ‚Ä¢ Validation Loss: 0.5085
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5085, best: 0.5016)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4031
  ‚Ä¢ Validation Loss: 0.5070
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5070, best: 0.5016)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3739
  ‚Ä¢ Validation Loss: 0.5012
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5012
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3990
  ‚Ä¢ Validation Loss: 0.5062
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5062, best: 0.5012)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4224
  ‚Ä¢ Validation Loss: 0.4994
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4994
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3929
  ‚Ä¢ Validation Loss: 0.5044
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5044, best: 0.4994)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3429
  ‚Ä¢ Validation Loss: 0.5041
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5041, best: 0.4994)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4015
  ‚Ä¢ Validation Loss: 0.4983
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4983
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3946
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5034, best: 0.4983)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3761
  ‚Ä¢ Validation Loss: 0.4994
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4994, best: 0.4983)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4952
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3748
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4946
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3967
  ‚Ä¢ Validation Loss: 0.4971
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4971, best: 0.4946)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3763
  ‚Ä¢ Validation Loss: 0.4994
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4994, best: 0.4946)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3851
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4927
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3224
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4996, best: 0.4927)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3639
  ‚Ä¢ Validation Loss: 0.4982
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4982, best: 0.4927)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3710
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4975, best: 0.4927)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3588
  ‚Ä¢ Validation Loss: 0.4905
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4905
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3624
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4924, best: 0.4905)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3643
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4925, best: 0.4905)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3424
  ‚Ä¢ Validation Loss: 0.4923
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4923, best: 0.4905)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3404
  ‚Ä¢ Validation Loss: 0.4941
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4941, best: 0.4905)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2694
  ‚Ä¢ Validation Loss: 0.4919
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4919, best: 0.4905)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2809
  ‚Ä¢ Validation Loss: 0.4869
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4869
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2786
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4850
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2456
  ‚Ä¢ Validation Loss: 0.4871
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4871, best: 0.4850)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2119
  ‚Ä¢ Validation Loss: 0.4853
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4853, best: 0.4850)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2234
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4884, best: 0.4850)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2560
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4850
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2841
  ‚Ä¢ Validation Loss: 0.4864
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4864, best: 0.4850)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2806
  ‚Ä¢ Validation Loss: 0.4879
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4879, best: 0.4850)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2352
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4827
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2794
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4880, best: 0.4827)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2603
  ‚Ä¢ Validation Loss: 0.4879
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4879, best: 0.4827)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2249
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4829, best: 0.4827)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2680
  ‚Ä¢ Validation Loss: 0.4857
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4857, best: 0.4827)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2298
  ‚Ä¢ Validation Loss: 0.4841
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4841, best: 0.4827)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2957
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4834, best: 0.4827)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2684
  ‚Ä¢ Validation Loss: 0.4879
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4879, best: 0.4827)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2733
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4836, best: 0.4827)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2779
  ‚Ä¢ Validation Loss: 0.4863
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4863, best: 0.4827)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2288
  ‚Ä¢ Validation Loss: 0.4864
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4864, best: 0.4827)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2716
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4860, best: 0.4827)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2328
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4837, best: 0.4827)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2305
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4826
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2670
  ‚Ä¢ Validation Loss: 0.4853
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4853, best: 0.4826)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2766
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4810
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4810
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2705
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4803
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2668
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4878, best: 0.4803)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2746
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4797
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2659
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4823, best: 0.4797)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2740
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4809, best: 0.4797)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2984
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4803, best: 0.4797)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2323
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4815, best: 0.4797)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2559
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4815, best: 0.4797)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2263
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4813, best: 0.4797)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2395
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4805, best: 0.4797)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4784
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2804
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4778
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2869
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4785, best: 0.4778)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2834
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4785, best: 0.4778)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2777
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4790, best: 0.4778)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2603
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4775
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1932
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4771
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2348
  ‚Ä¢ Validation Loss: 0.4770
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4770
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2678
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4799, best: 0.4770)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2997
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4775, best: 0.4770)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2525
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4784, best: 0.4770)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4781
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4781, best: 0.4770)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2369
  ‚Ä¢ Validation Loss: 0.4776
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4776, best: 0.4770)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2668
  ‚Ä¢ Validation Loss: 0.4774
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4774, best: 0.4770)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3073
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4773, best: 0.4770)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2891
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4778, best: 0.4770)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2466
  ‚Ä¢ Validation Loss: 0.4767
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4767
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2816
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4772, best: 0.4767)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2521
  ‚Ä¢ Validation Loss: 0.4783
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4783, best: 0.4767)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2885
  ‚Ä¢ Validation Loss: 0.4781
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4781, best: 0.4767)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2182
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4772, best: 0.4767)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2852
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4773, best: 0.4767)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2524
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4787, best: 0.4767)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2784
  ‚Ä¢ Validation Loss: 0.4862
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4862, best: 0.4767)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1841
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4856, best: 0.4767)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1106
  ‚Ä¢ Validation Loss: 0.4943
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4943, best: 0.4767)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0995
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4767)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0420
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5001, best: 0.4767)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0805
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4876, best: 0.4767)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0609
  ‚Ä¢ Validation Loss: 0.4912
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4912, best: 0.4767)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0732
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4909, best: 0.4767)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0341
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4855, best: 0.4767)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4922, best: 0.4767)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4921, best: 0.4767)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0159
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4889, best: 0.4767)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0988
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4876, best: 0.4767)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4808, best: 0.4767)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0787
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4878, best: 0.4767)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1053
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4803, best: 0.4767)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4871
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4871, best: 0.4767)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1232
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4852, best: 0.4767)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0762
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4809, best: 0.4767)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1426
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4773, best: 0.4767)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0888
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4790, best: 0.4767)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1096
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4832, best: 0.4767)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0840
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4775, best: 0.4767)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0963
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4802, best: 0.4767)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0562
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4834, best: 0.4767)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0881
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4772, best: 0.4767)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4820, best: 0.4767)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0868
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4808, best: 0.4767)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.4841
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4841, best: 0.4767)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0876
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4813, best: 0.4767)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1078
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4824, best: 0.4767)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1005
  ‚Ä¢ Validation Loss: 0.4796
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4796, best: 0.4767)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0957
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4741
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0482
  ‚Ä¢ Validation Loss: 0.4817
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4817, best: 0.4741)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0913
  ‚Ä¢ Validation Loss: 0.4781
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4781, best: 0.4741)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1080
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4910, best: 0.4741)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0514
  ‚Ä¢ Validation Loss: 0.4821
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4821, best: 0.4741)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4812, best: 0.4741)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4824, best: 0.4741)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0500
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4880, best: 0.4741)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0760
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4868, best: 0.4741)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0700
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4827, best: 0.4741)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1190
  ‚Ä¢ Validation Loss: 0.4818
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4818, best: 0.4741)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0951
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4827, best: 0.4741)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0893
  ‚Ä¢ Validation Loss: 0.4806
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4806, best: 0.4741)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0933
  ‚Ä¢ Validation Loss: 0.4859
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4859, best: 0.4741)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0506
  ‚Ä¢ Validation Loss: 0.4794
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4794, best: 0.4741)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0718
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4811, best: 0.4741)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0923
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4833, best: 0.4741)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0317
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4808, best: 0.4741)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1001
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4754, best: 0.4741)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0949
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4834, best: 0.4741)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1012
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4823, best: 0.4741)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0216
  ‚Ä¢ Validation Loss: 0.4871
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4871, best: 0.4741)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4857
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4857, best: 0.4741)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0111
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4852, best: 0.4741)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0769
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4777, best: 0.4741)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1097
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4823, best: 0.4741)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1078
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4777, best: 0.4741)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1227
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4765, best: 0.4741)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4753, best: 0.4741)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4753, best: 0.4741)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0545
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4780, best: 0.4741)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1589
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4799, best: 0.4741)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1122
  ‚Ä¢ Validation Loss: 0.4798
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4798, best: 0.4741)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0883
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4795, best: 0.4741)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1304
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4751, best: 0.4741)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0980
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4733
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1267
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4780, best: 0.4733)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0614
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1193
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4733, best: 0.4718)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0977
  ‚Ä¢ Validation Loss: 0.4781
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4781, best: 0.4718)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1195
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4731, best: 0.4718)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0975
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4850, best: 0.4718)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4747, best: 0.4718)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0805
  ‚Ä¢ Validation Loss: 0.4786
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4786, best: 0.4718)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1055
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4772, best: 0.4718)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1069
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4743, best: 0.4718)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1546
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4765, best: 0.4718)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0790
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4799, best: 0.4718)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0688
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4758, best: 0.4718)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0870
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4763, best: 0.4718)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1030
  ‚Ä¢ Validation Loss: 0.4756
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4756, best: 0.4718)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1073
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4719, best: 0.4718)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0844
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4761, best: 0.4718)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1382
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4724, best: 0.4718)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1114
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4742, best: 0.4718)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0717
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4705
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4724, best: 0.4705)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4778, best: 0.4705)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0581
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4784, best: 0.4705)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0367
  ‚Ä¢ Validation Loss: 0.4796
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4796, best: 0.4705)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0549
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4726, best: 0.4705)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1333
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4702
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1416
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4719, best: 0.4702)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1166
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4708, best: 0.4702)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0957
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4729, best: 0.4702)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1081
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4700
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1286
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4749, best: 0.4700)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0854
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4765, best: 0.4700)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1004
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4699
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0783
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4735, best: 0.4699)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1186
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4745, best: 0.4699)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0984
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0795
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4745, best: 0.4676)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1044
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4705, best: 0.4676)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0879
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4683, best: 0.4676)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4683, best: 0.4676)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0474
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4726, best: 0.4676)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1366
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4736, best: 0.4676)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0767
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4676)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0965
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4685, best: 0.4676)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1338
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4766, best: 0.4676)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4784, best: 0.4676)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1115
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4758, best: 0.4676)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1049
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4687, best: 0.4676)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1127
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4722, best: 0.4676)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0958
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4728, best: 0.4676)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0876
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4687, best: 0.4676)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1051
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4683, best: 0.4676)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1147
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4731, best: 0.4676)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1176
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4728, best: 0.4676)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0779
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4701, best: 0.4676)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0783
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4685, best: 0.4676)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0969
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4708, best: 0.4676)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0828
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4735, best: 0.4676)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0848
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4681, best: 0.4676)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1149
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4698, best: 0.4676)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0824
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4693, best: 0.4676)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0613
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4698, best: 0.4676)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1490
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4676, best: 0.4676)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1089
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4676, best: 0.4676)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0561
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4691, best: 0.4676)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1219
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4691, best: 0.4676)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1127
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4717, best: 0.4676)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1053
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1341
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4679, best: 0.4654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0897
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4696, best: 0.4654)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1286
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4660, best: 0.4654)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0504
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4684, best: 0.4654)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1567
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4710, best: 0.4654)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1093
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4687, best: 0.4654)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0841
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4677, best: 0.4654)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1105
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4712, best: 0.4654)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1321
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4694, best: 0.4654)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1241
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4691, best: 0.4654)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1460
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4715, best: 0.4654)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1250
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4669, best: 0.4654)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1299
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4675, best: 0.4654)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4660, best: 0.4654)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4654
Total Epochs:   300
Models Saved:   ./Result/a3/Latin14396
TensorBoard:    ./Result/a3/Latin14396/tensorboard_logs
================================================================================

[02:18:40] Training completed. Best val loss: 0.4654

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9870, Recall=0.9910, F1=0.9890, IoU=0.9782
Paratext            : Precision=0.6766, Recall=0.5319, F1=0.5956, IoU=0.4241
Decoration          : Precision=0.9137, Recall=0.9408, F1=0.9270, IoU=0.8640
Main Text           : Precision=0.8958, Recall=0.8698, F1=0.8826, IoU=0.7899
Title               : Precision=0.8475, Recall=0.8043, F1=0.8253, IoU=0.7026
Chapter Headings    : Precision=0.8885, Recall=0.6429, F1=0.7460, IoU=0.5949

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8682
Mean Recall:    0.7968
Mean F1-Score:  0.8276
Mean IoU:       0.7256
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a3/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=9,601,714
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0227
  ‚Ä¢ Validation Loss: 0.7666
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7666
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7645
  ‚Ä¢ Validation Loss: 0.6676
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7368
  ‚Ä¢ Validation Loss: 0.6518
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6518
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7005
  ‚Ä¢ Validation Loss: 0.6253
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6253
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6710
  ‚Ä¢ Validation Loss: 0.6277
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6277, best: 0.6253)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6452
  ‚Ä¢ Validation Loss: 0.6049
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6049
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6081
  ‚Ä¢ Validation Loss: 0.5748
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5748
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5943
  ‚Ä¢ Validation Loss: 0.5645
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5645
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5725
  ‚Ä¢ Validation Loss: 0.5802
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5802, best: 0.5645)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5568
  ‚Ä¢ Validation Loss: 0.5550
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5550
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5502
  ‚Ä¢ Validation Loss: 0.5451
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5451
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5312
  ‚Ä¢ Validation Loss: 0.5326
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5326
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5278
  ‚Ä¢ Validation Loss: 0.5382
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5382, best: 0.5326)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5176
  ‚Ä¢ Validation Loss: 0.5244
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5244
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5035
  ‚Ä¢ Validation Loss: 0.5366
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5366, best: 0.5244)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5057
  ‚Ä¢ Validation Loss: 0.5198
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5198
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4825
  ‚Ä¢ Validation Loss: 0.5247
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5247, best: 0.5198)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4908
  ‚Ä¢ Validation Loss: 0.5181
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5181
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4645
  ‚Ä¢ Validation Loss: 0.5088
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5088
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4640
  ‚Ä¢ Validation Loss: 0.5117
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5117, best: 0.5088)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4610
  ‚Ä¢ Validation Loss: 0.5105
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5105, best: 0.5088)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4628
  ‚Ä¢ Validation Loss: 0.5077
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5077
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4358
  ‚Ä¢ Validation Loss: 0.5042
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5042
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4647
  ‚Ä¢ Validation Loss: 0.5082
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5082, best: 0.5042)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4510
  ‚Ä¢ Validation Loss: 0.5005
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5005
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4511
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4954
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4400
  ‚Ä¢ Validation Loss: 0.4941
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4941
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4344
  ‚Ä¢ Validation Loss: 0.4912
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4912
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4368
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4953, best: 0.4912)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4344
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4909
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4273
  ‚Ä¢ Validation Loss: 0.4882
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4882
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4252
  ‚Ä¢ Validation Loss: 0.4908
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4908, best: 0.4882)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4168
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4872
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4242
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4866
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4196
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4914, best: 0.4866)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4144
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4852
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4136
  ‚Ä¢ Validation Loss: 0.4853
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4853, best: 0.4852)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4147
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4839
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4115
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4839, best: 0.4839)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4140
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4834
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4169
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4815
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4025
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4823, best: 0.4815)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4108
  ‚Ä¢ Validation Loss: 0.4817
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4817, best: 0.4815)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4062
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4799
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3873
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4803, best: 0.4799)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3926
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4808, best: 0.4799)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3882
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4797
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4017
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4803, best: 0.4797)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3857
  ‚Ä¢ Validation Loss: 0.4798
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4798, best: 0.4797)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4033
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4805, best: 0.4797)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4079
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4887, best: 0.4797)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4151
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4952, best: 0.4797)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4032
  ‚Ä¢ Validation Loss: 0.4849
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4849, best: 0.4797)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3976
  ‚Ä¢ Validation Loss: 0.4813
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4813, best: 0.4797)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3481
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4907, best: 0.4797)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4029
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4775
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3530
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4775)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3759
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4809, best: 0.4775)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3629
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4812, best: 0.4775)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3880
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4790, best: 0.4775)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3918
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4748
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3638
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4793, best: 0.4748)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3661
  ‚Ä¢ Validation Loss: 0.4762
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4762, best: 0.4748)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3848
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4704
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3717
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4688
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3566
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3591
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4685, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3416
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4674
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3516
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4674)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3504
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4657
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3242
  ‚Ä¢ Validation Loss: 0.4632
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4632
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3359
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4681, best: 0.4632)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3085
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4743, best: 0.4632)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3294
  ‚Ä¢ Validation Loss: 0.4649
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4649, best: 0.4632)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3472
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4639, best: 0.4632)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3614
  ‚Ä¢ Validation Loss: 0.4633
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4633, best: 0.4632)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3425
  ‚Ä¢ Validation Loss: 0.4650
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4650, best: 0.4632)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3488
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4678, best: 0.4632)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3392
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4577
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3542
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4577)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3379
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4569
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3565
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4566
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3272
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4587, best: 0.4566)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3423
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4532
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3067
  ‚Ä¢ Validation Loss: 0.4558
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4558, best: 0.4532)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3170
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4532)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2903
  ‚Ä¢ Validation Loss: 0.4576
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4576, best: 0.4532)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3115
  ‚Ä¢ Validation Loss: 0.4660
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4660, best: 0.4532)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3324
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4512
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3093
  ‚Ä¢ Validation Loss: 0.4549
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4549, best: 0.4512)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3172
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4511
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3264
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2048
  ‚Ä¢ Validation Loss: 0.4523
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4523, best: 0.4507)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2209
  ‚Ä¢ Validation Loss: 0.4509
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4509, best: 0.4507)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2399
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4530, best: 0.4507)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2077
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4511, best: 0.4507)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2427
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4516, best: 0.4507)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2734
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2209
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4516, best: 0.4507)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2236
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.4491
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2607
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4512, best: 0.4491)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2115
  ‚Ä¢ Validation Loss: 0.4476
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4476
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4567
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4567, best: 0.4476)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1905
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4504, best: 0.4476)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2623
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4507, best: 0.4476)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2012
  ‚Ä¢ Validation Loss: 0.4488
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4488, best: 0.4476)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1909
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4490, best: 0.4476)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2088
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4494, best: 0.4476)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4467
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2551
  ‚Ä¢ Validation Loss: 0.4459
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4459
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2189
  ‚Ä¢ Validation Loss: 0.4495
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4495, best: 0.4459)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2528
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4461, best: 0.4459)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.4514
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4514, best: 0.4459)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1797
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4461, best: 0.4459)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2349
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4454
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2184
  ‚Ä¢ Validation Loss: 0.4468
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4468, best: 0.4454)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2363
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4478, best: 0.4454)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2337
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4455, best: 0.4454)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2212
  ‚Ä¢ Validation Loss: 0.4450
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4450
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2094
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4437
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2755
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4442, best: 0.4437)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1995
  ‚Ä¢ Validation Loss: 0.4451
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4451, best: 0.4437)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2340
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4436
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2677
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4447, best: 0.4436)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2290
  ‚Ä¢ Validation Loss: 0.4457
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4457, best: 0.4436)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1910
  ‚Ä¢ Validation Loss: 0.4441
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4441, best: 0.4436)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2370
  ‚Ä¢ Validation Loss: 0.4432
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4432
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2393
  ‚Ä¢ Validation Loss: 0.4449
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4449, best: 0.4432)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2386
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4425
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2488
  ‚Ä¢ Validation Loss: 0.4436
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4436, best: 0.4425)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2234
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4440, best: 0.4425)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2633
  ‚Ä¢ Validation Loss: 0.4432
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4432, best: 0.4425)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2147
  ‚Ä¢ Validation Loss: 0.4434
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4434, best: 0.4425)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2886
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4425, best: 0.4425)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1708
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4430, best: 0.4425)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2001
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4425, best: 0.4425)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2462
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4425, best: 0.4425)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2198
  ‚Ä¢ Validation Loss: 0.4434
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4434, best: 0.4425)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4427, best: 0.4425)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2300
  ‚Ä¢ Validation Loss: 0.4428
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4428, best: 0.4425)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2213
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4431, best: 0.4425)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2368
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4431, best: 0.4425)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2452
  ‚Ä¢ Validation Loss: 0.4429
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4429, best: 0.4425)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2310
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4427, best: 0.4425)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2543
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4425
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2235
  ‚Ä¢ Validation Loss: 0.4428
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4428, best: 0.4425)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2640
  ‚Ä¢ Validation Loss: 0.4425
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4425, best: 0.4425)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1959
  ‚Ä¢ Validation Loss: 0.4426
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4426, best: 0.4425)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2245
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4431, best: 0.4425)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2524
  ‚Ä¢ Validation Loss: 0.4428
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4428, best: 0.4425)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2337
  ‚Ä¢ Validation Loss: 0.4492
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4492, best: 0.4425)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1934
  ‚Ä¢ Validation Loss: 0.4505
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4505, best: 0.4425)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1966
  ‚Ä¢ Validation Loss: 0.4539
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4539, best: 0.4425)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2175
  ‚Ä¢ Validation Loss: 0.4555
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4555, best: 0.4425)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2019
  ‚Ä¢ Validation Loss: 0.4473
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4473, best: 0.4425)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2338
  ‚Ä¢ Validation Loss: 0.4439
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4439, best: 0.4425)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0402
  ‚Ä¢ Validation Loss: 0.4501
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4501, best: 0.4425)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0270
  ‚Ä¢ Validation Loss: 0.4500
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4500, best: 0.4425)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0637
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4504, best: 0.4425)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0943
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4481, best: 0.4425)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0468
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4517, best: 0.4425)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0809
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4464, best: 0.4425)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0825
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4478, best: 0.4425)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1217
  ‚Ä¢ Validation Loss: 0.4465
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4465, best: 0.4425)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0372
  ‚Ä¢ Validation Loss: 0.4509
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4509, best: 0.4425)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0229
  ‚Ä¢ Validation Loss: 0.4505
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4505, best: 0.4425)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1019
  ‚Ä¢ Validation Loss: 0.4536
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4536, best: 0.4425)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0504
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4490, best: 0.4425)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0687
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4462, best: 0.4425)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0846
  ‚Ä¢ Validation Loss: 0.4472
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4472, best: 0.4425)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0612
  ‚Ä¢ Validation Loss: 0.4485
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4485, best: 0.4425)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0653
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4597, best: 0.4425)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0330
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4508, best: 0.4425)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0962
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4530, best: 0.4425)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0436
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4458, best: 0.4425)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0554
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4455, best: 0.4425)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0816
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4430, best: 0.4425)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0999
  ‚Ä¢ Validation Loss: 0.4415
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4415
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0998
  ‚Ä¢ Validation Loss: 0.4424
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4424, best: 0.4415)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0928
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4480, best: 0.4415)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0398
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4480, best: 0.4415)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1095
  ‚Ä¢ Validation Loss: 0.4430
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4430, best: 0.4415)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0734
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4462, best: 0.4415)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0639
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4458, best: 0.4415)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0869
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4464, best: 0.4415)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0700
  ‚Ä¢ Validation Loss: 0.4456
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4456, best: 0.4415)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0713
  ‚Ä¢ Validation Loss: 0.4440
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4440, best: 0.4415)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0840
  ‚Ä¢ Validation Loss: 0.4511
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4511, best: 0.4415)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0237
  ‚Ä¢ Validation Loss: 0.4507
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4507, best: 0.4415)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4517, best: 0.4415)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0876
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4437, best: 0.4415)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0835
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4408
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0620
  ‚Ä¢ Validation Loss: 0.4412
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4412, best: 0.4408)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1232
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4422, best: 0.4408)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0903
  ‚Ä¢ Validation Loss: 0.4423
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4423, best: 0.4408)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0972
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4416, best: 0.4408)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0646
  ‚Ä¢ Validation Loss: 0.4403
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4403
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4481
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4481, best: 0.4403)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0145
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4516, best: 0.4403)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0150
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4490, best: 0.4403)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0181
  ‚Ä¢ Validation Loss: 0.4470
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4470, best: 0.4403)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0835
  ‚Ä¢ Validation Loss: 0.4416
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4416, best: 0.4403)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0745
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4455, best: 0.4403)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0789
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4447, best: 0.4403)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0302
  ‚Ä¢ Validation Loss: 0.4542
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4542, best: 0.4403)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0258
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4581, best: 0.4403)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0361
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4524, best: 0.4403)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0477
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4524, best: 0.4403)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0086
  ‚Ä¢ Validation Loss: 0.4476
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4476, best: 0.4403)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0426
  ‚Ä¢ Validation Loss: 0.4421
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4421, best: 0.4403)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1122
  ‚Ä¢ Validation Loss: 0.4431
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4431, best: 0.4403)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0894
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4437, best: 0.4403)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0554
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4447, best: 0.4403)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0968
  ‚Ä¢ Validation Loss: 0.4448
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4448, best: 0.4403)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0624
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4455, best: 0.4403)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1408
  ‚Ä¢ Validation Loss: 0.4419
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4419, best: 0.4403)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4408, best: 0.4403)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0220
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4504, best: 0.4403)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0142
  ‚Ä¢ Validation Loss: 0.4528
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4528, best: 0.4403)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0078
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4561, best: 0.4403)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0274
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4530, best: 0.4403)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0228
  ‚Ä¢ Validation Loss: 0.4526
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4526, best: 0.4403)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0082
  ‚Ä¢ Validation Loss: 0.4508
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4508, best: 0.4403)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0564
  ‚Ä¢ Validation Loss: 0.4442
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4442, best: 0.4403)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0751
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4512, best: 0.4403)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0501
  ‚Ä¢ Validation Loss: 0.4433
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4433, best: 0.4403)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0453
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4493, best: 0.4403)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0869
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4447, best: 0.4403)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1015
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4384
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0892
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4389, best: 0.4384)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.4393
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4393, best: 0.4384)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1133
  ‚Ä¢ Validation Loss: 0.4388
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4388, best: 0.4384)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0820
  ‚Ä¢ Validation Loss: 0.4384
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4384
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.4424
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4424, best: 0.4384)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0802
  ‚Ä¢ Validation Loss: 0.4423
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4423, best: 0.4384)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1047
  ‚Ä¢ Validation Loss: 0.4400
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4400, best: 0.4384)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0643
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0741
  ‚Ä¢ Validation Loss: 0.4391
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4391, best: 0.4373)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1280
  ‚Ä¢ Validation Loss: 0.4377
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4377, best: 0.4373)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1167
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4396, best: 0.4373)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0840
  ‚Ä¢ Validation Loss: 0.4407
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4407, best: 0.4373)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0716
  ‚Ä¢ Validation Loss: 0.4429
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4429, best: 0.4373)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0787
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4397, best: 0.4373)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0757
  ‚Ä¢ Validation Loss: 0.4448
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4448, best: 0.4373)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1008
  ‚Ä¢ Validation Loss: 0.4455
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4455, best: 0.4373)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0362
  ‚Ä¢ Validation Loss: 0.4532
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4532, best: 0.4373)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0443
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4527, best: 0.4373)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0324
  ‚Ä¢ Validation Loss: 0.4503
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4503, best: 0.4373)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0140
  ‚Ä¢ Validation Loss: 0.4504
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4504, best: 0.4373)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0210
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4462, best: 0.4373)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1203
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4422, best: 0.4373)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0873
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4385, best: 0.4373)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1216
  ‚Ä¢ Validation Loss: 0.4385
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4385, best: 0.4373)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0950
  ‚Ä¢ Validation Loss: 0.4380
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4380, best: 0.4373)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0810
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4361
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1032
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4369, best: 0.4361)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0660
  ‚Ä¢ Validation Loss: 0.4422
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4422, best: 0.4361)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1370
  ‚Ä¢ Validation Loss: 0.4365
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4365, best: 0.4361)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0914
  ‚Ä¢ Validation Loss: 0.4404
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4404, best: 0.4361)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0984
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4354
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1310
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4361, best: 0.4354)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1144
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4362, best: 0.4354)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1390
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4356, best: 0.4354)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1350
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4357, best: 0.4354)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1040
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4362, best: 0.4354)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0669
  ‚Ä¢ Validation Loss: 0.4378
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4378, best: 0.4354)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1279
  ‚Ä¢ Validation Loss: 0.4377
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4377, best: 0.4354)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1224
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4370, best: 0.4354)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1544
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4386, best: 0.4354)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0744
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4379, best: 0.4354)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1007
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4346
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1141
  ‚Ä¢ Validation Loss: 0.4348
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4348, best: 0.4346)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1344
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4369, best: 0.4346)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1127
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4369, best: 0.4346)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1308
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4358, best: 0.4346)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0944
  ‚Ä¢ Validation Loss: 0.4361
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4361, best: 0.4346)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.4391
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4391, best: 0.4346)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0670
  ‚Ä¢ Validation Loss: 0.4421
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4421, best: 0.4346)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1498
  ‚Ä¢ Validation Loss: 0.4379
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4379, best: 0.4346)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4399
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4399, best: 0.4346)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1369
  ‚Ä¢ Validation Loss: 0.4372
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4372, best: 0.4346)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1150
  ‚Ä¢ Validation Loss: 0.4373
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4373, best: 0.4346)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1187
  ‚Ä¢ Validation Loss: 0.4386
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4386, best: 0.4346)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1291
  ‚Ä¢ Validation Loss: 0.4345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1710
  ‚Ä¢ Validation Loss: 0.4349
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4349, best: 0.4345)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1106
  ‚Ä¢ Validation Loss: 0.4380
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4380, best: 0.4345)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1124
  ‚Ä¢ Validation Loss: 0.4343
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4343
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1167
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4356, best: 0.4343)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1296
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4353, best: 0.4343)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1257
  ‚Ä¢ Validation Loss: 0.4354
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4354, best: 0.4343)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1261
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4356, best: 0.4343)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1424
  ‚Ä¢ Validation Loss: 0.4346
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4346, best: 0.4343)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0835
  ‚Ä¢ Validation Loss: 0.4367
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4367, best: 0.4343)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.4356
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4356, best: 0.4343)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1250
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4352, best: 0.4343)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1027
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4390, best: 0.4343)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0601
  ‚Ä¢ Validation Loss: 0.4380
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4380, best: 0.4343)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1264
  ‚Ä¢ Validation Loss: 0.4349
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4349, best: 0.4343)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1084
  ‚Ä¢ Validation Loss: 0.4369
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4369, best: 0.4343)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0942
  ‚Ä¢ Validation Loss: 0.4359
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4359, best: 0.4343)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4343
Total Epochs:   300
Models Saved:   ./Result/a3/Latin16746
TensorBoard:    ./Result/a3/Latin16746/tensorboard_logs
================================================================================

[03:36:37] Training completed. Best val loss: 0.4343

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9880, Recall=0.9896, F1=0.9888, IoU=0.9779
Paratext            : Precision=0.7890, Recall=0.7867, F1=0.7878, IoU=0.6499
Decoration          : Precision=0.9727, Recall=0.9291, F1=0.9504, IoU=0.9054
Main Text           : Precision=0.8894, Recall=0.9137, F1=0.9014, IoU=0.8204
Title               : Precision=0.7868, Recall=0.7197, F1=0.7517, IoU=0.6022
Chapter Headings    : Precision=0.9053, Recall=0.6686, F1=0.7691, IoU=0.6248

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8885
Mean Recall:    0.8345
Mean F1-Score:  0.8582
Mean IoU:       0.7635
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Output Directory: ./Result/a3/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: SIMPLE
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=9,600,974
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a3/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.5721
  ‚Ä¢ Validation Loss: 0.7610
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7610
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8589
  ‚Ä¢ Validation Loss: 0.7087
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7087
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7543
  ‚Ä¢ Validation Loss: 0.6301
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6301
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7043
  ‚Ä¢ Validation Loss: 0.6227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6227
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6737
  ‚Ä¢ Validation Loss: 0.6114
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6114
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6531
  ‚Ä¢ Validation Loss: 0.5982
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5982
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5957
  ‚Ä¢ Validation Loss: 0.6091
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6091, best: 0.5982)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5555
  ‚Ä¢ Validation Loss: 0.6018
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6018, best: 0.5982)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5764
  ‚Ä¢ Validation Loss: 0.5856
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5856
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5145
  ‚Ä¢ Validation Loss: 0.5841
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5841
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5216
  ‚Ä¢ Validation Loss: 0.5782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5782
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4688
  ‚Ä¢ Validation Loss: 0.5763
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5763
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5105
  ‚Ä¢ Validation Loss: 0.5686
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5686
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4540
  ‚Ä¢ Validation Loss: 0.5711
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5711, best: 0.5686)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4028
  ‚Ä¢ Validation Loss: 0.5683
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5683
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4682
  ‚Ä¢ Validation Loss: 0.5746
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5746, best: 0.5683)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4556
  ‚Ä¢ Validation Loss: 0.5693
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5693, best: 0.5683)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5046
  ‚Ä¢ Validation Loss: 0.5605
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4616
  ‚Ä¢ Validation Loss: 0.5583
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5583
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4652
  ‚Ä¢ Validation Loss: 0.5624
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5624, best: 0.5583)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4417
  ‚Ä¢ Validation Loss: 0.5547
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4328
  ‚Ä¢ Validation Loss: 0.5600
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5600, best: 0.5547)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4137
  ‚Ä¢ Validation Loss: 0.5513
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5513
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4173
  ‚Ä¢ Validation Loss: 0.5505
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5505
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3583
  ‚Ä¢ Validation Loss: 0.5501
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5501
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4492
  ‚Ä¢ Validation Loss: 0.5504
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5504, best: 0.5501)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4204
  ‚Ä¢ Validation Loss: 0.5527
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5527, best: 0.5501)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4643
  ‚Ä¢ Validation Loss: 0.5490
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5490
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4129
  ‚Ä¢ Validation Loss: 0.5493
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5493, best: 0.5490)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4248
  ‚Ä¢ Validation Loss: 0.5484
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5484
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4323
  ‚Ä¢ Validation Loss: 0.5457
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5457
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4097
  ‚Ä¢ Validation Loss: 0.5502
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5502, best: 0.5457)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4326
  ‚Ä¢ Validation Loss: 0.5505
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5505, best: 0.5457)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4460
  ‚Ä¢ Validation Loss: 0.5409
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5409
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4156
  ‚Ä¢ Validation Loss: 0.5423
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5423, best: 0.5409)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4354
  ‚Ä¢ Validation Loss: 0.5445
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5445, best: 0.5409)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4323
  ‚Ä¢ Validation Loss: 0.5458
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5458, best: 0.5409)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4137
  ‚Ä¢ Validation Loss: 0.5404
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5404
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4107
  ‚Ä¢ Validation Loss: 0.5472
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5472, best: 0.5404)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4316
  ‚Ä¢ Validation Loss: 0.5385
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5385
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4252
  ‚Ä¢ Validation Loss: 0.5409
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5409, best: 0.5385)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4315
  ‚Ä¢ Validation Loss: 0.5474
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5474, best: 0.5385)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3982
  ‚Ä¢ Validation Loss: 0.5406
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5406, best: 0.5385)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4308
  ‚Ä¢ Validation Loss: 0.5390
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5390, best: 0.5385)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3852
  ‚Ä¢ Validation Loss: 0.5402
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5402, best: 0.5385)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3912
  ‚Ä¢ Validation Loss: 0.5417
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5417, best: 0.5385)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3935
  ‚Ä¢ Validation Loss: 0.5415
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5415, best: 0.5385)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4175
  ‚Ä¢ Validation Loss: 0.5405
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5405, best: 0.5385)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4281
  ‚Ä¢ Validation Loss: 0.5404
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5404, best: 0.5385)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3455
  ‚Ä¢ Validation Loss: 0.5418
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5418, best: 0.5385)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3276
  ‚Ä¢ Validation Loss: 0.5406
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5406, best: 0.5385)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3460
  ‚Ä¢ Validation Loss: 0.5413
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5413, best: 0.5385)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3428
  ‚Ä¢ Validation Loss: 0.5470
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5470, best: 0.5385)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3556
  ‚Ä¢ Validation Loss: 0.5457
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5457, best: 0.5385)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3280
  ‚Ä¢ Validation Loss: 0.5464
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5464, best: 0.5385)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3430
  ‚Ä¢ Validation Loss: 0.5371
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5371
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3338
  ‚Ä¢ Validation Loss: 0.5593
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5593, best: 0.5371)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2945
  ‚Ä¢ Validation Loss: 0.5409
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5409, best: 0.5371)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3550
  ‚Ä¢ Validation Loss: 0.5453
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5453, best: 0.5371)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3219
  ‚Ä¢ Validation Loss: 0.5379
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5379, best: 0.5371)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3161
  ‚Ä¢ Validation Loss: 0.5408
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5408, best: 0.5371)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3027
  ‚Ä¢ Validation Loss: 0.5349
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5349
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3228
  ‚Ä¢ Validation Loss: 0.5381
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5381, best: 0.5349)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3664
  ‚Ä¢ Validation Loss: 0.5345
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5345
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3015
  ‚Ä¢ Validation Loss: 0.5340
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5340
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3252
  ‚Ä¢ Validation Loss: 0.5483
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5483, best: 0.5340)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3145
  ‚Ä¢ Validation Loss: 0.5396
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5396, best: 0.5340)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3176
  ‚Ä¢ Validation Loss: 0.5309
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5309
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3112
  ‚Ä¢ Validation Loss: 0.5333
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5333, best: 0.5309)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2692
  ‚Ä¢ Validation Loss: 0.5319
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5319, best: 0.5309)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3340
  ‚Ä¢ Validation Loss: 0.5278
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5278
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2706
  ‚Ä¢ Validation Loss: 0.5363
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5363, best: 0.5278)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3320
  ‚Ä¢ Validation Loss: 0.5288
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5288, best: 0.5278)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3092
  ‚Ä¢ Validation Loss: 0.5291
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5291, best: 0.5278)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3124
  ‚Ä¢ Validation Loss: 0.5399
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5399, best: 0.5278)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2846
  ‚Ä¢ Validation Loss: 0.5368
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5368, best: 0.5278)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3003
  ‚Ä¢ Validation Loss: 0.5299
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5299, best: 0.5278)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3602
  ‚Ä¢ Validation Loss: 0.5316
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5316, best: 0.5278)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3303
  ‚Ä¢ Validation Loss: 0.5208
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5208
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2568
  ‚Ä¢ Validation Loss: 0.5293
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5293, best: 0.5208)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3365
  ‚Ä¢ Validation Loss: 0.5356
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5356, best: 0.5208)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3363
  ‚Ä¢ Validation Loss: 0.5227
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5227, best: 0.5208)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2620
  ‚Ä¢ Validation Loss: 0.5185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5185
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3135
  ‚Ä¢ Validation Loss: 0.5293
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5293, best: 0.5185)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3422
  ‚Ä¢ Validation Loss: 0.5228
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5228, best: 0.5185)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3151
  ‚Ä¢ Validation Loss: 0.5220
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5220, best: 0.5185)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2781
  ‚Ä¢ Validation Loss: 0.5222
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5222, best: 0.5185)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3292
  ‚Ä¢ Validation Loss: 0.5192
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5192, best: 0.5185)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3030
  ‚Ä¢ Validation Loss: 0.5238
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5238, best: 0.5185)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2764
  ‚Ä¢ Validation Loss: 0.5247
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5247, best: 0.5185)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3360
  ‚Ä¢ Validation Loss: 0.5208
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5208, best: 0.5185)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2932
  ‚Ä¢ Validation Loss: 0.5208
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5208, best: 0.5185)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2614
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5283, best: 0.5185)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2488
  ‚Ä¢ Validation Loss: 0.5171
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5171
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3431
  ‚Ä¢ Validation Loss: 0.5224
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5224, best: 0.5171)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3101
  ‚Ä¢ Validation Loss: 0.5184
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5184, best: 0.5171)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3306
  ‚Ä¢ Validation Loss: 0.5226
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5226, best: 0.5171)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2674
  ‚Ä¢ Validation Loss: 0.5167
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5167
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2974
  ‚Ä¢ Validation Loss: 0.5118
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5118
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3204
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.5157, best: 0.5118)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3001
  ‚Ä¢ Validation Loss: 0.5139
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5139, best: 0.5118)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2816
  ‚Ä¢ Validation Loss: 0.5172
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5172, best: 0.5118)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2862
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5157, best: 0.5118)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2840
  ‚Ä¢ Validation Loss: 0.5141
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5141, best: 0.5118)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2389
  ‚Ä¢ Validation Loss: 0.5162
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5162, best: 0.5118)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2852
  ‚Ä¢ Validation Loss: 0.5156
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5156, best: 0.5118)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2718
  ‚Ä¢ Validation Loss: 0.5210
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5210, best: 0.5118)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2770
  ‚Ä¢ Validation Loss: 0.5162
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5162, best: 0.5118)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3226
  ‚Ä¢ Validation Loss: 0.5100
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5100
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3171
  ‚Ä¢ Validation Loss: 0.5136
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5136, best: 0.5100)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2243
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5152, best: 0.5100)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1858
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5152, best: 0.5100)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1430
  ‚Ä¢ Validation Loss: 0.5105
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5105, best: 0.5100)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1747
  ‚Ä¢ Validation Loss: 0.5110
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5110, best: 0.5100)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1640
  ‚Ä¢ Validation Loss: 0.5109
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5109, best: 0.5100)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1929
  ‚Ä¢ Validation Loss: 0.5121
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5121, best: 0.5100)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1580
  ‚Ä¢ Validation Loss: 0.5104
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5104, best: 0.5100)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1512
  ‚Ä¢ Validation Loss: 0.5120
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5120, best: 0.5100)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2051
  ‚Ä¢ Validation Loss: 0.5111
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5111, best: 0.5100)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2291
  ‚Ä¢ Validation Loss: 0.5097
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5097
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1736
  ‚Ä¢ Validation Loss: 0.5081
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5081
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1273
  ‚Ä¢ Validation Loss: 0.5096
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5096, best: 0.5081)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1658
  ‚Ä¢ Validation Loss: 0.5125
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5125, best: 0.5081)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1608
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5080
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1492
  ‚Ä¢ Validation Loss: 0.5113
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5113, best: 0.5080)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2056
  ‚Ä¢ Validation Loss: 0.5096
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5096, best: 0.5080)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1805
  ‚Ä¢ Validation Loss: 0.5102
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5102, best: 0.5080)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1174
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5080, best: 0.5080)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1143
  ‚Ä¢ Validation Loss: 0.5083
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5083, best: 0.5080)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1601
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5098, best: 0.5080)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1709
  ‚Ä¢ Validation Loss: 0.5122
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5122, best: 0.5080)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1185
  ‚Ä¢ Validation Loss: 0.5099
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5099, best: 0.5080)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2185
  ‚Ä¢ Validation Loss: 0.5117
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5117, best: 0.5080)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1554
  ‚Ä¢ Validation Loss: 0.5088
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5088, best: 0.5080)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1199
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1402
  ‚Ä¢ Validation Loss: 0.5084
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5084, best: 0.5078)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1474
  ‚Ä¢ Validation Loss: 0.5107
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5107, best: 0.5078)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2114
  ‚Ä¢ Validation Loss: 0.5088
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5088, best: 0.5078)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1611
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5098, best: 0.5078)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1450
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5080, best: 0.5078)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1419
  ‚Ä¢ Validation Loss: 0.5072
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5072
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1296
  ‚Ä¢ Validation Loss: 0.5086
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5086, best: 0.5072)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1048
  ‚Ä¢ Validation Loss: 0.5087
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5087, best: 0.5072)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1797
  ‚Ä¢ Validation Loss: 0.5073
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5073, best: 0.5072)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1281
  ‚Ä¢ Validation Loss: 0.5090
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5090, best: 0.5072)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1137
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5080, best: 0.5072)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1445
  ‚Ä¢ Validation Loss: 0.5083
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5083, best: 0.5072)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1433
  ‚Ä¢ Validation Loss: 0.5085
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5085, best: 0.5072)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1640
  ‚Ä¢ Validation Loss: 0.5081
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5081, best: 0.5072)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1477
  ‚Ä¢ Validation Loss: 0.5079
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5079, best: 0.5072)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0788
  ‚Ä¢ Validation Loss: 0.5181
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5181, best: 0.5072)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1130
  ‚Ä¢ Validation Loss: 0.5219
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5219, best: 0.5072)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1456
  ‚Ä¢ Validation Loss: 0.5184
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5184, best: 0.5072)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1592
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5161, best: 0.5072)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1384
  ‚Ä¢ Validation Loss: 0.5238
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5238, best: 0.5072)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1192
  ‚Ä¢ Validation Loss: 0.5144
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5144, best: 0.5072)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1504
  ‚Ä¢ Validation Loss: 0.5122
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5122, best: 0.5072)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1807
  ‚Ä¢ Validation Loss: 0.5208
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5208, best: 0.5072)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1712
  ‚Ä¢ Validation Loss: 0.5242
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5242, best: 0.5072)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1594
  ‚Ä¢ Validation Loss: 0.5097
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5097, best: 0.5072)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2020
  ‚Ä¢ Validation Loss: 0.5092
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5092, best: 0.5072)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1478
  ‚Ä¢ Validation Loss: 0.5153
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5153, best: 0.5072)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1754
  ‚Ä¢ Validation Loss: 0.5115
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5115, best: 0.5072)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1849
  ‚Ä¢ Validation Loss: 0.5067
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5067
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1458
  ‚Ä¢ Validation Loss: 0.5135
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5135, best: 0.5067)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2080
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5161, best: 0.5067)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1692
  ‚Ä¢ Validation Loss: 0.5120
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5120, best: 0.5067)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1244
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5098, best: 0.5067)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1435
  ‚Ä¢ Validation Loss: 0.5110
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5110, best: 0.5067)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.5119
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5119, best: 0.5067)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1183
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5142, best: 0.5067)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0984
  ‚Ä¢ Validation Loss: 0.5071
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5071, best: 0.5067)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1359
  ‚Ä¢ Validation Loss: 0.5167
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5167, best: 0.5067)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1597
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5157, best: 0.5067)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0621
  ‚Ä¢ Validation Loss: 0.5084
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5084, best: 0.5067)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1075
  ‚Ä¢ Validation Loss: 0.5123
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5123, best: 0.5067)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1265
  ‚Ä¢ Validation Loss: 0.5113
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5113, best: 0.5067)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1561
  ‚Ä¢ Validation Loss: 0.5054
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5054
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1184
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5076, best: 0.5054)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1322
  ‚Ä¢ Validation Loss: 0.5062
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5062, best: 0.5054)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1556
  ‚Ä¢ Validation Loss: 0.5035
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5035
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1141
  ‚Ä¢ Validation Loss: 0.5103
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5103, best: 0.5035)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1111
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5142, best: 0.5035)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1143
  ‚Ä¢ Validation Loss: 0.5126
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5126, best: 0.5035)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1718
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5058, best: 0.5035)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1545
  ‚Ä¢ Validation Loss: 0.5121
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5121, best: 0.5035)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1366
  ‚Ä¢ Validation Loss: 0.5116
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5116, best: 0.5035)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1050
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5034
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1425
  ‚Ä¢ Validation Loss: 0.5149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5149, best: 0.5034)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0943
  ‚Ä¢ Validation Loss: 0.5112
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5112, best: 0.5034)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1447
  ‚Ä¢ Validation Loss: 0.5096
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5096, best: 0.5034)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1224
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5034, best: 0.5034)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1530
  ‚Ä¢ Validation Loss: 0.5012
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5012
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1303
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5078, best: 0.5012)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1217
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5094, best: 0.5012)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1470
  ‚Ä¢ Validation Loss: 0.5052
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5052, best: 0.5012)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1486
  ‚Ä¢ Validation Loss: 0.5057
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5057, best: 0.5012)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1526
  ‚Ä¢ Validation Loss: 0.5021
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5021, best: 0.5012)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1002
  ‚Ä¢ Validation Loss: 0.5073
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5073, best: 0.5012)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1807
  ‚Ä¢ Validation Loss: 0.5039
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.5039, best: 0.5012)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1665
  ‚Ä¢ Validation Loss: 0.5082
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5082, best: 0.5012)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1329
  ‚Ä¢ Validation Loss: 0.5022
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5022, best: 0.5012)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1172
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4996
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1002
  ‚Ä¢ Validation Loss: 0.5023
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5023, best: 0.4996)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1331
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5058, best: 0.4996)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1117
  ‚Ä¢ Validation Loss: 0.5068
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5068, best: 0.4996)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1128
  ‚Ä¢ Validation Loss: 0.5039
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5039, best: 0.4996)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1001
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5036, best: 0.4996)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1489
  ‚Ä¢ Validation Loss: 0.5005
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5005, best: 0.4996)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1180
  ‚Ä¢ Validation Loss: 0.5009
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5009, best: 0.4996)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1950
  ‚Ä¢ Validation Loss: 0.5046
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5046, best: 0.4996)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1157
  ‚Ä¢ Validation Loss: 0.5031
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5031, best: 0.4996)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1460
  ‚Ä¢ Validation Loss: 0.5062
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5062, best: 0.4996)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1278
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5032, best: 0.4996)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1042
  ‚Ä¢ Validation Loss: 0.5058
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5058, best: 0.4996)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1496
  ‚Ä¢ Validation Loss: 0.5037
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5037, best: 0.4996)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1714
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5056, best: 0.4996)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1246
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5076, best: 0.4996)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1723
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5036, best: 0.4996)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1493
  ‚Ä¢ Validation Loss: 0.5013
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5013, best: 0.4996)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1020
  ‚Ä¢ Validation Loss: 0.5030
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5030, best: 0.4996)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1381
  ‚Ä¢ Validation Loss: 0.5038
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5038, best: 0.4996)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0237
  ‚Ä¢ Validation Loss: 0.5071
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5071, best: 0.4996)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0085
  ‚Ä¢ Validation Loss: 0.5046
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5046, best: 0.4996)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0153
  ‚Ä¢ Validation Loss: 0.5027
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5027, best: 0.4996)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0260
  ‚Ä¢ Validation Loss: 0.5060
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5060, best: 0.4996)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0229
  ‚Ä¢ Validation Loss: 0.5050
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5050, best: 0.4996)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0208
  ‚Ä¢ Validation Loss: 0.5057
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5057, best: 0.4996)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0152
  ‚Ä¢ Validation Loss: 0.5064
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5064, best: 0.4996)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.5035
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5035, best: 0.4996)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0162
  ‚Ä¢ Validation Loss: 0.5024
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5024, best: 0.4996)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.5041
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5041, best: 0.4996)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0339
  ‚Ä¢ Validation Loss: 0.5042
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5042, best: 0.4996)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0345
  ‚Ä¢ Validation Loss: 0.5050
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5050, best: 0.4996)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0201
  ‚Ä¢ Validation Loss: 0.5054
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5054, best: 0.4996)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.5063
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5063, best: 0.4996)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0528
  ‚Ä¢ Validation Loss: 0.5103
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5103, best: 0.4996)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0250
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5056, best: 0.4996)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0366
  ‚Ä¢ Validation Loss: 0.5050
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5050, best: 0.4996)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0067
  ‚Ä¢ Validation Loss: 0.5022
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5022, best: 0.4996)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0155
  ‚Ä¢ Validation Loss: 0.5025
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5025, best: 0.4996)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0070
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5007, best: 0.4996)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0217
  ‚Ä¢ Validation Loss: 0.5006
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5006, best: 0.4996)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0205
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5034, best: 0.4996)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0227
  ‚Ä¢ Validation Loss: 0.5057
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5057, best: 0.4996)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0229
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5020, best: 0.4996)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0423
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4993
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0411
  ‚Ä¢ Validation Loss: 0.5025
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5025, best: 0.4993)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0632
  ‚Ä¢ Validation Loss: 0.5081
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5081, best: 0.4993)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0585
  ‚Ä¢ Validation Loss: 0.5030
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5030, best: 0.4993)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0364
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5010, best: 0.4993)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0476
  ‚Ä¢ Validation Loss: 0.4995
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4995, best: 0.4993)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0179
  ‚Ä¢ Validation Loss: 0.5016
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5016, best: 0.4993)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0222
  ‚Ä¢ Validation Loss: 0.5027
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5027, best: 0.4993)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5034, best: 0.4993)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0319
  ‚Ä¢ Validation Loss: 0.5041
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5041, best: 0.4993)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0339
  ‚Ä¢ Validation Loss: 0.5018
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5018, best: 0.4993)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0067
  ‚Ä¢ Validation Loss: 0.5033
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5033, best: 0.4993)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0131
  ‚Ä¢ Validation Loss: 0.5024
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5024, best: 0.4993)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0135
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5001, best: 0.4993)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0218
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0308
  ‚Ä¢ Validation Loss: 0.4970
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4970
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0581
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4988, best: 0.4970)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0080
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4996, best: 0.4970)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0070
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4990, best: 0.4970)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0250
  ‚Ä¢ Validation Loss: 0.5013
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5013, best: 0.4970)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0065
  ‚Ä¢ Validation Loss: 0.5015
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5015, best: 0.4970)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0344
  ‚Ä¢ Validation Loss: 0.5016
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5016, best: 0.4970)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0272
  ‚Ä¢ Validation Loss: 0.5017
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5017, best: 0.4970)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0324
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5036, best: 0.4970)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0542
  ‚Ä¢ Validation Loss: 0.5002
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5002, best: 0.4970)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0165
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5010, best: 0.4970)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0479
  ‚Ä¢ Validation Loss: 0.5013
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5013, best: 0.4970)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0482
  ‚Ä¢ Validation Loss: 0.4997
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4997, best: 0.4970)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0190
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4993, best: 0.4970)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0301
  ‚Ä¢ Validation Loss: 0.4989
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4989, best: 0.4970)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0356
  ‚Ä¢ Validation Loss: 0.5003
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5003, best: 0.4970)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0244
  ‚Ä¢ Validation Loss: 0.5014
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5014, best: 0.4970)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0175
  ‚Ä¢ Validation Loss: 0.5026
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5026, best: 0.4970)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0136
  ‚Ä¢ Validation Loss: 0.5021
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5021, best: 0.4970)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0295
  ‚Ä¢ Validation Loss: 0.5046
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5046, best: 0.4970)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0066
  ‚Ä¢ Validation Loss: 0.5045
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5045, best: 0.4970)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0331
  ‚Ä¢ Validation Loss: 0.5029
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5029, best: 0.4970)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.5009
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5009, best: 0.4970)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0248
  ‚Ä¢ Validation Loss: 0.4998
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4998, best: 0.4970)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0194
  ‚Ä¢ Validation Loss: 0.4984
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4984, best: 0.4970)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0133
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4990, best: 0.4970)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0431
  ‚Ä¢ Validation Loss: 0.4981
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4981, best: 0.4970)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0128
  ‚Ä¢ Validation Loss: 0.4994
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4994, best: 0.4970)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0333
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5001, best: 0.4970)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0146
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4988, best: 0.4970)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0221
  ‚Ä¢ Validation Loss: 0.5018
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5018, best: 0.4970)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0416
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5007, best: 0.4970)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0144
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5007, best: 0.4970)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0494
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4974, best: 0.4970)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0293
  ‚Ä¢ Validation Loss: 0.4979
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4979, best: 0.4970)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0126
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4978, best: 0.4970)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0203
  ‚Ä¢ Validation Loss: 0.4995
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4995, best: 0.4970)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0602
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4988, best: 0.4970)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0260
  ‚Ä¢ Validation Loss: 0.4983
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4983, best: 0.4970)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4970
Total Epochs:   300
Models Saved:   ./Result/a3/Syr341
TensorBoard:    ./Result/a3/Syr341/tensorboard_logs
================================================================================

[04:50:10] Training completed. Best val loss: 0.4970

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: simple
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (convolutional heads)
   Aux dims: [384, 192, 96]
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SIMPLE
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9606, Recall=0.9816, F1=0.9710, IoU=0.9436
Paratext            : Precision=0.4867, Recall=0.3581, F1=0.4126, IoU=0.2599
Decoration          : Precision=0.9395, Recall=0.5804, F1=0.7175, IoU=0.5595
Main Text           : Precision=0.8547, Recall=0.7971, F1=0.8249, IoU=0.7020
Title               : Precision=0.3115, Recall=0.1433, F1=0.1963, IoU=0.1088

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7106
Mean Recall:    0.5721
Mean F1-Score:  0.6245
Mean IoU:       0.5148
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.8154
Mean Recall:    0.7365
Mean F1-Score:  0.7660
Mean IoU:       0.6626
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
Results Location: ./Result/a3/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Mon Nov 17 04:53:01 AM CET 2025
= Job-ID             : 1391401 on tinygpu
= Job-Name           : 5th
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run3.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 05:07:52
= Total RAM usage    : 10.7 GiB of requested  GiB (%)   
= Node list          : tg065
= Subm/Elig/Start/End: 2025-11-16T23:44:41 / 2025-11-16T23:44:41 / 2025-11-16T23:44:59 / 2025-11-17T04:52:51
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              85.4G   104.9G   209.7G        N/A     236K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 164889, 61 %, 40 %, 4530 MiB, 4521158 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 176150, 28 %, 16 %, 876 MiB, 150277 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 176188, 62 %, 41 %, 4530 MiB, 4490935 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 187407, 21 %, 11 %, 876 MiB, 151695 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 187450, 61 %, 40 %, 4530 MiB, 4505974 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 198652, 20 %, 11 %, 876 MiB, 156216 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 198693, 63 %, 41 %, 4550 MiB, 4237091 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:AF:00.0, 209992, 20 %, 10 %, 876 MiB, 161799 ms
