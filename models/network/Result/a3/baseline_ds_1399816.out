### Starting TaskPrologue of job 1399816 on tg083 at Wed Nov 19 03:23:08 PM CET 2025
Running on cores 12-13,28-29,44-45,60-61 with governor powersave
Wed Nov 19 15:23:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:B1:00.0 Off |                  N/A |
| 37%   63C    P5             61W /  300W |       1MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + DEEP SUPERVISION + SMART FUSION

Component Details:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: 2 Swin Transformer blocks (enabled)
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart (Attention-based Smart Skip Connections)
  âœ“ Adapter mode: streaming (integrated)
  âœ“ GroupNorm: enabled
  âœ“ Deep Supervision: ENABLED
    - 3 auxiliary outputs from decoder layers 2, 3, 4
    - Resolutions: H/16, H/8, H/4 (native, no upsampling)
    - MSAGHNet-style: Simple OutConv heads
  âœ“ Smart Skip Connections:
    - 4 skip connections: Encoder 4â†’Decoder 4, Encoder 3â†’Decoder 3, Encoder 2â†’Decoder 2, Encoder 1â†’Decoder 1
    - Attention-based fusion (Alignment + Multi-head Attention + Fusion + Residual)
  âœ— Multi-Scale Aggregation: disabled
  âœ— Fourier Feature Fusion: disabled (using Smart fusion)
  âœ— GCFF Fusion: disabled (using Smart fusion)
  âœ“ Balanced Sampler: ENABLED (oversamples rare classes)
  âœ“ Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)

Training Parameters:
  - Batch Size: 24
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
  - Loss: CE (weighted) + Focal (Î³=2.0) + Dice + Deep Supervision Auxiliary Loss
============================================================================


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING BASELINE + DEEP SUPERVISION: Latin2
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a3/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
âœ“ Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
âœ“ Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Enabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
ğŸš€ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - âœ… Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ“
  â€¢ Fusion Method: SMART
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin2
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

âœ“ Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=21,839,602
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a3/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin2/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a3/Latin2/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âš ï¸  Could not load optimizer state: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
   Starting with fresh optimizer state (this is OK if architecture changed)
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 276
   âœ“ Best validation loss: 0.3859
   âœ“ Resuming from epoch 277

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 277
================================================================================


EPOCH 278/300
--------------------------------------------------

âŒ CUDA OUT OF MEMORY at epoch 278
Attempting to recover...
Emergency checkpoint saved: ./Result/a3/Latin2/emergency_epoch_278.pth
[15:26:20] CUDA OOM at epoch 278: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 409, in <module>
    main()
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 403, in main
    result = trainer_synapse(args, model, args.output_dir, train_dataset, val_dataset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 1227, in trainer_synapse
    train_losses = run_training_epoch(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 686, in run_training_epoch
    predictions = model(images)
                  ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/vision_transformer_cnn.py", line 118, in forward
    return self.model(x)
           ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1845, in forward
    x = self.smart_skips[inx](skip_features, x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1095, in forward
    skip_enhanced, _ = self.attention(skip_tokens, skip_tokens, skip_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1380, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 6450, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 1422, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ— TRAINING FAILED: Latin2 (Exit Code: 1)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Skipping testing for Latin2 due to training failure.


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING BASELINE + DEEP SUPERVISION: Latin14396
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a3/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
âœ“ Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
âœ“ Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Enabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
ğŸš€ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - âœ… Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ“
  â€¢ Fusion Method: SMART
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin14396
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

âœ“ Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=21,839,602
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a3/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin14396/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a3/Latin14396/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âš ï¸  Could not load optimizer state: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
   Starting with fresh optimizer state (this is OK if architecture changed)
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 285
   âœ“ Best validation loss: 0.4009
   âœ“ Resuming from epoch 286

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 286
================================================================================


EPOCH 287/300
--------------------------------------------------

âŒ CUDA OUT OF MEMORY at epoch 287
Attempting to recover...
Emergency checkpoint saved: ./Result/a3/Latin14396/emergency_epoch_287.pth
[15:28:44] CUDA OOM at epoch 287: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 409, in <module>
    main()
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 403, in main
    result = trainer_synapse(args, model, args.output_dir, train_dataset, val_dataset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 1227, in trainer_synapse
    train_losses = run_training_epoch(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 686, in run_training_epoch
    predictions = model(images)
                  ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/vision_transformer_cnn.py", line 118, in forward
    return self.model(x)
           ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1845, in forward
    x = self.smart_skips[inx](skip_features, x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1095, in forward
    skip_enhanced, _ = self.attention(skip_tokens, skip_tokens, skip_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1380, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 6450, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 1422, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ— TRAINING FAILED: Latin14396 (Exit Code: 1)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Skipping testing for Latin14396 due to training failure.


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING BASELINE + DEEP SUPERVISION: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a3/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
âœ“ Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
âœ“ Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Enabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
ğŸš€ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - âœ… Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ“
  â€¢ Fusion Method: SMART
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Latin16746
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

âœ“ Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=21,839,602
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a3/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Latin16746/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a3/Latin16746/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âš ï¸  Could not load optimizer state: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
   Starting with fresh optimizer state (this is OK if architecture changed)
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 299
   âœ“ Best validation loss: 0.3543
   âœ“ Resuming from epoch 300

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 300
================================================================================


================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.3543
Total Epochs:   300
Models Saved:   ./Result/a3/Latin16746
TensorBoard:    ./Result/a3/Latin16746/tensorboard_logs
================================================================================

[15:31:05] Training completed. Best val loss: 0.3543

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TRAINING COMPLETED: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Proceeding to testing...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TESTING BASELINE + DEEP SUPERVISION: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test Configuration:
  âœ“ Test-Time Augmentation (TTA): ENABLED
  âœ— CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
WARNING:root:Checkpoint and model have architecture differences - loading with strict=False
WARNING:root:  Deep Supervision: True, Fusion: True, Bottleneck: False, Adapter: False, Multi-scale: False
WARNING:root:Ignored 60 unexpected keys
WARNING:root:Missing 72 fusion-related keys
ERROR:root:Missing 8 other keys - model may not work correctly!
=== Historical Document Segmentation Testing ===

================================================================================
ğŸš€ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Enabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
ğŸš€ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - âœ… Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
âš ï¸  WARNING: Checkpoint and model have architecture differences!
   Deep Supervision mismatch: True
   Fusion mismatch: True
   Bottleneck mismatch: False
   Adapter mismatch: False
   Multi-scale mismatch: False
   Loading with strict=False (some weights may not load correctly)
âš ï¸  Ignored 60 unexpected keys in checkpoint
âš ï¸  Missing 72 fusion-related keys (expected if fusion method differs)
âš ï¸  CRITICAL: Missing 8 other keys - model may not work correctly!
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): âœ“ ENABLED
  â†’ Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90Â°
  â†’ Averaging predictions across all augmentations
CRF POST-PROCESSING: âœ— DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
âœ“ Ground truth found for 009
âœ“ Completed: 009
Processing: 020 (54 patches)
âœ“ Ground truth found for 020
âœ“ Completed: 020
Processing: 022 (54 patches)
âœ“ Ground truth found for 022
âœ“ Completed: 022
Processing: 029 (54 patches)
âœ“ Ground truth found for 029
âœ“ Completed: 029
Processing: 035 (54 patches)
âœ“ Ground truth found for 035
âœ“ Completed: 035
Processing: 048 (54 patches)
âœ“ Ground truth found for 048
âœ“ Completed: 048
Processing: 069 (54 patches)
âœ“ Ground truth found for 069
âœ“ Completed: 069
Processing: 082 (54 patches)
âœ“ Ground truth found for 082
âœ“ Completed: 082
Processing: 088 (54 patches)
âœ“ Ground truth found for 088
âœ“ Completed: 088
Processing: 089 (54 patches)
âœ“ Ground truth found for 089
âœ“ Completed: 089
Processing: 091 (54 patches)
âœ“ Ground truth found for 091
âœ“ Completed: 091
Processing: 100 (54 patches)
âœ“ Ground truth found for 100
âœ“ Completed: 100
Processing: 106 (54 patches)
âœ“ Ground truth found for 106
âœ“ Completed: 106
Processing: 117 (54 patches)
âœ“ Ground truth found for 117
âœ“ Completed: 117
Processing: 123 (54 patches)
âœ“ Ground truth found for 123
âœ“ Completed: 123
Processing: 125 (54 patches)
âœ“ Ground truth found for 125
âœ“ Completed: 125
Processing: 130 (54 patches)
âœ“ Ground truth found for 130
âœ“ Completed: 130
Processing: 133 (54 patches)
âœ“ Ground truth found for 133
âœ“ Completed: 133
Processing: 137 (54 patches)
âœ“ Ground truth found for 137
âœ“ Completed: 137
Processing: 146 (54 patches)
âœ“ Ground truth found for 146
âœ“ Completed: 146
Processing: 166 (54 patches)
âœ“ Ground truth found for 166
âœ“ Completed: 166
Processing: 184 (54 patches)
âœ“ Ground truth found for 184
âœ“ Completed: 184
Processing: 215 (54 patches)
âœ“ Ground truth found for 215
âœ“ Completed: 215
Processing: 237 (54 patches)
âœ“ Ground truth found for 237
âœ“ Completed: 237
Processing: 243 (54 patches)
âœ“ Ground truth found for 243
âœ“ Completed: 243
Processing: 255 (54 patches)
âœ“ Ground truth found for 255
âœ“ Completed: 255
Processing: 258 (54 patches)
âœ“ Ground truth found for 258
âœ“ Completed: 258
Processing: 284 (54 patches)
âœ“ Ground truth found for 284
âœ“ Completed: 284
Processing: 325 (54 patches)
âœ“ Ground truth found for 325
âœ“ Completed: 325
Processing: 357 (54 patches)
âœ“ Ground truth found for 357
âœ“ Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.8703, Recall=0.7689, F1=0.8165, IoU=0.6899
Paratext            : Precision=0.0084, Recall=0.0055, F1=0.0067, IoU=0.0033
Decoration          : Precision=0.0403, Recall=0.2216, F1=0.0682, IoU=0.0353
Main Text           : Precision=0.2045, Recall=0.0100, F1=0.0191, IoU=0.0096
Title               : Precision=0.0004, Recall=0.0001, F1=0.0001, IoU=0.0001
Chapter Headings    : Precision=0.0091, Recall=0.0322, F1=0.0142, IoU=0.0072

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.1888
Mean Recall:    0.1730
Mean F1-Score:  0.1541
Mean IoU:       0.1242
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ“ TESTING COMPLETED: Latin16746
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRAINING BASELINE + DEEP SUPERVISION: Syr341
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Output Directory: ./Result/a3/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
âœ“ Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
âœ“ Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
ğŸš€ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  âœ“ EfficientNet-B4 Encoder
  âœ“ Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  âœ“ Swin Transformer Decoder
  âœ“ Fusion Method: smart
  âœ“ Adapter Mode: streaming
  âœ“ Deep Supervision: Enabled
  âœ“ Multi-Scale Aggregation: Disabled
  âœ“ Normalization: GroupNorm
================================================================================
ğŸš€ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - âœ… Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: SMART
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - âœ… GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + AFF + FL
  â€¢ Bottleneck: âœ“
  â€¢ Adapter Mode: streaming
  â€¢ Deep Supervision: âœ“
  â€¢ Fusion Method: SMART
  â€¢ Multi-Scale Aggregation: âœ—
  â€¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a3/Syr341
================================================================================

ğŸ“Š Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

ğŸ“ˆ Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

âœ“ Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=21,833,486
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

ğŸš€ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

ğŸ” Checking for checkpoint at: ./Result/a3/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a3/Syr341/best_model_latest.pth
   File exists: True

ğŸ“‚ Found checkpoint: ./Result/a3/Syr341/best_model_latest.pth
   Attempting to resume training...
   âœ“ Loaded model state
   âš ï¸  Could not load optimizer state: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
   Starting with fresh optimizer state (this is OK if architecture changed)
   âœ“ Loaded scheduler state
   âœ“ Loaded scaler state (AMP)
   âœ“ Successfully loaded checkpoint from epoch 290
   âœ“ Best validation loss: 0.4176
   âœ“ Resuming from epoch 291

================================================================================
ğŸš€ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
Resuming from epoch: 291
================================================================================


EPOCH 292/300
--------------------------------------------------

âŒ CUDA OUT OF MEMORY at epoch 292
Attempting to recover...
Emergency checkpoint saved: ./Result/a3/Syr341/emergency_epoch_292.pth
[15:39:10] CUDA OOM at epoch 292: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 409, in <module>
    main()
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/train.py", line 403, in main
    result = trainer_synapse(args, model, args.output_dir, train_dataset, val_dataset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 1227, in trainer_synapse
    train_losses = run_training_epoch(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/trainer.py", line 686, in run_training_epoch
    predictions = model(images)
                  ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/vision_transformer_cnn.py", line 118, in forward
    return self.model(x)
           ^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1845, in forward
    x = self.smart_skips[inx](skip_features, x)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/cnn_transformer.py", line 1095, in forward
    skip_enhanced, _ = self.attention(skip_tokens, skip_tokens, skip_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1380, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 6450, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpc/iwi5/iwi5250h/.local/lib/python3.12/site-packages/torch/nn/functional.py", line 1422, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 9.64 GiB of which 682.12 MiB is free. Including non-PyTorch memory, this process has 8.97 GiB memory in use. Of the allocated memory 8.63 GiB is allocated by PyTorch, and 78.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ— TRAINING FAILED: Syr341 (Exit Code: 1)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Skipping testing for Syr341 due to training failure.


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: BASELINE + DEEP SUPERVISION + SMART FEATURE FUSION
Results Location: ./Result/a3/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Wed Nov 19 03:39:11 PM CET 2025
= Job-ID             : 1399816 on tinygpu
= Job-Name           : 3rd
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run3.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 00:16:05
= Total RAM usage    : 4.8 GiB of requested  GiB (%)   
= Node list          : tg083
= Subm/Elig/Start/End: 2025-11-19T14:09:19 / 2025-11-19T14:09:19 / 2025-11-19T15:23:06 / 2025-11-19T15:39:11
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody             0.0K  1000.0G  1500.0G        N/A       1    5,000K   7,500K        N/A    
    /home/hpc              71.2G   104.9G   209.7G        N/A     238K     500K   1,000K        N/A    
    /home/vault             0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:B1:00.0, 818055, 1 %, 0 %, 9864 MiB, 12759 ms
NVIDIA GeForce RTX 3080, 00000000:B1:00.0, 818184, 2 %, 0 %, 9864 MiB, 10919 ms
NVIDIA GeForce RTX 3080, 00000000:B1:00.0, 818297, 1 %, 0 %, 914 MiB, 7830 ms
NVIDIA GeForce RTX 3080, 00000000:B1:00.0, 818375, 17 %, 11 %, 1354 MiB, 209347 ms
NVIDIA GeForce RTX 3080, 00000000:B1:00.0, 818436, 2 %, 0 %, 9864 MiB, 10749 ms
