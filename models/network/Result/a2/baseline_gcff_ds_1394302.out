### Starting TaskPrologue of job 1394302 on tg06b at Wed Nov 19 02:18:41 AM CET 2025
Running on cores 2-3,10-11,16,19,26-27 with governor ondemand
Wed Nov 19 02:18:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:3B:00.0 Off |                  N/A |
| 33%   37C    P8             24W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff (Global Context Feature Fusion from MSAGHNet)
    - Global Context Block (attention pooling + MLP)
    - Channel Attention Module (max/avg pool + MLP)
    - Applied at 3 skip connection stages
  ‚úì Deep Supervision: ENABLED (multi-resolution auxiliary outputs)
    - 3 auxiliary outputs at decoder stages
    - Ground truth downsampled to match resolutions
    - Multi-resolution loss computation (MSAGHNet-style)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)
  ‚úì Loss: CB Loss (Class-Balanced, beta=0.9999) + Focal (Œ≥=2.0) + Dice
  ‚úó SE-MSFE: disabled
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Smart Skip Connections: disabled (using GCFF)
  ‚úó Fourier Feature Fusion: disabled (using GCFF)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
Output Directory: ./Result/a2/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF, SE-MSFE, or MSFA+MCT bottleneck)
   ‚Üí Encoder LR reduced to 0.01x (from 0.05x) for better stability
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up enabled (first 10 epochs)

Encoder     : LR=0.000001, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,991,185
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
   ‚Üí Warm-up: Manual LR control for first 10 epochs
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
  üî• LR Warm-up: 10.0% complete (factor: 0.190x)
Results:
  ‚Ä¢ Train Loss: 0.6322
  ‚Ä¢ Validation Loss: 1.9221
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 1.9221
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.6417
  ‚Ä¢ Validation Loss: 1.2666
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 1.2666
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.1610
  ‚Ä¢ Validation Loss: 0.9926
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.9926
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0516
  ‚Ä¢ Validation Loss: 0.9256
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.9256
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
  üî• LR Warm-up: 50.0% complete (factor: 0.550x)
Results:
  ‚Ä¢ Train Loss: 0.9245
  ‚Ä¢ Validation Loss: 0.8672
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.8672
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.8788
  ‚Ä¢ Validation Loss: 0.8202
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.8202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8763
  ‚Ä¢ Validation Loss: 0.7888
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.7888
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8262
  ‚Ä¢ Validation Loss: 0.7636
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.7636
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7937
  ‚Ä¢ Validation Loss: 0.7396
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.7396
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
  üî• LR Warm-up: 100.0% complete (factor: 1.000x)
Results:
  ‚Ä¢ Train Loss: 0.7410
  ‚Ä¢ Validation Loss: 0.7240
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.7240
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7473
  ‚Ä¢ Validation Loss: 0.7109
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.7109
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7247
  ‚Ä¢ Validation Loss: 0.6972
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6972
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7171
  ‚Ä¢ Validation Loss: 0.6942
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6988
  ‚Ä¢ Validation Loss: 0.6736
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6736
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6895
  ‚Ä¢ Validation Loss: 0.6631
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6631
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6835
  ‚Ä¢ Validation Loss: 0.6600
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6600
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6673
  ‚Ä¢ Validation Loss: 0.6545
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6545
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6526
  ‚Ä¢ Validation Loss: 0.6526
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6526
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6479
  ‚Ä¢ Validation Loss: 0.6385
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6385
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6294
  ‚Ä¢ Validation Loss: 0.6333
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6333
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6236
  ‚Ä¢ Validation Loss: 0.6255
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6255
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6215
  ‚Ä¢ Validation Loss: 0.6234
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6234
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6198
  ‚Ä¢ Validation Loss: 0.6192
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6192
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6171
  ‚Ä¢ Validation Loss: 0.6173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6173
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6025
  ‚Ä¢ Validation Loss: 0.6088
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6088
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6063
  ‚Ä¢ Validation Loss: 0.6014
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.6014
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5690
  ‚Ä¢ Validation Loss: 0.5972
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5972
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5805
  ‚Ä¢ Validation Loss: 0.5945
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5945
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5761
  ‚Ä¢ Validation Loss: 0.5914
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5914
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5744
  ‚Ä¢ Validation Loss: 0.5823
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5823
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5543
  ‚Ä¢ Validation Loss: 0.5795
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5753
  ‚Ä¢ Validation Loss: 0.5771
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5771
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5656
  ‚Ä¢ Validation Loss: 0.5775
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5775, best: 0.5771)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5606
  ‚Ä¢ Validation Loss: 0.5780
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5780, best: 0.5771)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5540
  ‚Ä¢ Validation Loss: 0.5687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5687
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5464
  ‚Ä¢ Validation Loss: 0.5714
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5714, best: 0.5687)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5469
  ‚Ä¢ Validation Loss: 0.5703
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5703, best: 0.5687)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5326
  ‚Ä¢ Validation Loss: 0.5656
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5656
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5479
  ‚Ä¢ Validation Loss: 0.5667
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5667, best: 0.5656)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5293
  ‚Ä¢ Validation Loss: 0.5634
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5634
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5264
  ‚Ä¢ Validation Loss: 0.5608
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5608
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5335
  ‚Ä¢ Validation Loss: 0.5586
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5586
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5299
  ‚Ä¢ Validation Loss: 0.5585
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5585
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5372
  ‚Ä¢ Validation Loss: 0.5588
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5588, best: 0.5585)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5322
  ‚Ä¢ Validation Loss: 0.5581
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5581
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5008
  ‚Ä¢ Validation Loss: 0.5565
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5565
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4861
  ‚Ä¢ Validation Loss: 0.5577
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5577, best: 0.5565)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5130
  ‚Ä¢ Validation Loss: 0.5558
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5053
  ‚Ä¢ Validation Loss: 0.5575
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5575, best: 0.5558)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5353
  ‚Ä¢ Validation Loss: 0.5558
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5558
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5077
  ‚Ä¢ Validation Loss: 0.5542
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5542
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5228
  ‚Ä¢ Validation Loss: 0.5551
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5551, best: 0.5542)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4978
  ‚Ä¢ Validation Loss: 0.5543
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5543, best: 0.5542)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5270
  ‚Ä¢ Validation Loss: 0.5538
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5538
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5260
  ‚Ä¢ Validation Loss: 0.5538
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5538
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5028
  ‚Ä¢ Validation Loss: 0.5531
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5531
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5247
  ‚Ä¢ Validation Loss: 0.5514
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5514
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5268
  ‚Ä¢ Validation Loss: 0.5530
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5530, best: 0.5514)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5130
  ‚Ä¢ Validation Loss: 0.5544
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5544, best: 0.5514)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5282
  ‚Ä¢ Validation Loss: 0.5535
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5535, best: 0.5514)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5260
  ‚Ä¢ Validation Loss: 0.5595
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5595, best: 0.5514)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5243
  ‚Ä¢ Validation Loss: 0.5570
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5570, best: 0.5514)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5210
  ‚Ä¢ Validation Loss: 0.5493
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5493
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5297
  ‚Ä¢ Validation Loss: 0.5509
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5509, best: 0.5493)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5177
  ‚Ä¢ Validation Loss: 0.5531
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5531, best: 0.5493)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4908
  ‚Ä¢ Validation Loss: 0.5467
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5467
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5102
  ‚Ä¢ Validation Loss: 0.5468
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5468, best: 0.5467)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5059
  ‚Ä¢ Validation Loss: 0.5455
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5455
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5016
  ‚Ä¢ Validation Loss: 0.5416
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5416
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5094
  ‚Ä¢ Validation Loss: 0.5412
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5412
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4539
  ‚Ä¢ Validation Loss: 0.5371
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5371
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5008
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5364
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4684
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5364, best: 0.5364)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4994
  ‚Ä¢ Validation Loss: 0.5332
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5332
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4652
  ‚Ä¢ Validation Loss: 0.5392
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5392, best: 0.5332)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4870
  ‚Ä¢ Validation Loss: 0.5346
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5346, best: 0.5332)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4879
  ‚Ä¢ Validation Loss: 0.5277
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4846
  ‚Ä¢ Validation Loss: 0.5310
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5310, best: 0.5277)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4866
  ‚Ä¢ Validation Loss: 0.5314
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5314, best: 0.5277)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4586
  ‚Ä¢ Validation Loss: 0.5320
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5320, best: 0.5277)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4618
  ‚Ä¢ Validation Loss: 0.5241
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5241
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4794
  ‚Ä¢ Validation Loss: 0.5255
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5255, best: 0.5241)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4585
  ‚Ä¢ Validation Loss: 0.5258
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5258, best: 0.5241)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4806
  ‚Ä¢ Validation Loss: 0.5224
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5224
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4353
  ‚Ä¢ Validation Loss: 0.5201
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5201
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4730
  ‚Ä¢ Validation Loss: 0.5220
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5220, best: 0.5201)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4466
  ‚Ä¢ Validation Loss: 0.5198
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5198
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4647
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5161
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4231
  ‚Ä¢ Validation Loss: 0.5188
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5188, best: 0.5161)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4720
  ‚Ä¢ Validation Loss: 0.5221
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5221, best: 0.5161)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4367
  ‚Ä¢ Validation Loss: 0.5150
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5150
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4595
  ‚Ä¢ Validation Loss: 0.5184
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5184, best: 0.5150)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4389
  ‚Ä¢ Validation Loss: 0.5134
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5134
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3888
  ‚Ä¢ Validation Loss: 0.5161
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5161, best: 0.5134)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4213
  ‚Ä¢ Validation Loss: 0.5129
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5129
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4557
  ‚Ä¢ Validation Loss: 0.5135
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5135, best: 0.5129)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4142
  ‚Ä¢ Validation Loss: 0.5127
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5127
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4543
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5152, best: 0.5127)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4561
  ‚Ä¢ Validation Loss: 0.5168
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5168, best: 0.5127)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4138
  ‚Ä¢ Validation Loss: 0.5109
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_100.pth
    ‚úì New best checkpoint saved! Val loss: 0.5109
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4349
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5078
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3718
  ‚Ä¢ Validation Loss: 0.5097
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5097, best: 0.5078)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4179
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5094, best: 0.5078)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4100
  ‚Ä¢ Validation Loss: 0.5091
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5091, best: 0.5078)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3979
  ‚Ä¢ Validation Loss: 0.5130
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5130, best: 0.5078)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4078
  ‚Ä¢ Validation Loss: 0.5148
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5148, best: 0.5078)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3468
  ‚Ä¢ Validation Loss: 0.5043
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5043
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3872
  ‚Ä¢ Validation Loss: 0.5014
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5014
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3595
  ‚Ä¢ Validation Loss: 0.5042
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5042, best: 0.5014)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4297
  ‚Ä¢ Validation Loss: 0.5066
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5066, best: 0.5014)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4187
  ‚Ä¢ Validation Loss: 0.5062
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5062, best: 0.5014)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4258
  ‚Ä¢ Validation Loss: 0.5043
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5043, best: 0.5014)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4194
  ‚Ä¢ Validation Loss: 0.5043
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5043, best: 0.5014)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 114/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4370
  ‚Ä¢ Validation Loss: 0.5040
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5040, best: 0.5014)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4153
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5007
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3970
  ‚Ä¢ Validation Loss: 0.5023
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5023, best: 0.5007)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4114
  ‚Ä¢ Validation Loss: 0.4997
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4997
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3633
  ‚Ä¢ Validation Loss: 0.4992
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4992
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4009
  ‚Ä¢ Validation Loss: 0.5010
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5010, best: 0.4992)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 120/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4385
  ‚Ä¢ Validation Loss: 0.4968
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4968
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3577
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4964
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3586
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5001, best: 0.4964)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3708
  ‚Ä¢ Validation Loss: 0.4979
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4979, best: 0.4964)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3810
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4974, best: 0.4964)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3921
  ‚Ä¢ Validation Loss: 0.4991
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4991, best: 0.4964)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3137
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4969, best: 0.4964)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3823
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4967, best: 0.4964)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3866
  ‚Ä¢ Validation Loss: 0.4962
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4962
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3972
  ‚Ä¢ Validation Loss: 0.4944
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4944
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3402
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4935
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3917
  ‚Ä¢ Validation Loss: 0.4963
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4963, best: 0.4935)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4308
  ‚Ä¢ Validation Loss: 0.4948
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4948, best: 0.4935)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4175
  ‚Ä¢ Validation Loss: 0.4932
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4932
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3210
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4934, best: 0.4932)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3861
  ‚Ä¢ Validation Loss: 0.4950
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4950, best: 0.4932)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3949
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4925
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3349
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4921
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4086
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4922, best: 0.4921)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3913
  ‚Ä¢ Validation Loss: 0.4936
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4936, best: 0.4921)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3904
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4927, best: 0.4921)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3717
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4916
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3817
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4913
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3999
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4933, best: 0.4913)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3973
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4911
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3997
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4914, best: 0.4911)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 146/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4234
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3868
  ‚Ä¢ Validation Loss: 0.4917
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4917, best: 0.4902)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3687
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4918, best: 0.4902)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3537
  ‚Ä¢ Validation Loss: 0.4917
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4917, best: 0.4902)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3568
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4927, best: 0.4902)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2971
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4922, best: 0.4902)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3664
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4903, best: 0.4902)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4007
  ‚Ä¢ Validation Loss: 0.4919
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4919, best: 0.4902)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3277
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4922, best: 0.4902)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3651
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4913, best: 0.4902)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3634
  ‚Ä¢ Validation Loss: 0.4897
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4897
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3513
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4906, best: 0.4897)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3346
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4916, best: 0.4897)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3325
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4909, best: 0.4897)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4108
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4907, best: 0.4897)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3819
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4935, best: 0.4897)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3780
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5036, best: 0.4897)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3867
  ‚Ä¢ Validation Loss: 0.5014
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5014, best: 0.4897)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3184
  ‚Ä¢ Validation Loss: 0.5022
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5022, best: 0.4897)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3617
  ‚Ä¢ Validation Loss: 0.5004
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5004, best: 0.4897)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3843
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4946, best: 0.4897)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4157
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4975, best: 0.4897)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3702
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4921, best: 0.4897)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2852
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4914, best: 0.4897)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3383
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4922, best: 0.4897)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4066
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4913, best: 0.4897)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3150
  ‚Ä¢ Validation Loss: 0.4900
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4900, best: 0.4897)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4097
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4894
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3713
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4890
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3506
  ‚Ä¢ Validation Loss: 0.4896
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4896, best: 0.4890)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3544
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4953, best: 0.4890)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3111
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4903, best: 0.4890)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3943
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4910, best: 0.4890)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2754
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4924, best: 0.4890)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3460
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4855
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3556
  ‚Ä¢ Validation Loss: 0.4817
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4817
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3883
  ‚Ä¢ Validation Loss: 0.4873
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4873, best: 0.4817)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3635
  ‚Ä¢ Validation Loss: 0.4936
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4936, best: 0.4817)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3301
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4807
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3789
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4881, best: 0.4807)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3234
  ‚Ä¢ Validation Loss: 0.4816
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4816, best: 0.4807)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3730
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4814, best: 0.4807)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2274
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4852, best: 0.4807)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1619
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4808, best: 0.4807)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2284
  ‚Ä¢ Validation Loss: 0.4786
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4786
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1754
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4795, best: 0.4786)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0873
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4768
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0709
  ‚Ä¢ Validation Loss: 0.4794
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4794, best: 0.4768)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1011
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4787, best: 0.4768)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0859
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4910, best: 0.4768)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1459
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4812, best: 0.4768)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1780
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4804, best: 0.4768)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1773
  ‚Ä¢ Validation Loss: 0.4865
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4865, best: 0.4768)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1753
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4769, best: 0.4768)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1070
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4800, best: 0.4768)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1533
  ‚Ä¢ Validation Loss: 0.4783
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4783, best: 0.4768)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0723
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4814, best: 0.4768)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1574
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4826, best: 0.4768)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1598
  ‚Ä¢ Validation Loss: 0.4844
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4844, best: 0.4768)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1564
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4746
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1398
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4768, best: 0.4746)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1569
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4870, best: 0.4746)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1219
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4804, best: 0.4746)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1090
  ‚Ä¢ Validation Loss: 0.4796
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4796, best: 0.4746)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1060
  ‚Ä¢ Validation Loss: 0.4818
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4818, best: 0.4746)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1188
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4725
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1234
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4777, best: 0.4725)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2064
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4769, best: 0.4725)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1594
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4799, best: 0.4725)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1198
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4850, best: 0.4725)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0348
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4868, best: 0.4725)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0363
  ‚Ä¢ Validation Loss: 0.4783
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4783, best: 0.4725)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1588
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4764, best: 0.4725)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0867
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4738, best: 0.4725)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0855
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4744, best: 0.4725)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1046
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4757, best: 0.4725)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1161
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4716
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1193
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4824, best: 0.4716)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0347
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4820, best: 0.4716)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1812
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4731, best: 0.4716)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1722
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4740, best: 0.4716)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2110
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4744, best: 0.4716)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1208
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4730, best: 0.4716)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1884
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4769, best: 0.4716)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1591
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4739, best: 0.4716)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1011
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4750, best: 0.4716)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0825
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4754, best: 0.4716)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1548
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4799, best: 0.4716)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1512
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4719, best: 0.4716)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0985
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4787, best: 0.4716)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1178
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4760, best: 0.4716)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0845
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4742, best: 0.4716)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1179
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4723, best: 0.4716)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0895
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4702
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1348
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4688
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1541
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4785, best: 0.4688)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0504
  ‚Ä¢ Validation Loss: 0.4762
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4762, best: 0.4688)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1192
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0497
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4739, best: 0.4684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0730
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4778, best: 0.4684)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0638
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4746, best: 0.4684)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2038
  ‚Ä¢ Validation Loss: 0.4776
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4776, best: 0.4684)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0496
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4749, best: 0.4684)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1465
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4739, best: 0.4684)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0852
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4709, best: 0.4684)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1029
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4683
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0971
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4738, best: 0.4683)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4747, best: 0.4683)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0674
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4723, best: 0.4683)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1498
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4764, best: 0.4683)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1189
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4698, best: 0.4683)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1669
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4705, best: 0.4683)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0785
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1000
  ‚Ä¢ Validation Loss: 0.4671
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4671
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1015
  ‚Ä¢ Validation Loss: 0.4819
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4819, best: 0.4671)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0699
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4757, best: 0.4671)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0494
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4698, best: 0.4671)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0821
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4726, best: 0.4671)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0513
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4787, best: 0.4671)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1387
  ‚Ä¢ Validation Loss: 0.4775
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4775, best: 0.4671)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1201
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4695, best: 0.4671)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1657
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4730, best: 0.4671)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1188
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4669
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1396
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4714, best: 0.4669)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0866
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4736, best: 0.4669)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0988
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4726, best: 0.4669)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1365
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4672, best: 0.4669)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1305
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4679, best: 0.4669)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1008
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4701, best: 0.4669)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1349
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4668
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1179
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4697, best: 0.4668)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1148
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4699, best: 0.4668)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1272
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4682, best: 0.4668)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1538
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4754, best: 0.4668)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1703
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4702, best: 0.4668)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1125
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4714, best: 0.4668)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4742, best: 0.4668)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1017
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4683, best: 0.4668)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1343
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4659
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1182
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4670, best: 0.4659)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0958
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4689, best: 0.4659)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0992
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4717, best: 0.4659)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1133
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4678, best: 0.4659)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0813
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4677, best: 0.4659)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1052
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4701, best: 0.4659)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1298
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4669, best: 0.4659)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1181
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4672, best: 0.4659)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1521
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4683, best: 0.4659)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1671
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4689, best: 0.4659)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1448
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4679, best: 0.4659)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1150
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4718, best: 0.4659)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1482
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4674, best: 0.4659)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2060
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1882
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4682, best: 0.4654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0815
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4672, best: 0.4654)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4654
Total Epochs:   300
Models Saved:   ./Result/a2/Latin2
TensorBoard:    ./Result/a2/Latin2/tensorboard_logs
================================================================================

[03:26:38] Training completed. Best val loss: 0.4654

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9887, Recall=0.9890, F1=0.9888, IoU=0.9779
Paratext            : Precision=0.6117, Recall=0.6556, F1=0.6329, IoU=0.4630
Decoration          : Precision=0.8513, Recall=0.8702, F1=0.8606, IoU=0.7553
Main Text           : Precision=0.7715, Recall=0.8468, F1=0.8074, IoU=0.6770
Title               : Precision=0.8207, Recall=0.6544, F1=0.7281, IoU=0.5725
Chapter Headings    : Precision=0.6991, Recall=0.0927, F1=0.1637, IoU=0.0891

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.7905
Mean Recall:    0.6848
Mean F1-Score:  0.6969
Mean IoU:       0.5891
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
Output Directory: ./Result/a2/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF or MSFA+MCT bottleneck)
   ‚Üí Encoder LR: 0.05x (default)
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up: DISABLED (only for SE-MSFE)

Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,991,185
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.6795
  ‚Ä¢ Validation Loss: 1.1487
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 1.1487
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.1142
  ‚Ä¢ Validation Loss: 0.9119
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.9119
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.9581
  ‚Ä¢ Validation Loss: 0.7828
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7828
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8992
  ‚Ä¢ Validation Loss: 0.7441
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7441
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8554
  ‚Ä¢ Validation Loss: 0.7244
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7244
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8303
  ‚Ä¢ Validation Loss: 0.7053
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7053
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7938
  ‚Ä¢ Validation Loss: 0.6888
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6888
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7702
  ‚Ä¢ Validation Loss: 0.6723
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6723
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7458
  ‚Ä¢ Validation Loss: 0.6605
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7294
  ‚Ä¢ Validation Loss: 0.6496
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6496
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7007
  ‚Ä¢ Validation Loss: 0.6413
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6413
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6982
  ‚Ä¢ Validation Loss: 0.6324
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6324
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6928
  ‚Ä¢ Validation Loss: 0.6314
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6314
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6742
  ‚Ä¢ Validation Loss: 0.6208
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6208
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6598
  ‚Ä¢ Validation Loss: 0.6140
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6140
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6524
  ‚Ä¢ Validation Loss: 0.6115
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6115
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6396
  ‚Ä¢ Validation Loss: 0.6053
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6053
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6270
  ‚Ä¢ Validation Loss: 0.6018
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6018
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6185
  ‚Ä¢ Validation Loss: 0.5937
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5937
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6196
  ‚Ä¢ Validation Loss: 0.5878
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5878
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6000
  ‚Ä¢ Validation Loss: 0.5885
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5885, best: 0.5878)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6087
  ‚Ä¢ Validation Loss: 0.5885
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5885, best: 0.5878)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5920
  ‚Ä¢ Validation Loss: 0.5845
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5845
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5996
  ‚Ä¢ Validation Loss: 0.5797
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5797
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5983
  ‚Ä¢ Validation Loss: 0.5840
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5840, best: 0.5797)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5865
  ‚Ä¢ Validation Loss: 0.5833
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5833, best: 0.5797)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5805
  ‚Ä¢ Validation Loss: 0.5755
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5755
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5776
  ‚Ä¢ Validation Loss: 0.5748
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5748
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5582
  ‚Ä¢ Validation Loss: 0.5762
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5762, best: 0.5748)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5826
  ‚Ä¢ Validation Loss: 0.5753
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5753, best: 0.5748)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5845
  ‚Ä¢ Validation Loss: 0.5715
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5715
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5803
  ‚Ä¢ Validation Loss: 0.5684
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5765
  ‚Ä¢ Validation Loss: 0.5697
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5697, best: 0.5684)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5621
  ‚Ä¢ Validation Loss: 0.5682
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5682
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5668
  ‚Ä¢ Validation Loss: 0.5677
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5677
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5641
  ‚Ä¢ Validation Loss: 0.5645
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5645
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5598
  ‚Ä¢ Validation Loss: 0.5659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5659, best: 0.5645)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5644
  ‚Ä¢ Validation Loss: 0.5649
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5649, best: 0.5645)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5511
  ‚Ä¢ Validation Loss: 0.5659
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5659, best: 0.5645)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5656
  ‚Ä¢ Validation Loss: 0.5646
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5646, best: 0.5645)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5571
  ‚Ä¢ Validation Loss: 0.5653
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5653, best: 0.5645)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5653
  ‚Ä¢ Validation Loss: 0.5629
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5629
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5469
  ‚Ä¢ Validation Loss: 0.5643
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5643, best: 0.5629)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5532
  ‚Ä¢ Validation Loss: 0.5648
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5648, best: 0.5629)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5637
  ‚Ä¢ Validation Loss: 0.5633
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5633, best: 0.5629)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5537
  ‚Ä¢ Validation Loss: 0.5632
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5632, best: 0.5629)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5512
  ‚Ä¢ Validation Loss: 0.5621
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5621
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5539
  ‚Ä¢ Validation Loss: 0.5624
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5624, best: 0.5621)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5297
  ‚Ä¢ Validation Loss: 0.5628
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5628, best: 0.5621)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5484
  ‚Ä¢ Validation Loss: 0.5625
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5625, best: 0.5621)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5086
  ‚Ä¢ Validation Loss: 0.5720
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5720, best: 0.5621)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4909
  ‚Ä¢ Validation Loss: 0.5648
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5648, best: 0.5621)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5559
  ‚Ä¢ Validation Loss: 0.5572
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5572
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5622
  ‚Ä¢ Validation Loss: 0.5585
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5585, best: 0.5572)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5471
  ‚Ä¢ Validation Loss: 0.5562
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5562
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5217
  ‚Ä¢ Validation Loss: 0.5588
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5588, best: 0.5562)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5221
  ‚Ä¢ Validation Loss: 0.5546
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5546
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5106
  ‚Ä¢ Validation Loss: 0.5507
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5507
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5266
  ‚Ä¢ Validation Loss: 0.5426
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5426
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5174
  ‚Ä¢ Validation Loss: 0.5454
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5454, best: 0.5426)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5095
  ‚Ä¢ Validation Loss: 0.5432
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5432, best: 0.5426)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5120
  ‚Ä¢ Validation Loss: 0.5403
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5403
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5115
  ‚Ä¢ Validation Loss: 0.5414
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5414, best: 0.5403)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4780
  ‚Ä¢ Validation Loss: 0.5322
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5322
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5114
  ‚Ä¢ Validation Loss: 0.5539
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5539, best: 0.5322)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5044
  ‚Ä¢ Validation Loss: 0.5460
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5460, best: 0.5322)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 67/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5046
  ‚Ä¢ Validation Loss: 0.5320
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5320
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4649
  ‚Ä¢ Validation Loss: 0.5324
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5324, best: 0.5320)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5008
  ‚Ä¢ Validation Loss: 0.5271
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5271
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4891
  ‚Ä¢ Validation Loss: 0.5296
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5296, best: 0.5271)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4836
  ‚Ä¢ Validation Loss: 0.5313
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5313, best: 0.5271)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4935
  ‚Ä¢ Validation Loss: 0.5305
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5305, best: 0.5271)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4892
  ‚Ä¢ Validation Loss: 0.5226
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5226
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4987
  ‚Ä¢ Validation Loss: 0.5234
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5234, best: 0.5226)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4810
  ‚Ä¢ Validation Loss: 0.5202
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5202
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4751
  ‚Ä¢ Validation Loss: 0.5185
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5185
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4690
  ‚Ä¢ Validation Loss: 0.5195
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5195, best: 0.5185)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4630
  ‚Ä¢ Validation Loss: 0.5197
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5197, best: 0.5185)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4560
  ‚Ä¢ Validation Loss: 0.5133
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5133
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4643
  ‚Ä¢ Validation Loss: 0.5162
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5162, best: 0.5133)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4634
  ‚Ä¢ Validation Loss: 0.5152
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5152, best: 0.5133)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4731
  ‚Ä¢ Validation Loss: 0.5190
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5190, best: 0.5133)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4616
  ‚Ä¢ Validation Loss: 0.5142
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5142, best: 0.5133)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4571
  ‚Ä¢ Validation Loss: 0.5112
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5112
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4551
  ‚Ä¢ Validation Loss: 0.5139
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5139, best: 0.5112)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4576
  ‚Ä¢ Validation Loss: 0.5116
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5116, best: 0.5112)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4309
  ‚Ä¢ Validation Loss: 0.5091
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5091
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4434
  ‚Ä¢ Validation Loss: 0.5107
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5107, best: 0.5091)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3945
  ‚Ä¢ Validation Loss: 0.5080
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5080
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4242
  ‚Ä¢ Validation Loss: 0.5146
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5146, best: 0.5080)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4221
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5094, best: 0.5080)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3931
  ‚Ä¢ Validation Loss: 0.5129
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5129, best: 0.5080)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3894
  ‚Ä¢ Validation Loss: 0.5101
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5101, best: 0.5080)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3743
  ‚Ä¢ Validation Loss: 0.5033
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5033
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3693
  ‚Ä¢ Validation Loss: 0.5059
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5059, best: 0.5033)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4438
  ‚Ä¢ Validation Loss: 0.5019
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5019
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4332
  ‚Ä¢ Validation Loss: 0.5048
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5048, best: 0.5019)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3792
  ‚Ä¢ Validation Loss: 0.5054
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5054, best: 0.5019)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3735
  ‚Ä¢ Validation Loss: 0.5049
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5049, best: 0.5019)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3420
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.5020, best: 0.5019)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3916
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5020, best: 0.5019)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4054
  ‚Ä¢ Validation Loss: 0.5030
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5030, best: 0.5019)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3739
  ‚Ä¢ Validation Loss: 0.5028
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5028, best: 0.5019)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3487
  ‚Ä¢ Validation Loss: 0.5009
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5009
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3640
  ‚Ä¢ Validation Loss: 0.5004
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5004
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3323
  ‚Ä¢ Validation Loss: 0.5023
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5023, best: 0.5004)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3513
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5001
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3673
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5001
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3487
  ‚Ä¢ Validation Loss: 0.5045
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5045, best: 0.5001)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4209
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4996
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3876
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3792
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4996, best: 0.4978)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3227
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3447
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4980, best: 0.4978)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3562
  ‚Ä¢ Validation Loss: 0.5003
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5003, best: 0.4978)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3687
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4956
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3901
  ‚Ä¢ Validation Loss: 0.5001
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5001, best: 0.4956)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3868
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4975, best: 0.4956)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3309
  ‚Ä¢ Validation Loss: 0.4979
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4979, best: 0.4956)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3448
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4980, best: 0.4956)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3990
  ‚Ä¢ Validation Loss: 0.4989
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4989, best: 0.4956)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 122/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4070
  ‚Ä¢ Validation Loss: 0.4993
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4993, best: 0.4956)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3973
  ‚Ä¢ Validation Loss: 0.4984
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4984, best: 0.4956)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3740
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4952
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3803
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4980, best: 0.4952)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3737
  ‚Ä¢ Validation Loss: 0.4961
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4961, best: 0.4952)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3768
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4965, best: 0.4952)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3313
  ‚Ä¢ Validation Loss: 0.4975
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4975, best: 0.4952)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3942
  ‚Ä¢ Validation Loss: 0.4966
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4966, best: 0.4952)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3415
  ‚Ä¢ Validation Loss: 0.4961
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4961, best: 0.4952)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3869
  ‚Ä¢ Validation Loss: 0.4968
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4968, best: 0.4952)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3503
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4965, best: 0.4952)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3635
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4956, best: 0.4952)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3617
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4957, best: 0.4952)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3853
  ‚Ä¢ Validation Loss: 0.4951
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4951
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3513
  ‚Ä¢ Validation Loss: 0.4955
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4955, best: 0.4951)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3252
  ‚Ä¢ Validation Loss: 0.4948
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4948
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3467
  ‚Ä¢ Validation Loss: 0.4937
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4937
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3592
  ‚Ä¢ Validation Loss: 0.4941
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4941, best: 0.4937)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 140/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4112
  ‚Ä¢ Validation Loss: 0.4950
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4950, best: 0.4937)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 141/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4022
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4947, best: 0.4937)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3464
  ‚Ä¢ Validation Loss: 0.4959
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4959, best: 0.4937)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3687
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4954, best: 0.4937)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3540
  ‚Ä¢ Validation Loss: 0.4954
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4954, best: 0.4937)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 145/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4009
  ‚Ä¢ Validation Loss: 0.4953
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4953, best: 0.4937)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3688
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4956, best: 0.4937)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 147/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4061
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4956, best: 0.4937)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3553
  ‚Ä¢ Validation Loss: 0.4959
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4959, best: 0.4937)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3490
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4956, best: 0.4937)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3628
  ‚Ä¢ Validation Loss: 0.4957
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4957, best: 0.4937)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3974
  ‚Ä¢ Validation Loss: 0.5012
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5012, best: 0.4937)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3417
  ‚Ä¢ Validation Loss: 0.4999
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4999, best: 0.4937)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4165
  ‚Ä¢ Validation Loss: 0.5064
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5064, best: 0.4937)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3820
  ‚Ä¢ Validation Loss: 0.4984
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4984, best: 0.4937)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3665
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4934
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3701
  ‚Ä¢ Validation Loss: 0.4936
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4936, best: 0.4934)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3746
  ‚Ä¢ Validation Loss: 0.4955
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4955, best: 0.4934)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3785
  ‚Ä¢ Validation Loss: 0.5015
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5015, best: 0.4934)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3363
  ‚Ä¢ Validation Loss: 0.4932
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4932
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3951
  ‚Ä¢ Validation Loss: 0.4984
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4984, best: 0.4932)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3312
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4967, best: 0.4932)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3169
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4922
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3503
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4980, best: 0.4922)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 164/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4083
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4922, best: 0.4922)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 165/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4189
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4935, best: 0.4922)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3465
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4907
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3272
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4938, best: 0.4907)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3297
  ‚Ä¢ Validation Loss: 0.4861
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4861
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3646
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4889, best: 0.4861)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3846
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4870, best: 0.4861)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3549
  ‚Ä¢ Validation Loss: 0.4853
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4853
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 172/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3971
  ‚Ä¢ Validation Loss: 0.4825
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4825
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3434
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4870, best: 0.4825)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2885
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4829, best: 0.4825)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3292
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4868, best: 0.4825)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3689
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4878, best: 0.4825)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2957
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4832, best: 0.4825)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3088
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4901, best: 0.4825)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2737
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4938, best: 0.4825)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3204
  ‚Ä¢ Validation Loss: 0.4818
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4818
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3557
  ‚Ä¢ Validation Loss: 0.4802
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4802
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3364
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4801
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3566
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4768
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3909
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4850, best: 0.4768)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3390
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4838, best: 0.4768)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3696
  ‚Ä¢ Validation Loss: 0.4843
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4843, best: 0.4768)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2548
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4801, best: 0.4768)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1934
  ‚Ä¢ Validation Loss: 0.4851
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4851, best: 0.4768)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1857
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4805, best: 0.4768)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2347
  ‚Ä¢ Validation Loss: 0.4809
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4809, best: 0.4768)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2510
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4835, best: 0.4768)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2412
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4780, best: 0.4768)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2446
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4907, best: 0.4768)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1913
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4740
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3401
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4789, best: 0.4740)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2825
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4870, best: 0.4740)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2699
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4838, best: 0.4740)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2764
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4860, best: 0.4740)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1974
  ‚Ä¢ Validation Loss: 0.4804
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4804, best: 0.4740)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1781
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4808, best: 0.4740)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2754
  ‚Ä¢ Validation Loss: 0.4794
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4794, best: 0.4740)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1849
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4766, best: 0.4740)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1683
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4721
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1953
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4805, best: 0.4721)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2250
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4832, best: 0.4721)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1939
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1817
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4745, best: 0.4709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2435
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4682
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2066
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4724, best: 0.4682)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2113
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4729, best: 0.4682)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1604
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4801, best: 0.4682)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1874
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4745, best: 0.4682)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1472
  ‚Ä¢ Validation Loss: 0.4786
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4786, best: 0.4682)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2066
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4727, best: 0.4682)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2174
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4669
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2486
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4737, best: 0.4669)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2463
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4688, best: 0.4669)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2194
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4716, best: 0.4669)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2759
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4673, best: 0.4669)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2635
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4811, best: 0.4669)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1665
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4654
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2324
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4722, best: 0.4654)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2665
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4669, best: 0.4654)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2425
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4726, best: 0.4654)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1838
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4718, best: 0.4654)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2784
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4682, best: 0.4654)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2775
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4693, best: 0.4654)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2427
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4672, best: 0.4654)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2568
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4672, best: 0.4654)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2508
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4658, best: 0.4654)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2277
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4677, best: 0.4654)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2675
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4678, best: 0.4654)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2462
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4636
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2584
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4675, best: 0.4636)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2747
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4738, best: 0.4636)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2005
  ‚Ä¢ Validation Loss: 0.4671
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4671, best: 0.4636)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1971
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4696, best: 0.4636)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2226
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4663, best: 0.4636)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2373
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4685, best: 0.4636)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2310
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4638, best: 0.4636)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2050
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4638, best: 0.4636)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1335
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4595
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2149
  ‚Ä¢ Validation Loss: 0.4632
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4632, best: 0.4595)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2254
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4594
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2239
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4672, best: 0.4594)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2179
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4676, best: 0.4594)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2276
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4630, best: 0.4594)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2393
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4586
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1645
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4611, best: 0.4586)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2119
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4589, best: 0.4586)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2875
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4601, best: 0.4586)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2625
  ‚Ä¢ Validation Loss: 0.4578
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4578
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1787
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4598, best: 0.4578)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2031
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4595, best: 0.4578)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1781
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2172
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4611, best: 0.4570)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1999
  ‚Ä¢ Validation Loss: 0.4600
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4600, best: 0.4570)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1957
  ‚Ä¢ Validation Loss: 0.4633
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4633, best: 0.4570)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1918
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4569
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2288
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4639, best: 0.4569)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2033
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4594, best: 0.4569)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1770
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4621, best: 0.4569)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1631
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4590, best: 0.4569)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2528
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4586, best: 0.4569)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2785
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4594, best: 0.4569)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2162
  ‚Ä¢ Validation Loss: 0.4583
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4583, best: 0.4569)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2833
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4570, best: 0.4569)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2917
  ‚Ä¢ Validation Loss: 0.4572
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4572, best: 0.4569)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2278
  ‚Ä¢ Validation Loss: 0.4591
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4591, best: 0.4569)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2487
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4587, best: 0.4569)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2090
  ‚Ä¢ Validation Loss: 0.4578
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4578, best: 0.4569)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1993
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4565
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.4538
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4538
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2712
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4569, best: 0.4538)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2183
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4588, best: 0.4538)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2522
  ‚Ä¢ Validation Loss: 0.4557
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4557, best: 0.4538)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2112
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4569, best: 0.4538)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2366
  ‚Ä¢ Validation Loss: 0.4565
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4565, best: 0.4538)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2049
  ‚Ä¢ Validation Loss: 0.4604
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4604, best: 0.4538)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1909
  ‚Ä¢ Validation Loss: 0.4616
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4616, best: 0.4538)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1625
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4569, best: 0.4538)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2301
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4574, best: 0.4538)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2578
  ‚Ä¢ Validation Loss: 0.4562
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4562, best: 0.4538)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1473
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4596, best: 0.4538)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2313
  ‚Ä¢ Validation Loss: 0.4516
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4516
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2490
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4577, best: 0.4516)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2188
  ‚Ä¢ Validation Loss: 0.4534
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4534, best: 0.4516)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2542
  ‚Ä¢ Validation Loss: 0.4545
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4545, best: 0.4516)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2447
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4568, best: 0.4516)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2743
  ‚Ä¢ Validation Loss: 0.4548
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4548, best: 0.4516)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2183
  ‚Ä¢ Validation Loss: 0.4531
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4531, best: 0.4516)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.4534
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4534, best: 0.4516)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2436
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4574, best: 0.4516)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2275
  ‚Ä¢ Validation Loss: 0.4517
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4517, best: 0.4516)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2719
  ‚Ä¢ Validation Loss: 0.4554
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4554, best: 0.4516)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2108
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4570, best: 0.4516)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2136
  ‚Ä¢ Validation Loss: 0.4559
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4559, best: 0.4516)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2490
  ‚Ä¢ Validation Loss: 0.4530
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4530, best: 0.4516)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2593
  ‚Ä¢ Validation Loss: 0.4519
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4519, best: 0.4516)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2662
  ‚Ä¢ Validation Loss: 0.4529
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4529, best: 0.4516)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4516
Total Epochs:   300
Models Saved:   ./Result/a2/Latin14396
TensorBoard:    ./Result/a2/Latin14396/tensorboard_logs
================================================================================

[04:37:47] Training completed. Best val loss: 0.4516

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9886, Recall=0.9901, F1=0.9893, IoU=0.9789
Paratext            : Precision=0.6274, Recall=0.5579, F1=0.5906, IoU=0.4191
Decoration          : Precision=0.9137, Recall=0.9330, F1=0.9232, IoU=0.8574
Main Text           : Precision=0.8867, Recall=0.8889, F1=0.8878, IoU=0.7983
Title               : Precision=0.8665, Recall=0.7460, F1=0.8018, IoU=0.6691
Chapter Headings    : Precision=0.8402, Recall=0.6473, F1=0.7312, IoU=0.5763

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8538
Mean Recall:    0.7939
Mean F1-Score:  0.8207
Mean IoU:       0.7165
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
Output Directory: ./Result/a2/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF or MSFA+MCT bottleneck)
   ‚Üí Encoder LR: 0.05x (default)
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up: DISABLED (only for SE-MSFE)

Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,991,185
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 3.3640
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 3.3640
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.5413
  ‚Ä¢ Validation Loss: 1.1297
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 1.1297
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.0659
  ‚Ä¢ Validation Loss: 0.8613
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8613
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.0295
  ‚Ä¢ Validation Loss: 0.7985
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7985
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.9428
  ‚Ä¢ Validation Loss: 0.7562
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7562
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.8456
  ‚Ä¢ Validation Loss: 0.7277
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7277
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8580
  ‚Ä¢ Validation Loss: 0.7037
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7037
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.8047
  ‚Ä¢ Validation Loss: 0.6886
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6886
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7915
  ‚Ä¢ Validation Loss: 0.6652
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6652
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7554
  ‚Ä¢ Validation Loss: 0.6531
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6531
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7376
  ‚Ä¢ Validation Loss: 0.6427
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7136
  ‚Ä¢ Validation Loss: 0.6344
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6344
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7001
  ‚Ä¢ Validation Loss: 0.6210
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6210
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6817
  ‚Ä¢ Validation Loss: 0.6101
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6101
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6646
  ‚Ä¢ Validation Loss: 0.6002
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6002
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6511
  ‚Ä¢ Validation Loss: 0.5910
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6283
  ‚Ä¢ Validation Loss: 0.5842
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5842
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6386
  ‚Ä¢ Validation Loss: 0.5835
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5835
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6189
  ‚Ä¢ Validation Loss: 0.5757
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5757
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6070
  ‚Ä¢ Validation Loss: 0.5707
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5707
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5975
  ‚Ä¢ Validation Loss: 0.5732
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5732, best: 0.5707)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6000
  ‚Ä¢ Validation Loss: 0.5638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5638
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5912
  ‚Ä¢ Validation Loss: 0.5597
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5597
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5809
  ‚Ä¢ Validation Loss: 0.5545
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5545
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5516
  ‚Ä¢ Validation Loss: 0.5535
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5535
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5804
  ‚Ä¢ Validation Loss: 0.5500
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5500
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5683
  ‚Ä¢ Validation Loss: 0.5512
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5512, best: 0.5500)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 28/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5649
  ‚Ä¢ Validation Loss: 0.5460
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5460
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5644
  ‚Ä¢ Validation Loss: 0.5463
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5463, best: 0.5460)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5545
  ‚Ä¢ Validation Loss: 0.5425
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5425
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5580
  ‚Ä¢ Validation Loss: 0.5394
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5394
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5525
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5364
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5151
  ‚Ä¢ Validation Loss: 0.5370
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5370, best: 0.5364)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 34/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5509
  ‚Ä¢ Validation Loss: 0.5370
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5370, best: 0.5364)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 35/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5378
  ‚Ä¢ Validation Loss: 0.5365
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5365, best: 0.5364)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 36/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5359
  ‚Ä¢ Validation Loss: 0.5335
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5335
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5370
  ‚Ä¢ Validation Loss: 0.5339
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5339, best: 0.5335)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 38/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5426
  ‚Ä¢ Validation Loss: 0.5338
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5338, best: 0.5335)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 39/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5359
  ‚Ä¢ Validation Loss: 0.5345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5345, best: 0.5335)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 40/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5371
  ‚Ä¢ Validation Loss: 0.5322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5322
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5367
  ‚Ä¢ Validation Loss: 0.5284
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5362
  ‚Ä¢ Validation Loss: 0.5289
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5289, best: 0.5284)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5347
  ‚Ä¢ Validation Loss: 0.5288
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5288, best: 0.5284)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5404
  ‚Ä¢ Validation Loss: 0.5293
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5293, best: 0.5284)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5384
  ‚Ä¢ Validation Loss: 0.5287
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5287, best: 0.5284)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 46/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5243
  ‚Ä¢ Validation Loss: 0.5294
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5294, best: 0.5284)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 47/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5292
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5283
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 48/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5382
  ‚Ä¢ Validation Loss: 0.5297
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5297, best: 0.5283)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 49/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5391
  ‚Ä¢ Validation Loss: 0.5274
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5274
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 50/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5361
  ‚Ä¢ Validation Loss: 0.5311
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5311, best: 0.5274)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 51/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5305
  ‚Ä¢ Validation Loss: 0.5376
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5376, best: 0.5274)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 52/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5347
  ‚Ä¢ Validation Loss: 0.5293
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5293, best: 0.5274)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 53/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5485
  ‚Ä¢ Validation Loss: 0.5353
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5353, best: 0.5274)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 54/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5249
  ‚Ä¢ Validation Loss: 0.5214
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5214
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5196
  ‚Ä¢ Validation Loss: 0.5172
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5172
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 56/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5162
  ‚Ä¢ Validation Loss: 0.5127
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5127
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5124
  ‚Ä¢ Validation Loss: 0.5151
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5151, best: 0.5127)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4975
  ‚Ä¢ Validation Loss: 0.5211
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5211, best: 0.5127)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5101
  ‚Ä¢ Validation Loss: 0.5068
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5068
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.5001
  ‚Ä¢ Validation Loss: 0.5050
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5050
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4885
  ‚Ä¢ Validation Loss: 0.5100
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5100, best: 0.5050)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4866
  ‚Ä¢ Validation Loss: 0.5009
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5009
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4937
  ‚Ä¢ Validation Loss: 0.4969
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4969
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4763
  ‚Ä¢ Validation Loss: 0.4951
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4951
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 65/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4731
  ‚Ä¢ Validation Loss: 0.5019
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5019, best: 0.4951)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 66/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4765
  ‚Ä¢ Validation Loss: 0.4917
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4917
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4532
  ‚Ä¢ Validation Loss: 0.5012
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5012, best: 0.4917)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 68/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4681
  ‚Ä¢ Validation Loss: 0.4884
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4884
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4593
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4903, best: 0.4884)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4595
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4838
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4527
  ‚Ä¢ Validation Loss: 0.4858
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4858, best: 0.4838)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 72/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4559
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4815
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4590
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4808
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4488
  ‚Ä¢ Validation Loss: 0.4845
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4845, best: 0.4808)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 75/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4326
  ‚Ä¢ Validation Loss: 0.4803
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4803
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4450
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4808, best: 0.4803)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 77/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4516
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4747
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4338
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4753, best: 0.4747)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4298
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4771, best: 0.4747)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 80/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4280
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4757, best: 0.4747)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 81/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4322
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4805, best: 0.4747)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 82/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4424
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4687
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4159
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4693, best: 0.4687)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4297
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4715, best: 0.4687)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 85/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4108
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4668
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4112
  ‚Ä¢ Validation Loss: 0.4645
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4645
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4206
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4690, best: 0.4645)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4143
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4652, best: 0.4645)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 89/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4152
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4673, best: 0.4645)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 90/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4157
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4617
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4064
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4608
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3994
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4138
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4623, best: 0.4605)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3902
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4608, best: 0.4605)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 95/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4092
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4602
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.4035
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4589
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3878
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4580
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3799
  ‚Ä¢ Validation Loss: 0.4579
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4579
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3732
  ‚Ä¢ Validation Loss: 0.4567
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4567
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3765
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4586, best: 0.4567)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 101/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3936
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4580, best: 0.4567)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 102/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3904
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4566
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3654
  ‚Ä¢ Validation Loss: 0.4559
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4559
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3860
  ‚Ä¢ Validation Loss: 0.4567
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4567, best: 0.4559)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 105/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3922
  ‚Ä¢ Validation Loss: 0.4552
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4552
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3925
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4569, best: 0.4552)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3782
  ‚Ä¢ Validation Loss: 0.4539
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4539
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3776
  ‚Ä¢ Validation Loss: 0.4549
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4549, best: 0.4539)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3526
  ‚Ä¢ Validation Loss: 0.4553
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4553, best: 0.4539)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 110/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3902
  ‚Ä¢ Validation Loss: 0.4527
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4527
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 111/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3821
  ‚Ä¢ Validation Loss: 0.4514
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4514
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3526
  ‚Ä¢ Validation Loss: 0.4520
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4520, best: 0.4514)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 113/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3833
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4512
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3544
  ‚Ä¢ Validation Loss: 0.4525
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4525, best: 0.4512)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3541
  ‚Ä¢ Validation Loss: 0.4524
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4524, best: 0.4512)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 116/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3860
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4491
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 117/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3816
  ‚Ä¢ Validation Loss: 0.4498
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4498, best: 0.4491)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3710
  ‚Ä¢ Validation Loss: 0.4495
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4495, best: 0.4491)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3683
  ‚Ä¢ Validation Loss: 0.4493
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4493, best: 0.4491)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 120/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3790
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4494, best: 0.4491)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3610
  ‚Ä¢ Validation Loss: 0.4494
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4494, best: 0.4491)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3735
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4491
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3808
  ‚Ä¢ Validation Loss: 0.4490
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4490
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3716
  ‚Ä¢ Validation Loss: 0.4497
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4497, best: 0.4490)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3710
  ‚Ä¢ Validation Loss: 0.4502
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4502, best: 0.4490)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3824
  ‚Ä¢ Validation Loss: 0.4510
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4510, best: 0.4490)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3794
  ‚Ä¢ Validation Loss: 0.4486
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4486
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3421
  ‚Ä¢ Validation Loss: 0.4488
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4488, best: 0.4486)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3852
  ‚Ä¢ Validation Loss: 0.4480
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4480
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3657
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4474
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3447
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4479, best: 0.4474)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3798
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4463
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 133/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3734
  ‚Ä¢ Validation Loss: 0.4470
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4470, best: 0.4463)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 134/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3897
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4469, best: 0.4463)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3630
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4469, best: 0.4463)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 136/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3768
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4461
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3611
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4464, best: 0.4461)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3639
  ‚Ä¢ Validation Loss: 0.4474
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4474, best: 0.4461)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3626
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4478, best: 0.4461)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3616
  ‚Ä¢ Validation Loss: 0.4460
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4460
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3469
  ‚Ä¢ Validation Loss: 0.4468
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4468, best: 0.4460)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3604
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4464, best: 0.4460)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 143/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3884
  ‚Ä¢ Validation Loss: 0.4468
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4468, best: 0.4460)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3633
  ‚Ä¢ Validation Loss: 0.4469
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4469, best: 0.4460)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 145/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3750
  ‚Ä¢ Validation Loss: 0.4461
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4461, best: 0.4460)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 146/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3745
  ‚Ä¢ Validation Loss: 0.4466
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4466, best: 0.4460)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3706
  ‚Ä¢ Validation Loss: 0.4471
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4471, best: 0.4460)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3632
  ‚Ä¢ Validation Loss: 0.4462
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4462, best: 0.4460)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3584
  ‚Ä¢ Validation Loss: 0.4467
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4467, best: 0.4460)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3610
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4479, best: 0.4460)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 151/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3874
  ‚Ä¢ Validation Loss: 0.4491
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4491, best: 0.4460)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 152/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3938
  ‚Ä¢ Validation Loss: 0.4582
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4582, best: 0.4460)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3747
  ‚Ä¢ Validation Loss: 0.4506
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4506, best: 0.4460)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 154/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3875
  ‚Ä¢ Validation Loss: 0.4486
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4486, best: 0.4460)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3696
  ‚Ä¢ Validation Loss: 0.4478
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4478, best: 0.4460)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 156/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3817
  ‚Ä¢ Validation Loss: 0.4458
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4458
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3659
  ‚Ä¢ Validation Loss: 0.4489
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4489, best: 0.4458)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3330
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4575, best: 0.4458)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3732
  ‚Ä¢ Validation Loss: 0.4486
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4486, best: 0.4458)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 160/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3788
  ‚Ä¢ Validation Loss: 0.4479
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4479, best: 0.4458)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3639
  ‚Ä¢ Validation Loss: 0.4512
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4512, best: 0.4458)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3615
  ‚Ä¢ Validation Loss: 0.4463
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4463, best: 0.4458)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3580
  ‚Ä¢ Validation Loss: 0.4447
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4447
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 164/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3742
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4427
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 165/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3750
  ‚Ä¢ Validation Loss: 0.4437
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4437, best: 0.4427)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3439
  ‚Ä¢ Validation Loss: 0.4397
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4397
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3564
  ‚Ä¢ Validation Loss: 0.4454
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4454, best: 0.4397)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 168/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3661
  ‚Ä¢ Validation Loss: 0.4464
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4464, best: 0.4397)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3385
  ‚Ä¢ Validation Loss: 0.4411
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4411, best: 0.4397)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3265
  ‚Ä¢ Validation Loss: 0.4408
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4408, best: 0.4397)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3586
  ‚Ä¢ Validation Loss: 0.4417
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4417, best: 0.4397)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 172/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3693
  ‚Ä¢ Validation Loss: 0.4417
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4417, best: 0.4397)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3470
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4396
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3620
  ‚Ä¢ Validation Loss: 0.4389
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4389
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3467
  ‚Ä¢ Validation Loss: 0.4415
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4415, best: 0.4389)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 176/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.3648
  ‚Ä¢ Validation Loss: 0.4376
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4376
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3411
  ‚Ä¢ Validation Loss: 0.4406
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4406, best: 0.4376)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3452
  ‚Ä¢ Validation Loss: 0.4390
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4390, best: 0.4376)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2375
  ‚Ä¢ Validation Loss: 0.4412
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4412, best: 0.4376)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2790
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4358
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2541
  ‚Ä¢ Validation Loss: 0.4358
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4358
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2324
  ‚Ä¢ Validation Loss: 0.4362
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4362, best: 0.4358)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2708
  ‚Ä¢ Validation Loss: 0.4357
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4357
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2500
  ‚Ä¢ Validation Loss: 0.4427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4427, best: 0.4357)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1650
  ‚Ä¢ Validation Loss: 0.4370
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4370, best: 0.4357)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2815
  ‚Ä¢ Validation Loss: 0.4353
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4353
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2561
  ‚Ä¢ Validation Loss: 0.4338
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4338
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2739
  ‚Ä¢ Validation Loss: 0.4325
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4325
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2615
  ‚Ä¢ Validation Loss: 0.4352
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4352, best: 0.4325)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2714
  ‚Ä¢ Validation Loss: 0.4371
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4371, best: 0.4325)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2144
  ‚Ä¢ Validation Loss: 0.4344
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4344, best: 0.4325)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2562
  ‚Ä¢ Validation Loss: 0.4396
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4396, best: 0.4325)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2277
  ‚Ä¢ Validation Loss: 0.4317
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4317
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2712
  ‚Ä¢ Validation Loss: 0.4363
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4363, best: 0.4317)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2836
  ‚Ä¢ Validation Loss: 0.4316
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4316
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2679
  ‚Ä¢ Validation Loss: 0.4301
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4301
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1924
  ‚Ä¢ Validation Loss: 0.4332
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4332, best: 0.4301)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1791
  ‚Ä¢ Validation Loss: 0.4340
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4340, best: 0.4301)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2702
  ‚Ä¢ Validation Loss: 0.4308
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4308, best: 0.4301)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1925
  ‚Ä¢ Validation Loss: 0.4333
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4333, best: 0.4301)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1694
  ‚Ä¢ Validation Loss: 0.4317
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4317, best: 0.4301)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2506
  ‚Ä¢ Validation Loss: 0.4299
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4299
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2850
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4280
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2791
  ‚Ä¢ Validation Loss: 0.4288
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4288, best: 0.4280)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1907
  ‚Ä¢ Validation Loss: 0.4284
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4284, best: 0.4280)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2049
  ‚Ä¢ Validation Loss: 0.4296
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4296, best: 0.4280)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2387
  ‚Ä¢ Validation Loss: 0.4272
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4272
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2228
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4274, best: 0.4272)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2235
  ‚Ä¢ Validation Loss: 0.4276
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4276, best: 0.4272)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2176
  ‚Ä¢ Validation Loss: 0.4300
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4300, best: 0.4272)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2939
  ‚Ä¢ Validation Loss: 0.4262
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4262
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2639
  ‚Ä¢ Validation Loss: 0.4270
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4270, best: 0.4262)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2063
  ‚Ä¢ Validation Loss: 0.4259
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4259
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2185
  ‚Ä¢ Validation Loss: 0.4285
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4285, best: 0.4259)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2301
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4266, best: 0.4259)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2951
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4266, best: 0.4259)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2178
  ‚Ä¢ Validation Loss: 0.4275
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4275, best: 0.4259)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2895
  ‚Ä¢ Validation Loss: 0.4266
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4266, best: 0.4259)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2464
  ‚Ä¢ Validation Loss: 0.4240
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4240
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2407
  ‚Ä¢ Validation Loss: 0.4261
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4261, best: 0.4240)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2518
  ‚Ä¢ Validation Loss: 0.4280
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4280, best: 0.4240)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2691
  ‚Ä¢ Validation Loss: 0.4246
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4246, best: 0.4240)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2169
  ‚Ä¢ Validation Loss: 0.4242
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4242, best: 0.4240)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2703
  ‚Ä¢ Validation Loss: 0.4290
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4290, best: 0.4240)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1694
  ‚Ä¢ Validation Loss: 0.4228
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4228
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2837
  ‚Ä¢ Validation Loss: 0.4238
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4238, best: 0.4228)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2669
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4256, best: 0.4228)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1683
  ‚Ä¢ Validation Loss: 0.4303
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4303, best: 0.4228)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1429
  ‚Ä¢ Validation Loss: 0.4258
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4258, best: 0.4228)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2588
  ‚Ä¢ Validation Loss: 0.4239
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4239, best: 0.4228)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1810
  ‚Ä¢ Validation Loss: 0.4236
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4236, best: 0.4228)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2618
  ‚Ä¢ Validation Loss: 0.4232
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4232, best: 0.4228)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2497
  ‚Ä¢ Validation Loss: 0.4243
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4243, best: 0.4228)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2964
  ‚Ä¢ Validation Loss: 0.4274
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4274, best: 0.4228)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1862
  ‚Ä¢ Validation Loss: 0.4256
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4256, best: 0.4228)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2378
  ‚Ä¢ Validation Loss: 0.4268
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4268, best: 0.4228)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1981
  ‚Ä¢ Validation Loss: 0.4212
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4212
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2078
  ‚Ä¢ Validation Loss: 0.4217
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4217, best: 0.4212)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2061
  ‚Ä¢ Validation Loss: 0.4195
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4195
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2756
  ‚Ä¢ Validation Loss: 0.4216
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4216, best: 0.4195)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2304
  ‚Ä¢ Validation Loss: 0.4203
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4203, best: 0.4195)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2830
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4189
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2377
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4202, best: 0.4189)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2104
  ‚Ä¢ Validation Loss: 0.4225
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4225, best: 0.4189)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2968
  ‚Ä¢ Validation Loss: 0.4221
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4221, best: 0.4189)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2522
  ‚Ä¢ Validation Loss: 0.4198
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4198, best: 0.4189)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2668
  ‚Ä¢ Validation Loss: 0.4208
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4208, best: 0.4189)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3115
  ‚Ä¢ Validation Loss: 0.4209
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4209, best: 0.4189)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2813
  ‚Ä¢ Validation Loss: 0.4180
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4180
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2451
  ‚Ä¢ Validation Loss: 0.4198
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4198, best: 0.4180)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2492
  ‚Ä¢ Validation Loss: 0.4202
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4202, best: 0.4180)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2864
  ‚Ä¢ Validation Loss: 0.4207
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4207, best: 0.4180)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2062
  ‚Ä¢ Validation Loss: 0.4179
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4179
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2221
  ‚Ä¢ Validation Loss: 0.4185
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4185, best: 0.4179)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2209
  ‚Ä¢ Validation Loss: 0.4190
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4190, best: 0.4179)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2333
  ‚Ä¢ Validation Loss: 0.4182
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4182, best: 0.4179)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2437
  ‚Ä¢ Validation Loss: 0.4189
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4189, best: 0.4179)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2710
  ‚Ä¢ Validation Loss: 0.4206
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4206, best: 0.4179)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2526
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4177
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2486
  ‚Ä¢ Validation Loss: 0.4175
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4175
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2240
  ‚Ä¢ Validation Loss: 0.4171
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4171
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2453
  ‚Ä¢ Validation Loss: 0.4167
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4167
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2881
  ‚Ä¢ Validation Loss: 0.4176
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4176, best: 0.4167)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2595
  ‚Ä¢ Validation Loss: 0.4164
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4164
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2568
  ‚Ä¢ Validation Loss: 0.4165
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4165, best: 0.4164)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2766
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4173, best: 0.4164)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2607
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4156
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2454
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4166, best: 0.4156)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2318
  ‚Ä¢ Validation Loss: 0.4177
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4177, best: 0.4156)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2428
  ‚Ä¢ Validation Loss: 0.4187
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4187, best: 0.4156)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1999
  ‚Ä¢ Validation Loss: 0.4156
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4156, best: 0.4156)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2482
  ‚Ä¢ Validation Loss: 0.4169
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4169, best: 0.4156)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2196
  ‚Ä¢ Validation Loss: 0.4182
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4182, best: 0.4156)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1391
  ‚Ä¢ Validation Loss: 0.4163
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4163, best: 0.4156)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2331
  ‚Ä¢ Validation Loss: 0.4160
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4160, best: 0.4156)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2488
  ‚Ä¢ Validation Loss: 0.4146
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4146
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2030
  ‚Ä¢ Validation Loss: 0.4164
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4164, best: 0.4146)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2546
  ‚Ä¢ Validation Loss: 0.4174
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4174, best: 0.4146)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2453
  ‚Ä¢ Validation Loss: 0.4166
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4166, best: 0.4146)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2801
  ‚Ä¢ Validation Loss: 0.4164
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4164, best: 0.4146)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2073
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4149, best: 0.4146)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2909
  ‚Ä¢ Validation Loss: 0.4161
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4161, best: 0.4146)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2553
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4149, best: 0.4146)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2934
  ‚Ä¢ Validation Loss: 0.4183
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4183, best: 0.4146)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2592
  ‚Ä¢ Validation Loss: 0.4139
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4139
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2667
  ‚Ä¢ Validation Loss: 0.4152
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4152, best: 0.4139)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2271
  ‚Ä¢ Validation Loss: 0.4151
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4151, best: 0.4139)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2785
  ‚Ä¢ Validation Loss: 0.4142
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4142, best: 0.4139)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2785
  ‚Ä¢ Validation Loss: 0.4159
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4159, best: 0.4139)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2654
  ‚Ä¢ Validation Loss: 0.4136
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4136
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2439
  ‚Ä¢ Validation Loss: 0.4145
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4145, best: 0.4136)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2412
  ‚Ä¢ Validation Loss: 0.4147
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4147, best: 0.4136)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2457
  ‚Ä¢ Validation Loss: 0.4173
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4173, best: 0.4136)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2420
  ‚Ä¢ Validation Loss: 0.4135
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4135
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2258
  ‚Ä¢ Validation Loss: 0.4149
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4149, best: 0.4135)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1209
  ‚Ä¢ Validation Loss: 0.4158
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4158, best: 0.4135)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0913
  ‚Ä¢ Validation Loss: 0.4137
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4137, best: 0.4135)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1794
  ‚Ä¢ Validation Loss: 0.4137
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4137, best: 0.4135)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1566
  ‚Ä¢ Validation Loss: 0.4150
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4150, best: 0.4135)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1319
  ‚Ä¢ Validation Loss: 0.4145
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4145, best: 0.4135)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4135
Total Epochs:   300
Models Saved:   ./Result/a2/Latin16746
TensorBoard:    ./Result/a2/Latin16746/tensorboard_logs
================================================================================

[05:54:59] Training completed. Best val loss: 0.4135

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9871, Recall=0.9905, F1=0.9888, IoU=0.9778
Paratext            : Precision=0.7622, Recall=0.8027, F1=0.7819, IoU=0.6419
Decoration          : Precision=0.9710, Recall=0.8927, F1=0.9302, IoU=0.8695
Main Text           : Precision=0.9009, Recall=0.9155, F1=0.9081, IoU=0.8317
Title               : Precision=0.7037, Recall=0.7749, F1=0.7376, IoU=0.5842
Chapter Headings    : Precision=0.9166, Recall=0.7051, F1=0.7971, IoU=0.6626

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.8736
Mean Recall:    0.8469
Mean F1-Score:  0.8573
Mean IoU:       0.7613
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
Output Directory: ./Result/a2/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 24
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + DS + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úì
  ‚Ä¢ Fusion Method: GCFF
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 24
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 24
   - Steps per epoch: 23


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
‚ö†Ô∏è  Advanced components detected (GCFF or MSFA+MCT bottleneck)
   ‚Üí Encoder LR: 0.05x (default)
   ‚Üí Gradient clipping enabled (max_norm=1.0) to reduce skipped batches
   ‚Üí Learning rate warm-up: DISABLED (only for SE-MSFE)

Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,985,069
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice (with Deep Supervision)
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 3.3443
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 3.3443
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 3.4900
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 3.4900, best: 3.3443)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 3.4905
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 3.4905, best: 3.3443)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 4/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1540
  ‚Ä¢ Validation Loss: 2.2619
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 2.2619
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.0569
  ‚Ä¢ Validation Loss: 1.0452
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 1.0452
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 1.1170
  ‚Ä¢ Validation Loss: 0.8321
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.8321
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.9036
  ‚Ä¢ Validation Loss: 0.7714
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7714
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.8686
  ‚Ä¢ Validation Loss: 0.7385
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7385
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7728
  ‚Ä¢ Validation Loss: 0.7207
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7207
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7292
  ‚Ä¢ Validation Loss: 0.6979
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6979
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6968
  ‚Ä¢ Validation Loss: 0.6858
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6858
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6419
  ‚Ä¢ Validation Loss: 0.6765
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6765
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6830
  ‚Ä¢ Validation Loss: 0.6577
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6577
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6263
  ‚Ä¢ Validation Loss: 0.6566
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6566
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5354
  ‚Ä¢ Validation Loss: 0.6486
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6486
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6015
  ‚Ä¢ Validation Loss: 0.6373
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6373
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4613
  ‚Ä¢ Validation Loss: 0.6363
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.6363
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5366
  ‚Ä¢ Validation Loss: 0.6420
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.6420, best: 0.6363)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5330
  ‚Ä¢ Validation Loss: 0.6273
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.6273
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5033
  ‚Ä¢ Validation Loss: 0.6256
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.6256
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4541
  ‚Ä¢ Validation Loss: 0.6209
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.6209
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5242
  ‚Ä¢ Validation Loss: 0.6172
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.6172
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4364
  ‚Ä¢ Validation Loss: 0.6225
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.6225, best: 0.6172)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4033
  ‚Ä¢ Validation Loss: 0.6082
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.6082
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4462
  ‚Ä¢ Validation Loss: 0.6121
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.6121, best: 0.6082)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3861
  ‚Ä¢ Validation Loss: 0.6060
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.6060
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3913
  ‚Ä¢ Validation Loss: 0.6034
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.6034
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4639
  ‚Ä¢ Validation Loss: 0.6118
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.6118, best: 0.6034)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4301
  ‚Ä¢ Validation Loss: 0.6027
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.6027
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4659
  ‚Ä¢ Validation Loss: 0.6027
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.6027
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4100
  ‚Ä¢ Validation Loss: 0.5986
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5986
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4039
  ‚Ä¢ Validation Loss: 0.6024
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.6024, best: 0.5986)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5154
  ‚Ä¢ Validation Loss: 0.6014
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.6014, best: 0.5986)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4142
  ‚Ä¢ Validation Loss: 0.6004
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.6004, best: 0.5986)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5146
  ‚Ä¢ Validation Loss: 0.5965
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5965
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5322
  ‚Ä¢ Validation Loss: 0.5957
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5957
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3647
  ‚Ä¢ Validation Loss: 0.5925
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5925
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3733
  ‚Ä¢ Validation Loss: 0.5976
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5976, best: 0.5925)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4070
  ‚Ä¢ Validation Loss: 0.5929
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5929, best: 0.5925)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3444
  ‚Ä¢ Validation Loss: 0.5917
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5917
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5269
  ‚Ä¢ Validation Loss: 0.5904
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5904
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3420
  ‚Ä¢ Validation Loss: 0.5923
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5923, best: 0.5904)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4041
  ‚Ä¢ Validation Loss: 0.5903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5903
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4974
  ‚Ä¢ Validation Loss: 0.5913
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5913, best: 0.5903)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3697
  ‚Ä¢ Validation Loss: 0.5920
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5920, best: 0.5903)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4201
  ‚Ä¢ Validation Loss: 0.5893
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5893
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4361
  ‚Ä¢ Validation Loss: 0.5933
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5933, best: 0.5893)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3998
  ‚Ä¢ Validation Loss: 0.5912
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5912, best: 0.5893)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4576
  ‚Ä¢ Validation Loss: 0.5930
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5930, best: 0.5893)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3437
  ‚Ä¢ Validation Loss: 0.5926
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5926, best: 0.5893)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3519
  ‚Ä¢ Validation Loss: 0.5964
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5964, best: 0.5893)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3799
  ‚Ä¢ Validation Loss: 0.5844
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5844
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4740
  ‚Ä¢ Validation Loss: 0.5897
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5897, best: 0.5844)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3850
  ‚Ä¢ Validation Loss: 0.5828
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5828
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4168
  ‚Ä¢ Validation Loss: 0.5857
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5857, best: 0.5828)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4197
  ‚Ä¢ Validation Loss: 0.5846
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5846, best: 0.5828)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3912
  ‚Ä¢ Validation Loss: 0.5780
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5780
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3117
  ‚Ä¢ Validation Loss: 0.5775
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5775
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3870
  ‚Ä¢ Validation Loss: 0.5781
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5781, best: 0.5775)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3432
  ‚Ä¢ Validation Loss: 0.5756
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5756
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2984
  ‚Ä¢ Validation Loss: 0.5819
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5819, best: 0.5756)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3621
  ‚Ä¢ Validation Loss: 0.5700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5700
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3659
  ‚Ä¢ Validation Loss: 0.5724
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5724, best: 0.5700)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4185
  ‚Ä¢ Validation Loss: 0.5762
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5762, best: 0.5700)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3379
  ‚Ä¢ Validation Loss: 0.5736
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5736, best: 0.5700)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3203
  ‚Ä¢ Validation Loss: 0.5755
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5755, best: 0.5700)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3583
  ‚Ä¢ Validation Loss: 0.5676
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5676
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3300
  ‚Ä¢ Validation Loss: 0.5664
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5664
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4414
  ‚Ä¢ Validation Loss: 0.5680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5680, best: 0.5664)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3111
  ‚Ä¢ Validation Loss: 0.5600
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5600
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3559
  ‚Ä¢ Validation Loss: 0.5688
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5688, best: 0.5600)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3331
  ‚Ä¢ Validation Loss: 0.5607
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5607, best: 0.5600)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3912
  ‚Ä¢ Validation Loss: 0.5554
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5554
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2546
  ‚Ä¢ Validation Loss: 0.5551
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5551
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4278
  ‚Ä¢ Validation Loss: 0.5584
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5584, best: 0.5551)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3675
  ‚Ä¢ Validation Loss: 0.5585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5585, best: 0.5551)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3230
  ‚Ä¢ Validation Loss: 0.5554
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5554, best: 0.5551)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3196
  ‚Ä¢ Validation Loss: 0.5588
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5588, best: 0.5551)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3159
  ‚Ä¢ Validation Loss: 0.5601
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5601, best: 0.5551)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3288
  ‚Ä¢ Validation Loss: 0.5561
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5561, best: 0.5551)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3519
  ‚Ä¢ Validation Loss: 0.5480
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5480
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3801
  ‚Ä¢ Validation Loss: 0.5526
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5526, best: 0.5480)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3542
  ‚Ä¢ Validation Loss: 0.5504
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5504, best: 0.5480)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3598
  ‚Ä¢ Validation Loss: 0.5589
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5589, best: 0.5480)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3951
  ‚Ä¢ Validation Loss: 0.5471
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5471
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4230
  ‚Ä¢ Validation Loss: 0.5448
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5448
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4217
  ‚Ä¢ Validation Loss: 0.5506
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5506, best: 0.5448)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2815
  ‚Ä¢ Validation Loss: 0.5493
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5493, best: 0.5448)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3297
  ‚Ä¢ Validation Loss: 0.5402
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5402
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3326
  ‚Ä¢ Validation Loss: 0.5490
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5490, best: 0.5402)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3409
  ‚Ä¢ Validation Loss: 0.5496
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5496, best: 0.5402)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4314
  ‚Ä¢ Validation Loss: 0.5444
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5444, best: 0.5402)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3384
  ‚Ä¢ Validation Loss: 0.5420
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5420, best: 0.5402)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4257
  ‚Ä¢ Validation Loss: 0.5454
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5454, best: 0.5402)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3854
  ‚Ä¢ Validation Loss: 0.5402
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5402
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3972
  ‚Ä¢ Validation Loss: 0.5386
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5386
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3102
  ‚Ä¢ Validation Loss: 0.5489
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5489, best: 0.5386)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2661
  ‚Ä¢ Validation Loss: 0.5384
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5384
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3269
  ‚Ä¢ Validation Loss: 0.5454
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5454, best: 0.5384)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3002
  ‚Ä¢ Validation Loss: 0.5442
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.5442, best: 0.5384)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3627
  ‚Ä¢ Validation Loss: 0.5394
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5394, best: 0.5384)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2540
  ‚Ä¢ Validation Loss: 0.5387
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5387, best: 0.5384)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4552
  ‚Ä¢ Validation Loss: 0.5418
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5418, best: 0.5384)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2992
  ‚Ä¢ Validation Loss: 0.5363
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5363
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3246
  ‚Ä¢ Validation Loss: 0.5379
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5379, best: 0.5363)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3703
  ‚Ä¢ Validation Loss: 0.5378
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5378, best: 0.5363)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3387
  ‚Ä¢ Validation Loss: 0.5363
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5363
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3311
  ‚Ä¢ Validation Loss: 0.5333
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5333
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2938
  ‚Ä¢ Validation Loss: 0.5388
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5388, best: 0.5333)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3811
  ‚Ä¢ Validation Loss: 0.5355
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5355, best: 0.5333)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3610
  ‚Ä¢ Validation Loss: 0.5327
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5327
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3485
  ‚Ä¢ Validation Loss: 0.5345
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5345, best: 0.5327)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1963
  ‚Ä¢ Validation Loss: 0.5335
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5335, best: 0.5327)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3593
  ‚Ä¢ Validation Loss: 0.5358
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5358, best: 0.5327)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3277
  ‚Ä¢ Validation Loss: 0.5359
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5359, best: 0.5327)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2681
  ‚Ä¢ Validation Loss: 0.5345
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5345, best: 0.5327)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3389
  ‚Ä¢ Validation Loss: 0.5323
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5323
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2934
  ‚Ä¢ Validation Loss: 0.5334
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5334, best: 0.5323)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3375
  ‚Ä¢ Validation Loss: 0.5324
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5324, best: 0.5323)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3090
  ‚Ä¢ Validation Loss: 0.5328
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5328, best: 0.5323)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3590
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5283
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2738
  ‚Ä¢ Validation Loss: 0.5325
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5325, best: 0.5283)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3151
  ‚Ä¢ Validation Loss: 0.5310
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5310, best: 0.5283)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2906
  ‚Ä¢ Validation Loss: 0.5288
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5288, best: 0.5283)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1469
  ‚Ä¢ Validation Loss: 0.5297
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5297, best: 0.5283)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2221
  ‚Ä¢ Validation Loss: 0.5300
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5300, best: 0.5283)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2729
  ‚Ä¢ Validation Loss: 0.5283
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5283
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1784
  ‚Ä¢ Validation Loss: 0.5325
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5325, best: 0.5283)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3373
  ‚Ä¢ Validation Loss: 0.5307
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5307, best: 0.5283)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3145
  ‚Ä¢ Validation Loss: 0.5282
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.5282
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2110
  ‚Ä¢ Validation Loss: 0.5322
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.5322, best: 0.5282)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2801
  ‚Ä¢ Validation Loss: 0.5315
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5315, best: 0.5282)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2540
  ‚Ä¢ Validation Loss: 0.5304
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5304, best: 0.5282)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2310
  ‚Ä¢ Validation Loss: 0.5297
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5297, best: 0.5282)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2718
  ‚Ä¢ Validation Loss: 0.5303
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5303, best: 0.5282)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2539
  ‚Ä¢ Validation Loss: 0.5317
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5317, best: 0.5282)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1749
  ‚Ä¢ Validation Loss: 0.5308
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5308, best: 0.5282)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3328
  ‚Ä¢ Validation Loss: 0.5304
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5304, best: 0.5282)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.5304
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5304, best: 0.5282)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3006
  ‚Ä¢ Validation Loss: 0.5307
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5307, best: 0.5282)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3003
  ‚Ä¢ Validation Loss: 0.5319
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5319, best: 0.5282)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3436
  ‚Ä¢ Validation Loss: 0.5281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5281
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3132
  ‚Ä¢ Validation Loss: 0.5280
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5280
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3097
  ‚Ä¢ Validation Loss: 0.5281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5281, best: 0.5280)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3469
  ‚Ä¢ Validation Loss: 0.5306
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5306, best: 0.5280)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2690
  ‚Ä¢ Validation Loss: 0.5281
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5281, best: 0.5280)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3403
  ‚Ä¢ Validation Loss: 0.5272
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.5272
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1866
  ‚Ä¢ Validation Loss: 0.5299
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5299, best: 0.5272)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3454
  ‚Ä¢ Validation Loss: 0.5272
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.5272, best: 0.5272)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2481
  ‚Ä¢ Validation Loss: 0.5292
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5292, best: 0.5272)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1929
  ‚Ä¢ Validation Loss: 0.5298
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5298, best: 0.5272)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2297
  ‚Ä¢ Validation Loss: 0.5403
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5403, best: 0.5272)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2117
  ‚Ä¢ Validation Loss: 0.5295
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5295, best: 0.5272)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3060
  ‚Ä¢ Validation Loss: 0.5373
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5373, best: 0.5272)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2942
  ‚Ä¢ Validation Loss: 0.5352
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5352, best: 0.5272)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2676
  ‚Ä¢ Validation Loss: 0.5296
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5296, best: 0.5272)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1534
  ‚Ä¢ Validation Loss: 0.5325
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5325, best: 0.5272)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3087
  ‚Ä¢ Validation Loss: 0.5322
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5322, best: 0.5272)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1468
  ‚Ä¢ Validation Loss: 0.5250
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5250
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2807
  ‚Ä¢ Validation Loss: 0.5280
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5280, best: 0.5250)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2376
  ‚Ä¢ Validation Loss: 0.5338
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5338, best: 0.5250)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2751
  ‚Ä¢ Validation Loss: 0.5286
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5286, best: 0.5250)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1680
  ‚Ä¢ Validation Loss: 0.5229
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5229
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1486
  ‚Ä¢ Validation Loss: 0.5329
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5329, best: 0.5229)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2017
  ‚Ä¢ Validation Loss: 0.5427
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5427, best: 0.5229)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2968
  ‚Ä¢ Validation Loss: 0.5271
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5271, best: 0.5229)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3023
  ‚Ä¢ Validation Loss: 0.5295
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5295, best: 0.5229)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3037
  ‚Ä¢ Validation Loss: 0.5222
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5222
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2989
  ‚Ä¢ Validation Loss: 0.5267
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5267, best: 0.5222)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2041
  ‚Ä¢ Validation Loss: 0.5284
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5284, best: 0.5222)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1690
  ‚Ä¢ Validation Loss: 0.5240
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5240, best: 0.5222)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2377
  ‚Ä¢ Validation Loss: 0.5265
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5265, best: 0.5222)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3407
  ‚Ä¢ Validation Loss: 0.5210
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5210
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2258
  ‚Ä¢ Validation Loss: 0.5203
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5203
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2730
  ‚Ä¢ Validation Loss: 0.5227
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5227, best: 0.5203)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2564
  ‚Ä¢ Validation Loss: 0.5141
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5141
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1610
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5182, best: 0.5141)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2876
  ‚Ä¢ Validation Loss: 0.5149
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5149, best: 0.5141)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1608
  ‚Ä¢ Validation Loss: 0.5181
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5181, best: 0.5141)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2747
  ‚Ä¢ Validation Loss: 0.5143
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5143, best: 0.5141)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2296
  ‚Ä¢ Validation Loss: 0.5186
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5186, best: 0.5141)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2319
  ‚Ä¢ Validation Loss: 0.5189
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5189, best: 0.5141)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2523
  ‚Ä¢ Validation Loss: 0.5136
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5136
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3140
  ‚Ä¢ Validation Loss: 0.5151
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5151, best: 0.5136)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1646
  ‚Ä¢ Validation Loss: 0.5232
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5232, best: 0.5136)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2034
  ‚Ä¢ Validation Loss: 0.5119
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5119
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2155
  ‚Ä¢ Validation Loss: 0.5172
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5172, best: 0.5119)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1957
  ‚Ä¢ Validation Loss: 0.5110
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5110
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1976
  ‚Ä¢ Validation Loss: 0.5103
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5103
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2840
  ‚Ä¢ Validation Loss: 0.5144
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5144, best: 0.5103)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1993
  ‚Ä¢ Validation Loss: 0.5106
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5106, best: 0.5103)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2589
  ‚Ä¢ Validation Loss: 0.5085
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5085
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3358
  ‚Ä¢ Validation Loss: 0.5077
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5077
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2718
  ‚Ä¢ Validation Loss: 0.5109
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5109, best: 0.5077)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2439
  ‚Ä¢ Validation Loss: 0.5071
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5071
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2945
  ‚Ä¢ Validation Loss: 0.5054
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5054
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1402
  ‚Ä¢ Validation Loss: 0.5061
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5061, best: 0.5054)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2661
  ‚Ä¢ Validation Loss: 0.5052
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5052
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2069
  ‚Ä¢ Validation Loss: 0.5089
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5089, best: 0.5052)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2493
  ‚Ä¢ Validation Loss: 0.5067
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.5067, best: 0.5052)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1727
  ‚Ä¢ Validation Loss: 0.5049
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5049
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2706
  ‚Ä¢ Validation Loss: 0.5078
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5078, best: 0.5049)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2035
  ‚Ä¢ Validation Loss: 0.5084
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5084, best: 0.5049)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1909
  ‚Ä¢ Validation Loss: 0.5100
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5100, best: 0.5049)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2555
  ‚Ä¢ Validation Loss: 0.5015
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5015
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2586
  ‚Ä¢ Validation Loss: 0.5060
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5060, best: 0.5015)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2414
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5032, best: 0.5015)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2201
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5032, best: 0.5015)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2748
  ‚Ä¢ Validation Loss: 0.5024
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5024, best: 0.5015)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1675
  ‚Ä¢ Validation Loss: 0.5025
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5025, best: 0.5015)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2043
  ‚Ä¢ Validation Loss: 0.4990
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4990
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1975
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4996, best: 0.4990)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2349
  ‚Ä¢ Validation Loss: 0.5030
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5030, best: 0.4990)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2695
  ‚Ä¢ Validation Loss: 0.4992
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4992, best: 0.4990)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2708
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5056, best: 0.4990)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3094
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4988
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2363
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4976
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2556
  ‚Ä¢ Validation Loss: 0.4994
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4994, best: 0.4976)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2207
  ‚Ä¢ Validation Loss: 0.4989
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4989, best: 0.4976)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2362
  ‚Ä¢ Validation Loss: 0.5034
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5034, best: 0.4976)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1962
  ‚Ä¢ Validation Loss: 0.4966
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4966
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2321
  ‚Ä¢ Validation Loss: 0.4987
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4987, best: 0.4966)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2271
  ‚Ä¢ Validation Loss: 0.4942
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4942
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2714
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4964, best: 0.4942)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1577
  ‚Ä¢ Validation Loss: 0.4976
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4976, best: 0.4942)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2155
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4973, best: 0.4942)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2196
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4965, best: 0.4942)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2683
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4996, best: 0.4942)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1823
  ‚Ä¢ Validation Loss: 0.4974
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4974, best: 0.4942)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1985
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4967, best: 0.4942)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2344
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4904
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2693
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4890
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2029
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4904, best: 0.4890)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2429
  ‚Ä¢ Validation Loss: 0.4923
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4923, best: 0.4890)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1966
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4934, best: 0.4890)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1991
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4972, best: 0.4890)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2422
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4945, best: 0.4890)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2169
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4940, best: 0.4890)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.4931
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4931, best: 0.4890)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2245
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4902, best: 0.4890)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2117
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4913, best: 0.4890)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2967
  ‚Ä¢ Validation Loss: 0.4951
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4951, best: 0.4890)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2697
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4922, best: 0.4890)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2088
  ‚Ä¢ Validation Loss: 0.4933
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4933, best: 0.4890)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2563
  ‚Ä¢ Validation Loss: 0.4936
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4936, best: 0.4890)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1952
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4911, best: 0.4890)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2601
  ‚Ä¢ Validation Loss: 0.4926
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4926, best: 0.4890)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2296
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4901, best: 0.4890)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1948
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4872
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1739
  ‚Ä¢ Validation Loss: 0.4877
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4877, best: 0.4872)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2138
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4894, best: 0.4872)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2411
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4934, best: 0.4872)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2428
  ‚Ä¢ Validation Loss: 0.4896
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4896, best: 0.4872)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1277
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4911, best: 0.4872)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1936
  ‚Ä¢ Validation Loss: 0.4920
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4920, best: 0.4872)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2089
  ‚Ä¢ Validation Loss: 0.4924
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4924, best: 0.4872)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1867
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4909, best: 0.4872)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2227
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4934, best: 0.4872)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2567
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4855
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2348
  ‚Ä¢ Validation Loss: 0.4900
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4900, best: 0.4855)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2648
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4868, best: 0.4855)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2254
  ‚Ä¢ Validation Loss: 0.4882
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4882, best: 0.4855)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2234
  ‚Ä¢ Validation Loss: 0.4896
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4896, best: 0.4855)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2093
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4904, best: 0.4855)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2641
  ‚Ä¢ Validation Loss: 0.4875
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4875, best: 0.4855)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2672
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4906, best: 0.4855)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1742
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4876, best: 0.4855)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2055
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4918, best: 0.4855)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3150
  ‚Ä¢ Validation Loss: 0.4863
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4863, best: 0.4855)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2529
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4867, best: 0.4855)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2140
  ‚Ä¢ Validation Loss: 0.4877
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4877, best: 0.4855)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0759
  ‚Ä¢ Validation Loss: 0.4882
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4882, best: 0.4855)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4868, best: 0.4855)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1493
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4870, best: 0.4855)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1527
  ‚Ä¢ Validation Loss: 0.4878
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4878, best: 0.4855)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1123
  ‚Ä¢ Validation Loss: 0.4863
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4863, best: 0.4855)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1520
  ‚Ä¢ Validation Loss: 0.4843
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4843
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1238
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4846, best: 0.4843)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1560
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4852, best: 0.4843)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1998
  ‚Ä¢ Validation Loss: 0.4864
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4864, best: 0.4843)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2170
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4866, best: 0.4843)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1809
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4837
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1142
  ‚Ä¢ Validation Loss: 0.4863
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4863, best: 0.4837)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2001
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4846, best: 0.4837)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0935
  ‚Ä¢ Validation Loss: 0.4854
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4854, best: 0.4837)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1362
  ‚Ä¢ Validation Loss: 0.4845
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4845, best: 0.4837)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1314
  ‚Ä¢ Validation Loss: 0.4844
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4844, best: 0.4837)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1226
  ‚Ä¢ Validation Loss: 0.4842
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4842, best: 0.4837)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1196
  ‚Ä¢ Validation Loss: 0.4849
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4849, best: 0.4837)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0912
  ‚Ä¢ Validation Loss: 0.4873
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4873, best: 0.4837)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1045
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4839, best: 0.4837)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0786
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4839, best: 0.4837)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1192
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4837, best: 0.4837)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1388
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4868, best: 0.4837)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1659
  ‚Ä¢ Validation Loss: 0.4847
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4847, best: 0.4837)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1645
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4856, best: 0.4837)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1653
  ‚Ä¢ Validation Loss: 0.4862
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4862, best: 0.4837)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1843
  ‚Ä¢ Validation Loss: 0.4865
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4865, best: 0.4837)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1331
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4850, best: 0.4837)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1520
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4839, best: 0.4837)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4837
Total Epochs:   300
Models Saved:   ./Result/a2/Syr341
TensorBoard:    ./Result/a2/Syr341/tensorboard_logs
================================================================================

[07:06:42] Training completed. Best val loss: 0.4837

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
    - Type: 2 Swin Transformer blocks
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: gcff
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Enabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
üöÄ Deep Supervision enabled: 3 auxiliary outputs (MSAGHNet-style multi-resolution)
   Aux dims: [384, 192, 96]
   Style: Simple OutConv (single Conv2d), outputs at native resolutions (H/16, H/8, H/4)
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - ‚úÖ Deep Supervision: ENABLED (3 auxiliary outputs)
  - Fusion Method: GCFF
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9652, Recall=0.9797, F1=0.9724, IoU=0.9463
Paratext            : Precision=0.3808, Recall=0.3665, F1=0.3735, IoU=0.2296
Decoration          : Precision=0.9355, Recall=0.6237, F1=0.7484, IoU=0.5980
Main Text           : Precision=0.8525, Recall=0.8237, F1=0.8379, IoU=0.7210
Title               : Precision=0.2228, Recall=0.1335, F1=0.1669, IoU=0.0911

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6714
Mean Recall:    0.5854
Mean F1-Score:  0.6198
Mean IoU:       0.5172
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.7973
Mean Recall:    0.7277
Mean F1-Score:  0.7487
Mean IoU:       0.6460
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL + GCFF + DEEP SUPERVISION
Results Location: ./Result/a2/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Wed Nov 19 07:10:18 AM CET 2025
= Job-ID             : 1394302 on tinygpu
= Job-Name           : baseline_gcff_ds
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 04:51:37
= Total RAM usage    : 7.4 GiB of requested  GiB (%)   
= Node list          : tg06b
= Subm/Elig/Start/End: 2025-11-19T02:18:04 / 2025-11-19T02:18:04 / 2025-11-19T02:18:21 / 2025-11-19T07:09:58
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              75.0G   104.9G   209.7G        N/A     238K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3532302, 60 %, 42 %, 8220 MiB, 4017807 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3543642, 21 %, 10 %, 876 MiB, 157638 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3543687, 60 %, 43 %, 8220 MiB, 4014444 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3555645, 19 %, 9 %, 876 MiB, 174782 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3556170, 56 %, 39 %, 8220 MiB, 4347826 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3575619, 20 %, 9 %, 876 MiB, 170495 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3576138, 58 %, 41 %, 8218 MiB, 4034979 ms
NVIDIA GeForce RTX 2080 Ti, 00000000:3B:00.0, 3593817, 20 %, 10 %, 876 MiB, 165366 ms
