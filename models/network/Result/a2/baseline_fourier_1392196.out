### Starting TaskPrologue of job 1392196 on tg082 at Mon Nov 17 09:36:59 PM CET 2025
Running on cores 14-15,30-31,46-47,62-63 with governor powersave
Mon Nov 17 21:36:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:DB:00.0 Off |                  N/A |
| 30%   33C    P8             21W /  300W |       1MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

============================================================================
CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
============================================================================
Configuration: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION

Component Details:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: 2 Swin Transformer blocks (enabled)
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier (FFT-based feature fusion)
  ‚úì Adapter mode: streaming (integrated)
  ‚úì GroupNorm: enabled
  ‚úì Balanced Sampler: ENABLED (oversamples rare classes)
  ‚úì Class-Aware Augmentation: ENABLED (stronger augmentation for rare classes)
  ‚úì Loss: CB Loss (Class-Balanced, beta=0.9999) + Focal (Œ≥=2.0) + Dice
  ‚úì Fourier Feature Fusion: ENABLED (FFT-based skip connections)
  ‚úó Deep Supervision: disabled
  ‚úó Multi-Scale Aggregation: disabled
  ‚úó Smart Skip Connections: disabled (using fourier fusion)

Training Parameters:
  - Batch Size: 12
  - Max Epochs: 300
  - Learning Rate: 0.0001
  - Scheduler: CosineAnnealingWarmRestarts
  - Early Stopping: 150 epochs patience
============================================================================


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
Output Directory: ./Result/a2/Latin2

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin2
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin2/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin2/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin2
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: FOURIER
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin2
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            92.66%       1.0000
Paratext               0.13%       1.0000
Decoration             2.36%       1.0000
Main Text              3.97%       1.0000
Title                  0.38%       1.0000
Chapter Heading        0.51%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,855,168
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin2/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin2/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 17 batches (0 NaN/Inf loss, 17 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6239
  ‚Ä¢ Validation Loss: 0.6968
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6968
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6847
  ‚Ä¢ Validation Loss: 0.6289
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6289
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6419
  ‚Ä¢ Validation Loss: 0.6122
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6122
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6326
  ‚Ä¢ Validation Loss: 0.5955
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5955
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6198
  ‚Ä¢ Validation Loss: 0.5799
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5799
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 3 batches (0 NaN/Inf loss, 3 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5498
  ‚Ä¢ Validation Loss: 0.5633
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5633
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5186
  ‚Ä¢ Validation Loss: 0.5547
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5547
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5067
  ‚Ä¢ Validation Loss: 0.5471
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5471
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4901
  ‚Ä¢ Validation Loss: 0.5364
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5364
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4274
  ‚Ä¢ Validation Loss: 0.5284
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5284
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4110
  ‚Ä¢ Validation Loss: 0.5278
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5278
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4114
  ‚Ä¢ Validation Loss: 0.5260
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5260
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3089
  ‚Ä¢ Validation Loss: 0.5269
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5269, best: 0.5260)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3587
  ‚Ä¢ Validation Loss: 0.5159
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5159
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3289
  ‚Ä¢ Validation Loss: 0.5166
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5166, best: 0.5159)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2888
  ‚Ä¢ Validation Loss: 0.5120
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5120
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2891
  ‚Ä¢ Validation Loss: 0.5061
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5061
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2417
  ‚Ä¢ Validation Loss: 0.5183
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5183, best: 0.5061)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1979
  ‚Ä¢ Validation Loss: 0.5117
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5117, best: 0.5061)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1584
  ‚Ä¢ Validation Loss: 0.5090
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5090, best: 0.5061)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 19 batches (0 NaN/Inf loss, 19 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2746
  ‚Ä¢ Validation Loss: 0.5041
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5041
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2293
  ‚Ä¢ Validation Loss: 0.4996
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4996
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2192
  ‚Ä¢ Validation Loss: 0.5067
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5067, best: 0.4996)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2011
  ‚Ä¢ Validation Loss: 0.4979
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4979
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2011
  ‚Ä¢ Validation Loss: 0.5059
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5059, best: 0.4979)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1664
  ‚Ä¢ Validation Loss: 0.5002
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5002, best: 0.4979)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1476
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4964
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1560
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4946
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2006
  ‚Ä¢ Validation Loss: 0.4966
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4966, best: 0.4946)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1748
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4972, best: 0.4946)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2128
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4909
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1562
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4911, best: 0.4909)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1682
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4913, best: 0.4909)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1383
  ‚Ä¢ Validation Loss: 0.4907
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4907
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4906
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1175
  ‚Ä¢ Validation Loss: 0.4908
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4908, best: 0.4906)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1449
  ‚Ä¢ Validation Loss: 0.4902
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4902
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1019
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4921, best: 0.4902)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1982
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4889
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2100
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4891, best: 0.4889)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1772
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4890, best: 0.4889)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1601
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4887
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1749
  ‚Ä¢ Validation Loss: 0.4896
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4896, best: 0.4887)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1107
  ‚Ä¢ Validation Loss: 0.4892
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4892, best: 0.4887)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1300
  ‚Ä¢ Validation Loss: 0.4897
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4897, best: 0.4887)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1460
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4886
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1138
  ‚Ä¢ Validation Loss: 0.4905
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4905, best: 0.4886)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1296
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4886, best: 0.4886)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1299
  ‚Ä¢ Validation Loss: 0.4890
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4890, best: 0.4886)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1328
  ‚Ä¢ Validation Loss: 0.4888
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4888, best: 0.4886)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1090
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4946, best: 0.4886)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.5015
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5015, best: 0.4886)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0836
  ‚Ä¢ Validation Loss: 0.4965
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4965, best: 0.4886)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1480
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4901, best: 0.4886)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1056
  ‚Ä¢ Validation Loss: 0.4913
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4913, best: 0.4886)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1991
  ‚Ä¢ Validation Loss: 0.4904
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4904, best: 0.4886)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1894
  ‚Ä¢ Validation Loss: 0.4906
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4906, best: 0.4886)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1518
  ‚Ä¢ Validation Loss: 0.4881
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4881
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1075
  ‚Ä¢ Validation Loss: 0.4875
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4875
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1834
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4883, best: 0.4875)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1521
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4872
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1422
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4860
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1074
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4887, best: 0.4860)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1229
  ‚Ä¢ Validation Loss: 0.4875
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4875, best: 0.4860)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1614
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4837
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1689
  ‚Ä¢ Validation Loss: 0.4868
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4868, best: 0.4837)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2013
  ‚Ä¢ Validation Loss: 0.4819
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4819
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1383
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4829, best: 0.4819)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1389
  ‚Ä¢ Validation Loss: 0.4893
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4893, best: 0.4819)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0623
  ‚Ä¢ Validation Loss: 0.4863
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4863, best: 0.4819)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0881
  ‚Ä¢ Validation Loss: 0.4805
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4805
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1091
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4815, best: 0.4805)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0685
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4815, best: 0.4805)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0798
  ‚Ä¢ Validation Loss: 0.4841
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4841, best: 0.4805)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1254
  ‚Ä¢ Validation Loss: 0.4769
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4769
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1468
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4773, best: 0.4769)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0888
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4777, best: 0.4769)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1145
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4768
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0964
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4755
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1168
  ‚Ä¢ Validation Loss: 0.4762
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4762, best: 0.4755)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0593
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4834, best: 0.4755)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1232
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4747
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0851
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4764, best: 0.4747)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0874
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4743
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0662
  ‚Ä¢ Validation Loss: 0.4760
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4760, best: 0.4743)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1450
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4751, best: 0.4743)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0966
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4744, best: 0.4743)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0376
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4744, best: 0.4743)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1062
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4787, best: 0.4743)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1060
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4739
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0747
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4697
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0946
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4711, best: 0.4697)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4739, best: 0.4697)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0914
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4704, best: 0.4697)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0910
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4738, best: 0.4697)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0853
  ‚Ä¢ Validation Loss: 0.4720
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4720, best: 0.4697)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0842
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4713, best: 0.4697)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0939
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4729, best: 0.4697)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0830
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4727, best: 0.4697)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1324
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4713, best: 0.4697)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0843
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4689
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0559
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4696, best: 0.4689)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0816
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4702, best: 0.4689)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0886
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4719, best: 0.4689)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1118
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4704, best: 0.4689)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0909
  ‚Ä¢ Validation Loss: 0.4734
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4734, best: 0.4689)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0627
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4667
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1159
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4665
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0480
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4694, best: 0.4665)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0749
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4676, best: 0.4665)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0833
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4683, best: 0.4665)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0579
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4673, best: 0.4665)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1024
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4704, best: 0.4665)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1031
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4687, best: 0.4665)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0437
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4686, best: 0.4665)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0359
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4678, best: 0.4665)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0540
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4682, best: 0.4665)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1251
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4674, best: 0.4665)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0265
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4665)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0771
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4696, best: 0.4665)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0483
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4710, best: 0.4665)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0192
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4702, best: 0.4665)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0610
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4662
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0481
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4668, best: 0.4662)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0848
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4679, best: 0.4662)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4690, best: 0.4662)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1125
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4652
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0257
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4662, best: 0.4652)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0579
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4641
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0385
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4646, best: 0.4641)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0628
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4665, best: 0.4641)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0175
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4655, best: 0.4641)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0922
  ‚Ä¢ Validation Loss: 0.4666
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4666, best: 0.4641)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0468
  ‚Ä¢ Validation Loss: 0.4656
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4656, best: 0.4641)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0477
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4652, best: 0.4641)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1113
  ‚Ä¢ Validation Loss: 0.4661
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4661, best: 0.4641)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0472
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4657, best: 0.4641)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0085
  ‚Ä¢ Validation Loss: 0.4648
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4648, best: 0.4641)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4654, best: 0.4641)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4668, best: 0.4641)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0202
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4653, best: 0.4641)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4654
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4654, best: 0.4641)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4670, best: 0.4641)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4662, best: 0.4641)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4657, best: 0.4641)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4651, best: 0.4641)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4649
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4649, best: 0.4641)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4676, best: 0.4641)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4662, best: 0.4641)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0185
  ‚Ä¢ Validation Loss: 0.4664
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4664, best: 0.4641)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4644
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4644, best: 0.4641)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0092
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4657, best: 0.4641)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4700, best: 0.4641)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4686, best: 0.4641)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4690, best: 0.4641)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4697, best: 0.4641)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4683, best: 0.4641)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4641)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4689, best: 0.4641)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4641)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4661
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4661, best: 0.4641)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4678, best: 0.4641)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4641)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4673
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4673, best: 0.4641)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4710, best: 0.4641)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4641)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4682, best: 0.4641)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4641)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4682, best: 0.4641)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0092
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4641)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4683, best: 0.4641)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4698, best: 0.4641)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4696, best: 0.4641)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0086
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4695, best: 0.4641)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4641)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0092
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4685, best: 0.4641)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4699, best: 0.4641)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4715, best: 0.4641)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4700, best: 0.4641)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4711, best: 0.4641)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0193
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4670, best: 0.4641)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0203
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4701, best: 0.4641)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4697, best: 0.4641)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4697, best: 0.4641)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4712, best: 0.4641)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4706, best: 0.4641)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4716, best: 0.4641)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4719, best: 0.4641)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4719, best: 0.4641)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0094
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4730, best: 0.4641)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0080
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4641)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4745, best: 0.4641)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0190
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4741, best: 0.4641)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0087
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4713, best: 0.4641)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4739, best: 0.4641)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4715, best: 0.4641)
    ‚ö† No improvement for 67 epochs (patience: 150, remaining: 83)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0081
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4693, best: 0.4641)
    ‚ö† No improvement for 68 epochs (patience: 150, remaining: 82)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4691, best: 0.4641)
    ‚ö† No improvement for 69 epochs (patience: 150, remaining: 81)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0350
  ‚Ä¢ Validation Loss: 0.4635
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4635
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4636, best: 0.4635)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0177
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4640, best: 0.4635)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4641, best: 0.4635)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0175
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4647, best: 0.4635)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4648
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4648, best: 0.4635)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4668, best: 0.4635)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4662, best: 0.4635)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4651, best: 0.4635)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0103
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4665, best: 0.4635)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0091
  ‚Ä¢ Validation Loss: 0.4661
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4661, best: 0.4635)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4667, best: 0.4635)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4677
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4677, best: 0.4635)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4663
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4663, best: 0.4635)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0075
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4665, best: 0.4635)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0131
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4676, best: 0.4635)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4678
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4678, best: 0.4635)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4688, best: 0.4635)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4676, best: 0.4635)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4704, best: 0.4635)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0091
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4711, best: 0.4635)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0102
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4698, best: 0.4635)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4737, best: 0.4635)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4742, best: 0.4635)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4723, best: 0.4635)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4714, best: 0.4635)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0090
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4694, best: 0.4635)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4704, best: 0.4635)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4684, best: 0.4635)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4703, best: 0.4635)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4691, best: 0.4635)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0186
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4715, best: 0.4635)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0287
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4738, best: 0.4635)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4766, best: 0.4635)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4765, best: 0.4635)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0181
  ‚Ä¢ Validation Loss: 0.4814
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4814, best: 0.4635)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0093
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4820, best: 0.4635)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4816
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4816, best: 0.4635)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4801, best: 0.4635)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0208
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4747, best: 0.4635)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4740, best: 0.4635)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4728, best: 0.4635)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4729, best: 0.4635)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4749, best: 0.4635)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4715, best: 0.4635)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4727, best: 0.4635)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4709, best: 0.4635)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4719, best: 0.4635)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0091
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4735, best: 0.4635)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4635)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4728, best: 0.4635)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4731, best: 0.4635)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4729, best: 0.4635)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4743, best: 0.4635)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0093
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4747, best: 0.4635)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4736, best: 0.4635)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4740, best: 0.4635)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4752, best: 0.4635)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4749, best: 0.4635)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4747, best: 0.4635)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4729, best: 0.4635)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4746, best: 0.4635)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4720
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4720, best: 0.4635)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4726, best: 0.4635)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4740, best: 0.4635)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0186
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4713, best: 0.4635)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0094
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4697, best: 0.4635)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4694, best: 0.4635)
    ‚ö† No improvement for 67 epochs (patience: 150, remaining: 83)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4719, best: 0.4635)
    ‚ö† No improvement for 68 epochs (patience: 150, remaining: 82)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4699, best: 0.4635)
    ‚ö† No improvement for 69 epochs (patience: 150, remaining: 81)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4685, best: 0.4635)
    ‚ö† No improvement for 70 epochs (patience: 150, remaining: 80)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4698, best: 0.4635)
    ‚ö† No improvement for 71 epochs (patience: 150, remaining: 79)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4703, best: 0.4635)
    ‚ö† No improvement for 72 epochs (patience: 150, remaining: 78)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4706, best: 0.4635)
    ‚ö† No improvement for 73 epochs (patience: 150, remaining: 77)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4699
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4699, best: 0.4635)
    ‚ö† No improvement for 74 epochs (patience: 150, remaining: 76)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4691, best: 0.4635)
    ‚ö† No improvement for 75 epochs (patience: 150, remaining: 75)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4679, best: 0.4635)
    ‚ö† No improvement for 76 epochs (patience: 150, remaining: 74)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0092
  ‚Ä¢ Validation Loss: 0.4685
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4685, best: 0.4635)
    ‚ö† No improvement for 77 epochs (patience: 150, remaining: 73)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4688, best: 0.4635)
    ‚ö† No improvement for 78 epochs (patience: 150, remaining: 72)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4705, best: 0.4635)
    ‚ö† No improvement for 79 epochs (patience: 150, remaining: 71)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4691, best: 0.4635)
    ‚ö† No improvement for 80 epochs (patience: 150, remaining: 70)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0189
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4689, best: 0.4635)
    ‚ö† No improvement for 81 epochs (patience: 150, remaining: 69)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4687, best: 0.4635)
    ‚ö† No improvement for 82 epochs (patience: 150, remaining: 68)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4635)
    ‚ö† No improvement for 83 epochs (patience: 150, remaining: 67)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0093
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4687, best: 0.4635)
    ‚ö† No improvement for 84 epochs (patience: 150, remaining: 66)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4696
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4696, best: 0.4635)
    ‚ö† No improvement for 85 epochs (patience: 150, remaining: 65)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4682, best: 0.4635)
    ‚ö† No improvement for 86 epochs (patience: 150, remaining: 64)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4692, best: 0.4635)
    ‚ö† No improvement for 87 epochs (patience: 150, remaining: 63)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4713, best: 0.4635)
    ‚ö† No improvement for 88 epochs (patience: 150, remaining: 62)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4679, best: 0.4635)
    ‚ö† No improvement for 89 epochs (patience: 150, remaining: 61)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4687, best: 0.4635)
    ‚ö† No improvement for 90 epochs (patience: 150, remaining: 60)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4692, best: 0.4635)
    ‚ö† No improvement for 91 epochs (patience: 150, remaining: 59)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0091
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4697, best: 0.4635)
    ‚ö† No improvement for 92 epochs (patience: 150, remaining: 58)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4694, best: 0.4635)
    ‚ö† No improvement for 93 epochs (patience: 150, remaining: 57)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4706, best: 0.4635)
    ‚ö† No improvement for 94 epochs (patience: 150, remaining: 56)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0180
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4710, best: 0.4635)
    ‚ö† No improvement for 95 epochs (patience: 150, remaining: 55)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4701, best: 0.4635)
    ‚ö† No improvement for 96 epochs (patience: 150, remaining: 54)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4692, best: 0.4635)
    ‚ö† No improvement for 97 epochs (patience: 150, remaining: 53)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4747, best: 0.4635)
    ‚ö† No improvement for 98 epochs (patience: 150, remaining: 52)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4708, best: 0.4635)
    ‚ö† No improvement for 99 epochs (patience: 150, remaining: 51)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4701, best: 0.4635)
    ‚ö† No improvement for 100 epochs (patience: 150, remaining: 50)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4707
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4707, best: 0.4635)
    ‚ö† No improvement for 101 epochs (patience: 150, remaining: 49)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4635
Total Epochs:   300
Models Saved:   ./Result/a2/Latin2
TensorBoard:    ./Result/a2/Latin2/tensorboard_logs
================================================================================

[22:48:25] Training completed. Best val loss: 0.4635

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin2
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 076 (54 patches)
‚úì Ground truth found for 076
‚úì Completed: 076
Processing: 079 (54 patches)
‚úì Ground truth found for 079
‚úì Completed: 079
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 095 (54 patches)
‚úì Ground truth found for 095
‚úì Completed: 095
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 111 (54 patches)
‚úì Ground truth found for 111
‚úì Completed: 111
Processing: 115 (54 patches)
‚úì Ground truth found for 115
‚úì Completed: 115
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 128 (54 patches)
‚úì Ground truth found for 128
‚úì Completed: 128
Processing: 134 (54 patches)
‚úì Ground truth found for 134
‚úì Completed: 134
Processing: 138 (54 patches)
‚úì Ground truth found for 138
‚úì Completed: 138
Processing: 142 (54 patches)
‚úì Ground truth found for 142
‚úì Completed: 142
Processing: 159 (54 patches)
‚úì Ground truth found for 159
‚úì Completed: 159
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 185 (54 patches)
‚úì Ground truth found for 185
‚úì Completed: 185
Processing: 200 (54 patches)
‚úì Ground truth found for 200
‚úì Completed: 200
Processing: 203 (54 patches)
‚úì Ground truth found for 203
‚úì Completed: 203
Processing: 208 (54 patches)
‚úì Ground truth found for 208
‚úì Completed: 208
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 230 (54 patches)
‚úì Ground truth found for 230
‚úì Completed: 230
Processing: 235 (54 patches)
‚úì Ground truth found for 235
‚úì Completed: 235
Processing: 236 (54 patches)
‚úì Ground truth found for 236
‚úì Completed: 236
Processing: 248 (54 patches)
‚úì Ground truth found for 248
‚úì Completed: 248
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 250 (54 patches)
‚úì Ground truth found for 250
‚úì Completed: 250
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 275 (54 patches)
‚úì Ground truth found for 275
‚úì Completed: 275
Processing: 277 (54 patches)
‚úì Ground truth found for 277
‚úì Completed: 277
Processing: 297 (54 patches)
‚úì Ground truth found for 297
‚úì Completed: 297

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9820, Recall=0.9878, F1=0.9849, IoU=0.9702
Paratext            : Precision=0.5046, Recall=0.5896, F1=0.5438, IoU=0.3735
Decoration          : Precision=0.7713, Recall=0.8182, F1=0.7941, IoU=0.6585
Main Text           : Precision=0.7387, Recall=0.7386, F1=0.7387, IoU=0.5856
Title               : Precision=0.7197, Recall=0.4708, F1=0.5692, IoU=0.3978
Chapter Headings    : Precision=0.3160, Recall=0.0288, F1=0.0528, IoU=0.0271

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6720
Mean Recall:    0.6056
Mean F1-Score:  0.6139
Mean IoU:       0.5021
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin2
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
Output Directory: ./Result/a2/Latin14396

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin14396
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin14396/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin14396/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin14396
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: FOURIER
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin14396
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            89.45%       0.9839
Paratext               0.09%       1.0807
Decoration             1.70%       0.9839
Main Text              7.59%       0.9839
Title                  0.61%       0.9839
Chapter Heading        0.57%       0.9839
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.10
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [0.9838655  1.0806724  0.9838655  0.9838655  0.98386556 0.9838657 ]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,855,168
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin14396/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin14396/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7382
  ‚Ä¢ Validation Loss: 0.6959
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6959
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7315
  ‚Ä¢ Validation Loss: 0.6055
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6055
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6836
  ‚Ä¢ Validation Loss: 0.6063
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.6063, best: 0.6055)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 4/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.6561
  ‚Ä¢ Validation Loss: 0.5781
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5781
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6266
  ‚Ä¢ Validation Loss: 0.5854
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5854, best: 0.5781)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5455
  ‚Ä¢ Validation Loss: 0.5628
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5628
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5443
  ‚Ä¢ Validation Loss: 0.5588
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5588
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4868
  ‚Ä¢ Validation Loss: 0.5419
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5419
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5090
  ‚Ä¢ Validation Loss: 0.5353
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5353
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4461
  ‚Ä¢ Validation Loss: 0.5295
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5295
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4222
  ‚Ä¢ Validation Loss: 0.5313
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5313, best: 0.5295)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3559
  ‚Ä¢ Validation Loss: 0.5173
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5173
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4658
  ‚Ä¢ Validation Loss: 0.5166
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5166
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3624
  ‚Ä¢ Validation Loss: 0.5182
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5182, best: 0.5166)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3872
  ‚Ä¢ Validation Loss: 0.5159
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5159
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3857
  ‚Ä¢ Validation Loss: 0.5150
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5150
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3746
  ‚Ä¢ Validation Loss: 0.5170
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5170, best: 0.5150)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4598
  ‚Ä¢ Validation Loss: 0.5084
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5084
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3858
  ‚Ä¢ Validation Loss: 0.5096
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5096, best: 0.5084)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2764
  ‚Ä¢ Validation Loss: 0.5157
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5157, best: 0.5084)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3506
  ‚Ä¢ Validation Loss: 0.5008
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5008
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2507
  ‚Ä¢ Validation Loss: 0.5063
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5063, best: 0.5008)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2732
  ‚Ä¢ Validation Loss: 0.5007
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5007
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3432
  ‚Ä¢ Validation Loss: 0.5004
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5004
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3055
  ‚Ä¢ Validation Loss: 0.5106
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5106, best: 0.5004)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3037
  ‚Ä¢ Validation Loss: 0.5003
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.5003
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2317
  ‚Ä¢ Validation Loss: 0.4978
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4978
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2620
  ‚Ä¢ Validation Loss: 0.4931
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4931
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2394
  ‚Ä¢ Validation Loss: 0.5003
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5003, best: 0.4931)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2607
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4964, best: 0.4931)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2228
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4935, best: 0.4931)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2350
  ‚Ä¢ Validation Loss: 0.4968
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4968, best: 0.4931)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2150
  ‚Ä¢ Validation Loss: 0.4973
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4973, best: 0.4931)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1896
  ‚Ä¢ Validation Loss: 0.4960
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4960, best: 0.4931)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1983
  ‚Ä¢ Validation Loss: 0.4992
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4992, best: 0.4931)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2117
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4938, best: 0.4931)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2279
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4940, best: 0.4931)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1418
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4922
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1971
  ‚Ä¢ Validation Loss: 0.4932
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4932, best: 0.4922)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2289
  ‚Ä¢ Validation Loss: 0.4931
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4931, best: 0.4922)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1938
  ‚Ä¢ Validation Loss: 0.4919
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4919
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 20 batches (0 NaN/Inf loss, 20 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2753
  ‚Ä¢ Validation Loss: 0.4932
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4932, best: 0.4919)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1722
  ‚Ä¢ Validation Loss: 0.4929
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4929, best: 0.4919)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1438
  ‚Ä¢ Validation Loss: 0.4935
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4935, best: 0.4919)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1206
  ‚Ä¢ Validation Loss: 0.4920
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4920, best: 0.4919)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1879
  ‚Ä¢ Validation Loss: 0.4934
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4934, best: 0.4919)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2010
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4938, best: 0.4919)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1775
  ‚Ä¢ Validation Loss: 0.4927
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4927, best: 0.4919)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1519
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4925, best: 0.4919)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1654
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4928, best: 0.4919)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1460
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4945, best: 0.4919)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1331
  ‚Ä¢ Validation Loss: 0.5094
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5094, best: 0.4919)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1957
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2333
  ‚Ä¢ Validation Loss: 0.4925
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4925, best: 0.4910)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2169
  ‚Ä¢ Validation Loss: 0.4988
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4988, best: 0.4910)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1716
  ‚Ä¢ Validation Loss: 0.4997
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4997, best: 0.4910)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2117
  ‚Ä¢ Validation Loss: 0.4909
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4909
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1958
  ‚Ä¢ Validation Loss: 0.5036
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5036, best: 0.4909)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2016
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4867
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1846
  ‚Ä¢ Validation Loss: 0.4915
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4915, best: 0.4867)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2081
  ‚Ä¢ Validation Loss: 0.4899
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4899, best: 0.4867)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1073
  ‚Ä¢ Validation Loss: 0.4877
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4877, best: 0.4867)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1621
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4866
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1501
  ‚Ä¢ Validation Loss: 0.4971
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4971, best: 0.4866)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2056
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4921, best: 0.4866)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1851
  ‚Ä¢ Validation Loss: 0.4897
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4897, best: 0.4866)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1829
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4880, best: 0.4866)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1302
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4852
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1074
  ‚Ä¢ Validation Loss: 0.4956
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4956, best: 0.4852)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0641
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4867, best: 0.4852)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0645
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4812
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0543
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4855, best: 0.4812)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1021
  ‚Ä¢ Validation Loss: 0.4825
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4825, best: 0.4812)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0888
  ‚Ä¢ Validation Loss: 0.4921
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4921, best: 0.4812)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1263
  ‚Ä¢ Validation Loss: 0.4843
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4843, best: 0.4812)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1149
  ‚Ä¢ Validation Loss: 0.4843
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4843, best: 0.4812)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0308
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4832, best: 0.4812)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0731
  ‚Ä¢ Validation Loss: 0.4816
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4816, best: 0.4812)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0414
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4876, best: 0.4812)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0591
  ‚Ä¢ Validation Loss: 0.4867
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4867, best: 0.4812)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0315
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4835, best: 0.4812)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0794
  ‚Ä¢ Validation Loss: 0.4848
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4848, best: 0.4812)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0880
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4795
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0950
  ‚Ä¢ Validation Loss: 0.4885
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4885, best: 0.4795)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0855
  ‚Ä¢ Validation Loss: 0.4843
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4843, best: 0.4795)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1115
  ‚Ä¢ Validation Loss: 0.4799
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4799, best: 0.4795)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0607
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4827, best: 0.4795)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0536
  ‚Ä¢ Validation Loss: 0.4850
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4850, best: 0.4795)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0814
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4836, best: 0.4795)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0831
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4832, best: 0.4795)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0406
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4812, best: 0.4795)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0615
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4758
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0681
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4766, best: 0.4758)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0327
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4749
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0531
  ‚Ä¢ Validation Loss: 0.4778
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4778, best: 0.4749)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0507
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4759, best: 0.4749)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0950
  ‚Ä¢ Validation Loss: 0.4747
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4747
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0511
  ‚Ä¢ Validation Loss: 0.4763
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4763, best: 0.4747)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0743
  ‚Ä¢ Validation Loss: 0.4816
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4816, best: 0.4747)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0506
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4782, best: 0.4747)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0534
  ‚Ä¢ Validation Loss: 0.4770
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4770, best: 0.4747)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0195
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4758, best: 0.4747)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0958
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4719
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0507
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4709
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0409
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4709)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0484
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4723, best: 0.4709)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0738
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4709)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0457
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4742, best: 0.4709)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0616
  ‚Ä¢ Validation Loss: 0.4786
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4786, best: 0.4709)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0631
  ‚Ä¢ Validation Loss: 0.4759
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4759, best: 0.4709)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0708
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4736, best: 0.4709)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4741, best: 0.4709)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0523
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4733, best: 0.4709)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0325
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4765, best: 0.4709)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0915
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4740, best: 0.4709)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0759
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4714, best: 0.4709)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0699
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4729, best: 0.4709)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0202
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4705
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0396
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4705)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0391
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4705
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0208
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4693
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0306
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4708, best: 0.4693)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4719, best: 0.4693)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0297
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4743, best: 0.4693)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0423
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4748, best: 0.4693)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0515
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4729, best: 0.4693)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0403
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4703, best: 0.4693)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0309
  ‚Ä¢ Validation Loss: 0.4702
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4702, best: 0.4693)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0855
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4694, best: 0.4693)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0450
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4687
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0320
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4697, best: 0.4687)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0489
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4712, best: 0.4687)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0105
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4711, best: 0.4687)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0618
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0525
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4715, best: 0.4687)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0105
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4735, best: 0.4687)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0215
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4704, best: 0.4687)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4687)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4727, best: 0.4687)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4724, best: 0.4687)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4709, best: 0.4687)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4713, best: 0.4687)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0104
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4727, best: 0.4687)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0105
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4710, best: 0.4687)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4687)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4715, best: 0.4687)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4721, best: 0.4687)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4723, best: 0.4687)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4687)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4687)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4710, best: 0.4687)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4711, best: 0.4687)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4713, best: 0.4687)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4700, best: 0.4687)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0105
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4687)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4726, best: 0.4687)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4746, best: 0.4687)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4733, best: 0.4687)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4687)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4752, best: 0.4687)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4745, best: 0.4687)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4728, best: 0.4687)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4734
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4734, best: 0.4687)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4722, best: 0.4687)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4740, best: 0.4687)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4730, best: 0.4687)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4736, best: 0.4687)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4687)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4740, best: 0.4687)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4745, best: 0.4687)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4748, best: 0.4687)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4723, best: 0.4687)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0107
  ‚Ä¢ Validation Loss: 0.4761
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4761, best: 0.4687)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4754
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4754, best: 0.4687)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4772
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4772, best: 0.4687)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0116
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4789, best: 0.4687)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4777, best: 0.4687)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4789, best: 0.4687)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0088
  ‚Ä¢ Validation Loss: 0.4808
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4808, best: 0.4687)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4807
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4807, best: 0.4687)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4815, best: 0.4687)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4822
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4822, best: 0.4687)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0088
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4835, best: 0.4687)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4855
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4855, best: 0.4687)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4845
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4845, best: 0.4687)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4825
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4825, best: 0.4687)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4835, best: 0.4687)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4826, best: 0.4687)
    ‚ö† No improvement for 67 epochs (patience: 150, remaining: 83)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4846, best: 0.4687)
    ‚ö† No improvement for 68 epochs (patience: 150, remaining: 82)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4839, best: 0.4687)
    ‚ö† No improvement for 69 epochs (patience: 150, remaining: 81)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0108
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4832, best: 0.4687)
    ‚ö† No improvement for 70 epochs (patience: 150, remaining: 80)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4837, best: 0.4687)
    ‚ö† No improvement for 71 epochs (patience: 150, remaining: 79)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4829, best: 0.4687)
    ‚ö† No improvement for 72 epochs (patience: 150, remaining: 78)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4842
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4842, best: 0.4687)
    ‚ö† No improvement for 73 epochs (patience: 150, remaining: 77)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4852, best: 0.4687)
    ‚ö† No improvement for 74 epochs (patience: 150, remaining: 76)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4830
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4830, best: 0.4687)
    ‚ö† No improvement for 75 epochs (patience: 150, remaining: 75)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4832
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4832, best: 0.4687)
    ‚ö† No improvement for 76 epochs (patience: 150, remaining: 74)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0116
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4846, best: 0.4687)
    ‚ö† No improvement for 77 epochs (patience: 150, remaining: 73)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4845
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4845, best: 0.4687)
    ‚ö† No improvement for 78 epochs (patience: 150, remaining: 72)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4834, best: 0.4687)
    ‚ö† No improvement for 79 epochs (patience: 150, remaining: 71)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0116
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4833, best: 0.4687)
    ‚ö† No improvement for 80 epochs (patience: 150, remaining: 70)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4818
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4818, best: 0.4687)
    ‚ö† No improvement for 81 epochs (patience: 150, remaining: 69)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0124
  ‚Ä¢ Validation Loss: 0.4812
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4812, best: 0.4687)
    ‚ö† No improvement for 82 epochs (patience: 150, remaining: 68)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4792
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4792, best: 0.4687)
    ‚ö† No improvement for 83 epochs (patience: 150, remaining: 67)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4768, best: 0.4687)
    ‚ö† No improvement for 84 epochs (patience: 150, remaining: 66)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4777, best: 0.4687)
    ‚ö† No improvement for 85 epochs (patience: 150, remaining: 65)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4766, best: 0.4687)
    ‚ö† No improvement for 86 epochs (patience: 150, remaining: 64)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4786
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4786, best: 0.4687)
    ‚ö† No improvement for 87 epochs (patience: 150, remaining: 63)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4782, best: 0.4687)
    ‚ö† No improvement for 88 epochs (patience: 150, remaining: 62)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4771, best: 0.4687)
    ‚ö† No improvement for 89 epochs (patience: 150, remaining: 61)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4779, best: 0.4687)
    ‚ö† No improvement for 90 epochs (patience: 150, remaining: 60)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4782, best: 0.4687)
    ‚ö† No improvement for 91 epochs (patience: 150, remaining: 59)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4773, best: 0.4687)
    ‚ö† No improvement for 92 epochs (patience: 150, remaining: 58)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4780, best: 0.4687)
    ‚ö† No improvement for 93 epochs (patience: 150, remaining: 57)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4784
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4784, best: 0.4687)
    ‚ö† No improvement for 94 epochs (patience: 150, remaining: 56)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0113
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4748, best: 0.4687)
    ‚ö† No improvement for 95 epochs (patience: 150, remaining: 55)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4758, best: 0.4687)
    ‚ö† No improvement for 96 epochs (patience: 150, remaining: 54)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0112
  ‚Ä¢ Validation Loss: 0.4736
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4736, best: 0.4687)
    ‚ö† No improvement for 97 epochs (patience: 150, remaining: 53)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4730, best: 0.4687)
    ‚ö† No improvement for 98 epochs (patience: 150, remaining: 52)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4728, best: 0.4687)
    ‚ö† No improvement for 99 epochs (patience: 150, remaining: 51)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4740, best: 0.4687)
    ‚ö† No improvement for 100 epochs (patience: 150, remaining: 50)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4738, best: 0.4687)
    ‚ö† No improvement for 101 epochs (patience: 150, remaining: 49)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4748
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4748, best: 0.4687)
    ‚ö† No improvement for 102 epochs (patience: 150, remaining: 48)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4733, best: 0.4687)
    ‚ö† No improvement for 103 epochs (patience: 150, remaining: 47)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4740, best: 0.4687)
    ‚ö† No improvement for 104 epochs (patience: 150, remaining: 46)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4737, best: 0.4687)
    ‚ö† No improvement for 105 epochs (patience: 150, remaining: 45)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4732, best: 0.4687)
    ‚ö† No improvement for 106 epochs (patience: 150, remaining: 44)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4752, best: 0.4687)
    ‚ö† No improvement for 107 epochs (patience: 150, remaining: 43)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4735, best: 0.4687)
    ‚ö† No improvement for 108 epochs (patience: 150, remaining: 42)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4746, best: 0.4687)
    ‚ö† No improvement for 109 epochs (patience: 150, remaining: 41)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4740, best: 0.4687)
    ‚ö† No improvement for 110 epochs (patience: 150, remaining: 40)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4687)
    ‚ö† No improvement for 111 epochs (patience: 150, remaining: 39)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4744, best: 0.4687)
    ‚ö† No improvement for 112 epochs (patience: 150, remaining: 38)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4731, best: 0.4687)
    ‚ö† No improvement for 113 epochs (patience: 150, remaining: 37)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4727, best: 0.4687)
    ‚ö† No improvement for 114 epochs (patience: 150, remaining: 36)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0102
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4704, best: 0.4687)
    ‚ö† No improvement for 115 epochs (patience: 150, remaining: 35)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4718, best: 0.4687)
    ‚ö† No improvement for 116 epochs (patience: 150, remaining: 34)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4719, best: 0.4687)
    ‚ö† No improvement for 117 epochs (patience: 150, remaining: 33)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4728, best: 0.4687)
    ‚ö† No improvement for 118 epochs (patience: 150, remaining: 32)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4732, best: 0.4687)
    ‚ö† No improvement for 119 epochs (patience: 150, remaining: 31)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4726, best: 0.4687)
    ‚ö† No improvement for 120 epochs (patience: 150, remaining: 30)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 121 epochs (patience: 150, remaining: 29)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4735, best: 0.4687)
    ‚ö† No improvement for 122 epochs (patience: 150, remaining: 28)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4704, best: 0.4687)
    ‚ö† No improvement for 123 epochs (patience: 150, remaining: 27)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4712, best: 0.4687)
    ‚ö† No improvement for 124 epochs (patience: 150, remaining: 26)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4731, best: 0.4687)
    ‚ö† No improvement for 125 epochs (patience: 150, remaining: 25)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4716, best: 0.4687)
    ‚ö† No improvement for 126 epochs (patience: 150, remaining: 24)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4718, best: 0.4687)
    ‚ö† No improvement for 127 epochs (patience: 150, remaining: 23)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4687)
    ‚ö† No improvement for 128 epochs (patience: 150, remaining: 22)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4726, best: 0.4687)
    ‚ö† No improvement for 129 epochs (patience: 150, remaining: 21)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4719
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4719, best: 0.4687)
    ‚ö† No improvement for 130 epochs (patience: 150, remaining: 20)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4710, best: 0.4687)
    ‚ö† No improvement for 131 epochs (patience: 150, remaining: 19)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4723, best: 0.4687)
    ‚ö† No improvement for 132 epochs (patience: 150, remaining: 18)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4738, best: 0.4687)
    ‚ö† No improvement for 133 epochs (patience: 150, remaining: 17)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4727
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4727, best: 0.4687)
    ‚ö† No improvement for 134 epochs (patience: 150, remaining: 16)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 135 epochs (patience: 150, remaining: 15)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4713, best: 0.4687)
    ‚ö† No improvement for 136 epochs (patience: 150, remaining: 14)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4718, best: 0.4687)
    ‚ö† No improvement for 137 epochs (patience: 150, remaining: 13)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4722, best: 0.4687)
    ‚ö† No improvement for 138 epochs (patience: 150, remaining: 12)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4717
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4717, best: 0.4687)
    ‚ö† No improvement for 139 epochs (patience: 150, remaining: 11)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4715, best: 0.4687)
    ‚ö† No improvement for 140 epochs (patience: 150, remaining: 10)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4723, best: 0.4687)
    ‚ö† No improvement for 141 epochs (patience: 150, remaining: 9)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4720
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4720, best: 0.4687)
    ‚ö† No improvement for 142 epochs (patience: 150, remaining: 8)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4726, best: 0.4687)
    ‚ö† No improvement for 143 epochs (patience: 150, remaining: 7)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4711, best: 0.4687)
    ‚ö† No improvement for 144 epochs (patience: 150, remaining: 6)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4721, best: 0.4687)
    ‚ö† No improvement for 145 epochs (patience: 150, remaining: 5)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4726, best: 0.4687)
    ‚ö† No improvement for 146 epochs (patience: 150, remaining: 4)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4713, best: 0.4687)
    ‚ö† No improvement for 147 epochs (patience: 150, remaining: 3)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4713, best: 0.4687)
    ‚ö† No improvement for 148 epochs (patience: 150, remaining: 2)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4721, best: 0.4687)
    ‚ö† No improvement for 149 epochs (patience: 150, remaining: 1)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4723, best: 0.4687)
    ‚ö† No improvement for 150 epochs (patience: 150, remaining: 0)

================================================================================
EARLY STOPPING TRIGGERED
================================================================================
No improvement for 150 consecutive epochs
Best validation loss: 0.4687
================================================================================


================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4687
Total Epochs:   280
Models Saved:   ./Result/a2/Latin14396
TensorBoard:    ./Result/a2/Latin14396/tensorboard_logs
================================================================================

[23:58:10] Training completed. Best val loss: 0.4687

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin14396
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 014 (54 patches)
‚úì Ground truth found for 014
‚úì Completed: 014
Processing: 032 (54 patches)
‚úì Ground truth found for 032
‚úì Completed: 032
Processing: 034 (54 patches)
‚úì Ground truth found for 034
‚úì Completed: 034
Processing: 036 (54 patches)
‚úì Ground truth found for 036
‚úì Completed: 036
Processing: 038 (54 patches)
‚úì Ground truth found for 038
‚úì Completed: 038
Processing: 047 (54 patches)
‚úì Ground truth found for 047
‚úì Completed: 047
Processing: 060 (54 patches)
‚úì Ground truth found for 060
‚úì Completed: 060
Processing: 085 (54 patches)
‚úì Ground truth found for 085
‚úì Completed: 085
Processing: 087 (54 patches)
‚úì Ground truth found for 087
‚úì Completed: 087
Processing: 104 (54 patches)
‚úì Ground truth found for 104
‚úì Completed: 104
Processing: 105 (54 patches)
‚úì Ground truth found for 105
‚úì Completed: 105
Processing: 108 (54 patches)
‚úì Ground truth found for 108
‚úì Completed: 108
Processing: 110 (54 patches)
‚úì Ground truth found for 110
‚úì Completed: 110
Processing: 136 (54 patches)
‚úì Ground truth found for 136
‚úì Completed: 136
Processing: 169 (54 patches)
‚úì Ground truth found for 169
‚úì Completed: 169
Processing: 195 (54 patches)
‚úì Ground truth found for 195
‚úì Completed: 195
Processing: 196 (54 patches)
‚úì Ground truth found for 196
‚úì Completed: 196
Processing: 198 (54 patches)
‚úì Ground truth found for 198
‚úì Completed: 198
Processing: 204 (54 patches)
‚úì Ground truth found for 204
‚úì Completed: 204
Processing: 223 (54 patches)
‚úì Ground truth found for 223
‚úì Completed: 223
Processing: 225 (54 patches)
‚úì Ground truth found for 225
‚úì Completed: 225
Processing: 227 (54 patches)
‚úì Ground truth found for 227
‚úì Completed: 227
Processing: 229 (54 patches)
‚úì Ground truth found for 229
‚úì Completed: 229
Processing: 251 (54 patches)
‚úì Ground truth found for 251
‚úì Completed: 251
Processing: 253 (54 patches)
‚úì Ground truth found for 253
‚úì Completed: 253
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 264 (54 patches)
‚úì Ground truth found for 264
‚úì Completed: 264
Processing: 270 (54 patches)
‚úì Ground truth found for 270
‚úì Completed: 270
Processing: 276 (54 patches)
‚úì Ground truth found for 276
‚úì Completed: 276
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9762, Recall=0.9883, F1=0.9822, IoU=0.9651
Paratext            : Precision=0.3249, Recall=0.2253, F1=0.2660, IoU=0.1534
Decoration          : Precision=0.8136, Recall=0.8003, F1=0.8069, IoU=0.6763
Main Text           : Precision=0.8292, Recall=0.7854, F1=0.8067, IoU=0.6760
Title               : Precision=0.6957, Recall=0.4693, F1=0.5605, IoU=0.3894
Chapter Headings    : Precision=0.4195, Recall=0.0584, F1=0.1025, IoU=0.0540

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6765
Mean Recall:    0.5545
Mean F1-Score:  0.5875
Mean IoU:       0.4857
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin14396
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
Output Directory: ./Result/a2/Latin16746

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Using 6 classes for manuscript: Latin16746
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Latin16746/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Latin16746/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 6 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Latin16746
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: FOURIER
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 6
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Latin16746
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            88.42%       1.0000
Paratext               0.34%       1.0000
Decoration             2.52%       1.0000
Main Text              7.49%       1.0000
Title                  0.18%       1.0000
Chapter Heading        1.04%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,855,168
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Latin16746/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Latin16746/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 1.0021
  ‚Ä¢ Validation Loss: 0.6904
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6904
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7094
  ‚Ä¢ Validation Loss: 0.6194
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6194
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.7049
  ‚Ä¢ Validation Loss: 0.6015
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6015
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6406
  ‚Ä¢ Validation Loss: 0.5718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6439
  ‚Ä¢ Validation Loss: 0.5694
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5694
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5815
  ‚Ä¢ Validation Loss: 0.5544
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5544
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5247
  ‚Ä¢ Validation Loss: 0.5357
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5357
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4906
  ‚Ä¢ Validation Loss: 0.5473
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5473, best: 0.5357)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4852
  ‚Ä¢ Validation Loss: 0.5210
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5210
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4299
  ‚Ä¢ Validation Loss: 0.5105
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5105
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3744
  ‚Ä¢ Validation Loss: 0.5066
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5066
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3560
  ‚Ä¢ Validation Loss: 0.5105
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5105, best: 0.5066)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3829
  ‚Ä¢ Validation Loss: 0.5049
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5049
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3200
  ‚Ä¢ Validation Loss: 0.5029
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5029
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3659
  ‚Ä¢ Validation Loss: 0.5032
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5032, best: 0.5029)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 18 batches (0 NaN/Inf loss, 18 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3136
  ‚Ä¢ Validation Loss: 0.5076
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5076, best: 0.5029)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4947
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3439
  ‚Ä¢ Validation Loss: 0.4985
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4985, best: 0.4947)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2624
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4918
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2085
  ‚Ä¢ Validation Loss: 0.4908
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4908
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2018
  ‚Ä¢ Validation Loss: 0.4914
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4914, best: 0.4908)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2706
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4947, best: 0.4908)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2019
  ‚Ä¢ Validation Loss: 0.4952
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4952, best: 0.4908)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2332
  ‚Ä¢ Validation Loss: 0.4893
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4893
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1961
  ‚Ä¢ Validation Loss: 0.4918
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4918, best: 0.4893)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1777
  ‚Ä¢ Validation Loss: 0.4964
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4964, best: 0.4893)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2077
  ‚Ä¢ Validation Loss: 0.4892
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4892
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2077
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4827
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1320
  ‚Ä¢ Validation Loss: 0.4889
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4889, best: 0.4827)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1644
  ‚Ä¢ Validation Loss: 0.4859
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4859, best: 0.4827)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2097
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4833, best: 0.4827)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1499
  ‚Ä¢ Validation Loss: 0.4883
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4883, best: 0.4827)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1824
  ‚Ä¢ Validation Loss: 0.4870
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4870, best: 0.4827)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1972
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4852, best: 0.4827)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2018
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4903, best: 0.4827)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2002
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4836, best: 0.4827)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1363
  ‚Ä¢ Validation Loss: 0.4824
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4824
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1491
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4820
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1290
  ‚Ä¢ Validation Loss: 0.4837
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4837, best: 0.4820)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1927
  ‚Ä¢ Validation Loss: 0.4856
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4856, best: 0.4820)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1618
  ‚Ä¢ Validation Loss: 0.4833
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4833, best: 0.4820)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1909
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4823, best: 0.4820)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2057
  ‚Ä¢ Validation Loss: 0.4821
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4821, best: 0.4820)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1967
  ‚Ä¢ Validation Loss: 0.4838
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4838, best: 0.4820)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2459
  ‚Ä¢ Validation Loss: 0.4820
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4820, best: 0.4820)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1694
  ‚Ä¢ Validation Loss: 0.4836
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4836, best: 0.4820)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2034
  ‚Ä¢ Validation Loss: 0.4835
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4835, best: 0.4820)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1716
  ‚Ä¢ Validation Loss: 0.4828
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4828, best: 0.4820)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1507
  ‚Ä¢ Validation Loss: 0.4830
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4830, best: 0.4820)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 23 batches (0 NaN/Inf loss, 23 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2352
  ‚Ä¢ Validation Loss: 0.4834
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4834, best: 0.4820)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1477
  ‚Ä¢ Validation Loss: 0.4891
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4891, best: 0.4820)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1712
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4839, best: 0.4820)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0659
  ‚Ä¢ Validation Loss: 0.4831
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4831, best: 0.4820)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1171
  ‚Ä¢ Validation Loss: 0.4815
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4815
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1600
  ‚Ä¢ Validation Loss: 0.4872
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4872, best: 0.4815)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1643
  ‚Ä¢ Validation Loss: 0.4790
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4790
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2021
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4811, best: 0.4790)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2102
  ‚Ä¢ Validation Loss: 0.4972
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4972, best: 0.4790)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1597
  ‚Ä¢ Validation Loss: 0.4849
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4849, best: 0.4790)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1361
  ‚Ä¢ Validation Loss: 0.4780
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4780
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1765
  ‚Ä¢ Validation Loss: 0.4801
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4801, best: 0.4780)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2091
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4725
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1579
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4764, best: 0.4725)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0835
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4744, best: 0.4725)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1124
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4753, best: 0.4725)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1515
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4725)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1564
  ‚Ä¢ Validation Loss: 0.4828
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4828, best: 0.4725)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1256
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4718
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1453
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4695
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0916
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4716, best: 0.4695)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0808
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4715, best: 0.4695)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4691
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1036
  ‚Ä¢ Validation Loss: 0.4791
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4791, best: 0.4691)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0726
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4715, best: 0.4691)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0602
  ‚Ä¢ Validation Loss: 0.4777
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4777, best: 0.4691)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0790
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4684
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0634
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4683
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0607
  ‚Ä¢ Validation Loss: 0.4734
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4734, best: 0.4683)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0726
  ‚Ä¢ Validation Loss: 0.4683
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4683, best: 0.4683)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1013
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4674
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4714, best: 0.4674)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0697
  ‚Ä¢ Validation Loss: 0.4670
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4670
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0608
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4692, best: 0.4670)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0110
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4708, best: 0.4670)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0285
  ‚Ä¢ Validation Loss: 0.4765
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4765, best: 0.4670)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0397
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4684, best: 0.4670)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0306
  ‚Ä¢ Validation Loss: 0.4671
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4671, best: 0.4670)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0777
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4674, best: 0.4670)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0700
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4668
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0670
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4730, best: 0.4668)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0664
  ‚Ä¢ Validation Loss: 0.4668
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4668, best: 0.4668)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1029
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4624
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0603
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4665, best: 0.4624)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0372
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4653, best: 0.4624)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0414
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4655, best: 0.4624)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0566
  ‚Ä¢ Validation Loss: 0.4703
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4703, best: 0.4624)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0389
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4722, best: 0.4624)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0496
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4638, best: 0.4624)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0567
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4634, best: 0.4624)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0425
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4625, best: 0.4624)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0530
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4629, best: 0.4624)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0417
  ‚Ä¢ Validation Loss: 0.4631
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4631, best: 0.4624)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0091
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4653, best: 0.4624)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0663
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4652, best: 0.4624)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0738
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4622
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0510
  ‚Ä¢ Validation Loss: 0.4616
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4616
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4623, best: 0.4616)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4627, best: 0.4616)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0307
  ‚Ä¢ Validation Loss: 0.4633
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4633, best: 0.4616)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0400
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4629, best: 0.4616)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0221
  ‚Ä¢ Validation Loss: 0.4642
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4642, best: 0.4616)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4634, best: 0.4616)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4636, best: 0.4616)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0298
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4651, best: 0.4616)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0204
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4652, best: 0.4616)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0281
  ‚Ä¢ Validation Loss: 0.4648
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4648, best: 0.4616)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4646, best: 0.4616)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0209
  ‚Ä¢ Validation Loss: 0.4645
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4645, best: 0.4616)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0373
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4618, best: 0.4616)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0304
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4605
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4605, best: 0.4605)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0290
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4603
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4606, best: 0.4603)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4611, best: 0.4603)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0388
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4622, best: 0.4603)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0197
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4629, best: 0.4603)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0205
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4629, best: 0.4603)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0099
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4629, best: 0.4603)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0200
  ‚Ä¢ Validation Loss: 0.4633
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4633, best: 0.4603)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0107
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4624, best: 0.4603)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0311
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4623, best: 0.4603)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4624, best: 0.4603)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0087
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4629, best: 0.4603)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0296
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4618, best: 0.4603)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0505
  ‚Ä¢ Validation Loss: 0.4628
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4628, best: 0.4603)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4627
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4627, best: 0.4603)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0225
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4626, best: 0.4603)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0392
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4653, best: 0.4603)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0189
  ‚Ä¢ Validation Loss: 0.4631
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4631, best: 0.4603)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4634, best: 0.4603)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0217
  ‚Ä¢ Validation Loss: 0.4639
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4639, best: 0.4603)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4644
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4644, best: 0.4603)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0314
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4638, best: 0.4603)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0212
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4626, best: 0.4603)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0378
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4625, best: 0.4603)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0475
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4636, best: 0.4603)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4635
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4635, best: 0.4603)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4631
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4631, best: 0.4603)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0196
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4638, best: 0.4603)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0094
  ‚Ä¢ Validation Loss: 0.4637
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4637, best: 0.4603)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0187
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4595
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0102
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4595, best: 0.4595)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4594
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4589
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0486
  ‚Ä¢ Validation Loss: 0.4687
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4687, best: 0.4589)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0304
  ‚Ä¢ Validation Loss: 0.4782
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4782, best: 0.4589)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0409
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4675, best: 0.4589)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0614
  ‚Ä¢ Validation Loss: 0.4629
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4629, best: 0.4589)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0204
  ‚Ä¢ Validation Loss: 0.4666
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4666, best: 0.4589)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0356
  ‚Ä¢ Validation Loss: 0.4694
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4694, best: 0.4589)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0204
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4653, best: 0.4589)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4630
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4630, best: 0.4589)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0306
  ‚Ä¢ Validation Loss: 0.4631
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4631, best: 0.4589)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0293
  ‚Ä¢ Validation Loss: 0.4655
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4655, best: 0.4589)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0212
  ‚Ä¢ Validation Loss: 0.4665
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4665, best: 0.4589)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4676, best: 0.4589)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0088
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4681, best: 0.4589)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4682, best: 0.4589)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0429
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4718, best: 0.4589)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4715, best: 0.4589)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0495
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4625, best: 0.4589)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0296
  ‚Ä¢ Validation Loss: 0.4628
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4628, best: 0.4589)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0496
  ‚Ä¢ Validation Loss: 0.4674
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4674, best: 0.4589)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0399
  ‚Ä¢ Validation Loss: 0.4709
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4709, best: 0.4589)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0693
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4658, best: 0.4589)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0497
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4658, best: 0.4589)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0475
  ‚Ä¢ Validation Loss: 0.4679
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4679, best: 0.4589)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0613
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4721, best: 0.4589)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0214
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4693, best: 0.4589)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0396
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4626, best: 0.4589)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0383
  ‚Ä¢ Validation Loss: 0.4688
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4688, best: 0.4589)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4689, best: 0.4589)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0266
  ‚Ä¢ Validation Loss: 0.4686
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4686, best: 0.4589)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0097
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4589)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0296
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4662, best: 0.4589)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0681
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4657, best: 0.4589)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0392
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4638, best: 0.4589)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0399
  ‚Ä¢ Validation Loss: 0.4682
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4682, best: 0.4589)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4621, best: 0.4589)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0668
  ‚Ä¢ Validation Loss: 0.4638
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4638, best: 0.4589)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4642
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4642, best: 0.4589)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0355
  ‚Ä¢ Validation Loss: 0.4626
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4626, best: 0.4589)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0296
  ‚Ä¢ Validation Loss: 0.4600
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4600, best: 0.4589)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0212
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4598, best: 0.4589)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0181
  ‚Ä¢ Validation Loss: 0.4584
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4584
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0087
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4595, best: 0.4584)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0100
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4593, best: 0.4584)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4588, best: 0.4584)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4605, best: 0.4584)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4595, best: 0.4584)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0189
  ‚Ä¢ Validation Loss: 0.4624
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4624, best: 0.4584)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0094
  ‚Ä¢ Validation Loss: 0.4637
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4637, best: 0.4584)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0287
  ‚Ä¢ Validation Loss: 0.4646
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4646, best: 0.4584)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0309
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4611, best: 0.4584)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0092
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4586, best: 0.4584)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0291
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4585, best: 0.4584)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4582
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4582
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4598, best: 0.4582)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0282
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4617, best: 0.4582)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0178
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4622, best: 0.4582)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0183
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4621, best: 0.4582)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0310
  ‚Ä¢ Validation Loss: 0.4610
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4610, best: 0.4582)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4570
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4570
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0388
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4602, best: 0.4570)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0208
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4647, best: 0.4570)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0484
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4623, best: 0.4570)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0089
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4618, best: 0.4570)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0213
  ‚Ä¢ Validation Loss: 0.4636
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4636, best: 0.4570)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0299
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4596, best: 0.4570)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0094
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4606, best: 0.4570)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0374
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4568
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0590
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4693, best: 0.4568)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0107
  ‚Ä¢ Validation Loss: 0.4707
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4707, best: 0.4568)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0386
  ‚Ä¢ Validation Loss: 0.4706
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4706, best: 0.4568)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0294
  ‚Ä¢ Validation Loss: 0.4610
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4610, best: 0.4568)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0191
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4573, best: 0.4568)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0409
  ‚Ä¢ Validation Loss: 0.4582
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4582, best: 0.4568)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0459
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4580, best: 0.4568)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0198
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4588, best: 0.4568)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4589, best: 0.4568)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4588, best: 0.4568)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0204
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4608, best: 0.4568)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4606, best: 0.4568)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4603, best: 0.4568)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4603, best: 0.4568)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4613
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4613, best: 0.4568)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4605, best: 0.4568)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4606, best: 0.4568)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4602, best: 0.4568)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4596, best: 0.4568)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4611, best: 0.4568)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4602, best: 0.4568)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4603, best: 0.4568)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4605, best: 0.4568)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4597, best: 0.4568)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4595, best: 0.4568)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4595, best: 0.4568)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4597, best: 0.4568)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4596, best: 0.4568)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4603
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4603, best: 0.4568)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4602, best: 0.4568)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4597, best: 0.4568)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4596, best: 0.4568)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4600
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4600, best: 0.4568)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4599, best: 0.4568)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4605, best: 0.4568)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4605
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4605, best: 0.4568)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4593, best: 0.4568)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4600
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4600, best: 0.4568)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4602, best: 0.4568)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4602, best: 0.4568)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4592, best: 0.4568)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4604
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4604, best: 0.4568)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4598
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4598, best: 0.4568)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4601, best: 0.4568)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4597, best: 0.4568)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0095
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4595, best: 0.4568)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4593, best: 0.4568)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4595, best: 0.4568)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4597, best: 0.4568)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0083
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4590, best: 0.4568)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4591
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4591, best: 0.4568)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4583
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4583, best: 0.4568)
    ‚ö† No improvement for 67 epochs (patience: 150, remaining: 83)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4585, best: 0.4568)
    ‚ö† No improvement for 68 epochs (patience: 150, remaining: 82)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4590, best: 0.4568)
    ‚ö† No improvement for 69 epochs (patience: 150, remaining: 81)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4591
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4591, best: 0.4568)
    ‚ö† No improvement for 70 epochs (patience: 150, remaining: 80)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4588, best: 0.4568)
    ‚ö† No improvement for 71 epochs (patience: 150, remaining: 79)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0098
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4586, best: 0.4568)
    ‚ö† No improvement for 72 epochs (patience: 150, remaining: 78)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4585, best: 0.4568)
    ‚ö† No improvement for 73 epochs (patience: 150, remaining: 77)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4580, best: 0.4568)
    ‚ö† No improvement for 74 epochs (patience: 150, remaining: 76)

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0120
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4581, best: 0.4568)
    ‚ö† No improvement for 75 epochs (patience: 150, remaining: 75)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4577, best: 0.4568)
    ‚ö† No improvement for 76 epochs (patience: 150, remaining: 74)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4579
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4579, best: 0.4568)
    ‚ö† No improvement for 77 epochs (patience: 150, remaining: 73)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4587, best: 0.4568)
    ‚ö† No improvement for 78 epochs (patience: 150, remaining: 72)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 45 batches (0 NaN/Inf loss, 45 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: inf
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4581, best: 0.4568)
    ‚ö† No improvement for 79 epochs (patience: 150, remaining: 71)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4568
Total Epochs:   300
Models Saved:   ./Result/a2/Latin16746
TensorBoard:    ./Result/a2/Latin16746/tensorboard_logs
================================================================================

[01:14:00] Training completed. Best val loss: 0.4568

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 6
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Latin16746
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 009 (54 patches)
‚úì Ground truth found for 009
‚úì Completed: 009
Processing: 020 (54 patches)
‚úì Ground truth found for 020
‚úì Completed: 020
Processing: 022 (54 patches)
‚úì Ground truth found for 022
‚úì Completed: 022
Processing: 029 (54 patches)
‚úì Ground truth found for 029
‚úì Completed: 029
Processing: 035 (54 patches)
‚úì Ground truth found for 035
‚úì Completed: 035
Processing: 048 (54 patches)
‚úì Ground truth found for 048
‚úì Completed: 048
Processing: 069 (54 patches)
‚úì Ground truth found for 069
‚úì Completed: 069
Processing: 082 (54 patches)
‚úì Ground truth found for 082
‚úì Completed: 082
Processing: 088 (54 patches)
‚úì Ground truth found for 088
‚úì Completed: 088
Processing: 089 (54 patches)
‚úì Ground truth found for 089
‚úì Completed: 089
Processing: 091 (54 patches)
‚úì Ground truth found for 091
‚úì Completed: 091
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 106 (54 patches)
‚úì Ground truth found for 106
‚úì Completed: 106
Processing: 117 (54 patches)
‚úì Ground truth found for 117
‚úì Completed: 117
Processing: 123 (54 patches)
‚úì Ground truth found for 123
‚úì Completed: 123
Processing: 125 (54 patches)
‚úì Ground truth found for 125
‚úì Completed: 125
Processing: 130 (54 patches)
‚úì Ground truth found for 130
‚úì Completed: 130
Processing: 133 (54 patches)
‚úì Ground truth found for 133
‚úì Completed: 133
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 146 (54 patches)
‚úì Ground truth found for 146
‚úì Completed: 146
Processing: 166 (54 patches)
‚úì Ground truth found for 166
‚úì Completed: 166
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 215 (54 patches)
‚úì Ground truth found for 215
‚úì Completed: 215
Processing: 237 (54 patches)
‚úì Ground truth found for 237
‚úì Completed: 237
Processing: 243 (54 patches)
‚úì Ground truth found for 243
‚úì Completed: 243
Processing: 255 (54 patches)
‚úì Ground truth found for 255
‚úì Completed: 255
Processing: 258 (54 patches)
‚úì Ground truth found for 258
‚úì Completed: 258
Processing: 284 (54 patches)
‚úì Ground truth found for 284
‚úì Completed: 284
Processing: 325 (54 patches)
‚úì Ground truth found for 325
‚úì Completed: 325
Processing: 357 (54 patches)
‚úì Ground truth found for 357
‚úì Completed: 357

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9744, Recall=0.9878, F1=0.9811, IoU=0.9628
Paratext            : Precision=0.5080, Recall=0.5873, F1=0.5447, IoU=0.3743
Decoration          : Precision=0.9498, Recall=0.8260, F1=0.8836, IoU=0.7915
Main Text           : Precision=0.8156, Recall=0.8101, F1=0.8128, IoU=0.6847
Title               : Precision=0.4927, Recall=0.3817, F1=0.4302, IoU=0.2740
Chapter Headings    : Precision=0.4043, Recall=0.0030, F1=0.0060, IoU=0.0030

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6908
Mean Recall:    0.5993
Mean F1-Score:  0.6097
Mean IoU:       0.5151
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Latin16746
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TRAINING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Configuration: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
Output Directory: ./Result/a2/Syr341

Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
All arguments validated successfully!
Setting random seed to 1234 for reproducible training...
Setting up U-DIADS-Bib dataset...
Detected Syriaque341 manuscript: using 5 classes (no Chapter Headings)
‚úì Class-aware augmentation enabled (stronger augmentation for rare classes)
  Rare classes: Paratext, Decoration, Title, Chapter Headings
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/training
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/training_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Looking for images in: ../../U-DIADS-Bib-MS_patched/Syr341/Image/validation
Looking for masks in: ../../U-DIADS-Bib-MS_patched/Syr341/mask/validation_labels
Found 540 patch images
Successfully matched 540 image-mask pairs
Balanced sampler created (continuous rarity-based oversampling).
‚úì Balanced sampler enabled (oversampling rare classes)
Loading CNN-Transformer model...
================================================================================
üöÄ Loading CNN-Transformer Model
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
Model moved to CUDA
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
Model created successfully with 5 classes

=== Starting Training ===
Dataset: UDIADS_BIB
Model: CNN-Transformer
Batch size: 12
Max epochs: 300
Learning rate: 0.0001


================================================================================
TRAINING CONFIGURATION
================================================================================
Dataset: UDIADS_BIB
Manuscript: Syr341
Model: CNN-Transformer (EfficientNet-B4 + Swin-UNet Decoder)
Configuration: Bo + FF + FL
  ‚Ä¢ Bottleneck: ‚úì
  ‚Ä¢ Adapter Mode: streaming
  ‚Ä¢ Deep Supervision: ‚úó
  ‚Ä¢ Fusion Method: FOURIER
  ‚Ä¢ Multi-Scale Aggregation: ‚úó
  ‚Ä¢ Normalization: GroupNorm
Batch Size: 12
Max Epochs: 300
Learning Rate: 0.0001
Number of Classes: 5
Early Stopping Patience: 150 epochs
Output Directory: ./Result/a2/Syr341
================================================================================

üìä Dataset Statistics:
   - Training samples: 540
   - Validation samples: 540
   - Batch size: 12
   - Steps per epoch: 45


================================================================================
COMPUTING CLASS WEIGHTS
================================================================================

------------------------------------------------------------
Class Name           Percentage      Weight         
------------------------------------------------------------
Background            83.95%       1.0000
Paratext               0.17%       1.0000
Decoration             4.62%       1.0000
Main Text             11.13%       1.0000
Title                  0.12%       1.0000
------------------------------------------------------------
Total pixels: 27,095,040.0
Weight ratio (max/min): 1.00
================================================================================

üìà Class weights computed with rarity-based boosting (mean scaled)
   Final weights: [1. 1. 1. 1. 1.]

‚úì Using Class-Balanced Loss (CB Loss) for extreme imbalance (beta=0.9999)
  CB Loss is best for class imbalance ratios >100:1

================================================================================
OPTIMIZER CONFIGURATION
================================================================================
Encoder     : LR=0.000005, WD=0.000100, Params=16,742,216
Bottleneck  : LR=0.000100, WD=0.000500, Params=14,183,856
Decoder     : LR=0.000100, WD=0.001000, Params=7,855,103
Scheduler:   CosineAnnealingWarmRestarts (T_0=50)
================================================================================

üöÄ Using automatic mixed precision (AMP) for faster training (2-3x speedup)

üîç Checking for checkpoint at: ./Result/a2/Syr341/best_model_latest.pth
   Absolute path: /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/Result/a2/Syr341/best_model_latest.pth
   File exists: False
   No checkpoint found - starting training from scratch

================================================================================
üöÄ STARTING TRAINING
================================================================================
Loss: 0.3*CE + 0.2*Focal + 0.5*Dice
Early stopping: 150 epochs patience
================================================================================


EPOCH 1/300
--------------------------------------------------
Results:
  ‚Ä¢ Train Loss: 0.9647
  ‚Ä¢ Validation Loss: 0.7044
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.7044
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 2/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.7074
  ‚Ä¢ Validation Loss: 0.6128
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.6128
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 3/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 1 batches (0 NaN/Inf loss, 1 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.6228
  ‚Ä¢ Validation Loss: 0.5540
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5540
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 4/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5408
  ‚Ä¢ Validation Loss: 0.5358
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5358
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 5/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5240
  ‚Ä¢ Validation Loss: 0.5285
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5285
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 6/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4844
  ‚Ä¢ Validation Loss: 0.5257
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5257
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 7/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.5034
  ‚Ä¢ Validation Loss: 0.5319
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5319, best: 0.5257)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 8/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4674
  ‚Ä¢ Validation Loss: 0.5315
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5315, best: 0.5257)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 9/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4273
  ‚Ä¢ Validation Loss: 0.5169
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5169
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 10/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3971
  ‚Ä¢ Validation Loss: 0.5098
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.5098
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 11/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4149
  ‚Ä¢ Validation Loss: 0.5122
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5122, best: 0.5098)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 12/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4198
  ‚Ä¢ Validation Loss: 0.5119
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5119, best: 0.5098)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 13/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4363
  ‚Ä¢ Validation Loss: 0.5103
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5103, best: 0.5098)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 14/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3763
  ‚Ä¢ Validation Loss: 0.5077
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5077
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 15/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4192
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5056
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 16/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3331
  ‚Ä¢ Validation Loss: 0.5020
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.5020
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 17/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3327
  ‚Ä¢ Validation Loss: 0.5051
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5051, best: 0.5020)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 18/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3667
  ‚Ä¢ Validation Loss: 0.5056
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.5056, best: 0.5020)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 19/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 15 batches (0 NaN/Inf loss, 15 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3053
  ‚Ä¢ Validation Loss: 0.5012
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.5012
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 20/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3494
  ‚Ä¢ Validation Loss: 0.5059
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5059, best: 0.5012)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 21/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3458
  ‚Ä¢ Validation Loss: 0.5025
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.5025, best: 0.5012)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 22/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3722
  ‚Ä¢ Validation Loss: 0.4980
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4980
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 23/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3725
  ‚Ä¢ Validation Loss: 0.4992
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4992, best: 0.4980)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 24/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 13 batches (0 NaN/Inf loss, 13 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3264
  ‚Ä¢ Validation Loss: 0.4997
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4997, best: 0.4980)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 25/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 12 batches (0 NaN/Inf loss, 12 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3414
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4938
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 26/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 2 batches (0 NaN/Inf loss, 2 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4263
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4946, best: 0.4938)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 27/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3457
  ‚Ä¢ Validation Loss: 0.5008
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.5008, best: 0.4938)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 28/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3423
  ‚Ä¢ Validation Loss: 0.4955
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4955, best: 0.4938)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 29/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3955
  ‚Ä¢ Validation Loss: 0.4946
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4946, best: 0.4938)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 30/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3589
  ‚Ä¢ Validation Loss: 0.4945
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4945, best: 0.4938)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 31/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 14 batches (0 NaN/Inf loss, 14 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3099
  ‚Ä¢ Validation Loss: 0.4911
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4911
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 32/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3781
  ‚Ä¢ Validation Loss: 0.4910
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4910
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 33/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3750
  ‚Ä¢ Validation Loss: 0.4905
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4905
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 34/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3537
  ‚Ä¢ Validation Loss: 0.4940
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4940, best: 0.4905)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 35/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3816
  ‚Ä¢ Validation Loss: 0.4928
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4928, best: 0.4905)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 36/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 4 batches (0 NaN/Inf loss, 4 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.4094
  ‚Ä¢ Validation Loss: 0.4967
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4967, best: 0.4905)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 37/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3718
  ‚Ä¢ Validation Loss: 0.4947
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4947, best: 0.4905)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 38/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3631
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4903
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 39/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3797
  ‚Ä¢ Validation Loss: 0.4916
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4916, best: 0.4903)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 40/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 6 batches (0 NaN/Inf loss, 6 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3763
  ‚Ä¢ Validation Loss: 0.4885
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4885
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 41/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3830
  ‚Ä¢ Validation Loss: 0.4894
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4894, best: 0.4885)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 42/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3345
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4903, best: 0.4885)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 43/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3376
  ‚Ä¢ Validation Loss: 0.4882
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4882
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 44/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3338
  ‚Ä¢ Validation Loss: 0.4886
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4886, best: 0.4882)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 45/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3677
  ‚Ä¢ Validation Loss: 0.4880
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4880
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 46/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3445
  ‚Ä¢ Validation Loss: 0.4879
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    ‚úì New best checkpoint saved! Val loss: 0.4879
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 47/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 16 batches (0 NaN/Inf loss, 16 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2941
  ‚Ä¢ Validation Loss: 0.4900
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4900, best: 0.4879)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 48/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3493
  ‚Ä¢ Validation Loss: 0.4887
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4887, best: 0.4879)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 49/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 8 batches (0 NaN/Inf loss, 8 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3647
  ‚Ä¢ Validation Loss: 0.4898
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4898, best: 0.4879)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 50/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 5 batches (0 NaN/Inf loss, 5 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3958
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4895, best: 0.4879)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 51/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 11 batches (0 NaN/Inf loss, 11 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3440
  ‚Ä¢ Validation Loss: 0.5017
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.5017, best: 0.4879)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 52/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 7 batches (0 NaN/Inf loss, 7 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3827
  ‚Ä¢ Validation Loss: 0.4876
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4876
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 53/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 9 batches (0 NaN/Inf loss, 9 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3523
  ‚Ä¢ Validation Loss: 0.4917
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4917, best: 0.4876)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 54/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 10 batches (0 NaN/Inf loss, 10 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.3428
  ‚Ä¢ Validation Loss: 0.4852
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4852
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 55/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2204
  ‚Ä¢ Validation Loss: 0.4874
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4874, best: 0.4852)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 56/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 25 batches (0 NaN/Inf loss, 25 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2083
  ‚Ä¢ Validation Loss: 0.4903
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4903, best: 0.4852)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 57/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 21 batches (0 NaN/Inf loss, 21 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2403
  ‚Ä¢ Validation Loss: 0.4858
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4858, best: 0.4852)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 58/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 24 batches (0 NaN/Inf loss, 24 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2024
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4860, best: 0.4852)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 59/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1897
  ‚Ä¢ Validation Loss: 0.4950
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4950, best: 0.4852)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 60/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2113
  ‚Ä¢ Validation Loss: 0.4823
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4823
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 61/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 27 batches (0 NaN/Inf loss, 27 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1684
  ‚Ä¢ Validation Loss: 0.4847
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4847, best: 0.4823)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 62/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2255
  ‚Ä¢ Validation Loss: 0.4853
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4853, best: 0.4823)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 63/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1604
  ‚Ä¢ Validation Loss: 0.4857
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4857, best: 0.4823)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 64/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1787
  ‚Ä¢ Validation Loss: 0.4860
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4860, best: 0.4823)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 65/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1731
  ‚Ä¢ Validation Loss: 0.4826
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4826, best: 0.4823)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 66/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0930
  ‚Ä¢ Validation Loss: 0.4977
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4977, best: 0.4823)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 67/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2162
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4771
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 68/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 22 batches (0 NaN/Inf loss, 22 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.2150
  ‚Ä¢ Validation Loss: 0.4752
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4752
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 69/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1321
  ‚Ä¢ Validation Loss: 0.4827
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4827, best: 0.4752)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 70/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1156
  ‚Ä¢ Validation Loss: 0.4811
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4811, best: 0.4752)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 71/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1489
  ‚Ä¢ Validation Loss: 0.4846
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4846, best: 0.4752)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 72/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1169
  ‚Ä¢ Validation Loss: 0.4829
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4829, best: 0.4752)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 73/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1300
  ‚Ä¢ Validation Loss: 0.4785
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4785, best: 0.4752)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 74/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 29 batches (0 NaN/Inf loss, 29 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1557
  ‚Ä¢ Validation Loss: 0.4901
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4901, best: 0.4752)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 75/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 26 batches (0 NaN/Inf loss, 26 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1782
  ‚Ä¢ Validation Loss: 0.4938
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4938, best: 0.4752)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 76/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0710
  ‚Ä¢ Validation Loss: 0.4922
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4922, best: 0.4752)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 77/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0742
  ‚Ä¢ Validation Loss: 0.4895
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4895, best: 0.4752)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 78/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0450
  ‚Ä¢ Validation Loss: 0.4866
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4866, best: 0.4752)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 79/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 28 batches (0 NaN/Inf loss, 28 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1521
  ‚Ä¢ Validation Loss: 0.4776
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4776, best: 0.4752)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 80/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0997
  ‚Ä¢ Validation Loss: 0.4810
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4810, best: 0.4752)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 81/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0551
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4729
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 82/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4793, best: 0.4729)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 83/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0754
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4800, best: 0.4729)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 84/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0819
  ‚Ä¢ Validation Loss: 0.4764
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4764, best: 0.4729)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 85/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1074
  ‚Ä¢ Validation Loss: 0.4787
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4787, best: 0.4729)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 86/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1318
  ‚Ä¢ Validation Loss: 0.4744
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4744, best: 0.4729)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 87/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0805
  ‚Ä¢ Validation Loss: 0.4750
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4750, best: 0.4729)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 88/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1033
  ‚Ä¢ Validation Loss: 0.4800
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4800, best: 0.4729)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 89/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0804
  ‚Ä¢ Validation Loss: 0.4779
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4779, best: 0.4729)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 90/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1060
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4732, best: 0.4729)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 91/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0697
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4695
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 92/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0529
  ‚Ä¢ Validation Loss: 0.4751
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4751, best: 0.4695)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 93/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1136
  ‚Ä¢ Validation Loss: 0.4711
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4711, best: 0.4695)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 94/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0514
  ‚Ä¢ Validation Loss: 0.4768
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4768, best: 0.4695)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 95/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1197
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4725, best: 0.4695)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 96/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1337
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4797, best: 0.4695)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 97/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1369
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4766, best: 0.4695)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 98/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0901
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4712, best: 0.4695)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 99/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0519
  ‚Ä¢ Validation Loss: 0.4713
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4713, best: 0.4695)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 100/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0905
  ‚Ä¢ Validation Loss: 0.4839
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
   üíæ Periodic checkpoint: epoch_100.pth
    No improvement (current: 0.4839, best: 0.4695)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 101/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0829
  ‚Ä¢ Validation Loss: 0.4675
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4675
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 102/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0593
  ‚Ä¢ Validation Loss: 0.4755
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4755, best: 0.4675)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 103/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0818
  ‚Ä¢ Validation Loss: 0.4774
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4774, best: 0.4675)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 104/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1085
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4729, best: 0.4675)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 105/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0845
  ‚Ä¢ Validation Loss: 0.4753
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4753, best: 0.4675)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 106/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0634
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4714, best: 0.4675)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 107/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0602
  ‚Ä¢ Validation Loss: 0.4693
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4693, best: 0.4675)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 108/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0360
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4691, best: 0.4675)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 109/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0578
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4704, best: 0.4675)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 110/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0794
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4771, best: 0.4675)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 111/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0854
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4733, best: 0.4675)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 112/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0730
  ‚Ä¢ Validation Loss: 0.4718
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4718, best: 0.4675)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 113/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0522
  ‚Ä¢ Validation Loss: 0.4724
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4724, best: 0.4675)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 114/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0683
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4716, best: 0.4675)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 115/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0872
  ‚Ä¢ Validation Loss: 0.4729
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4729, best: 0.4675)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 116/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0860
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4730, best: 0.4675)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 117/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0807
  ‚Ä¢ Validation Loss: 0.4728
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4728, best: 0.4675)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 118/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0603
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4743, best: 0.4675)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 119/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0833
  ‚Ä¢ Validation Loss: 0.4771
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4771, best: 0.4675)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 120/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0754
  ‚Ä¢ Validation Loss: 0.4745
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4745, best: 0.4675)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 121/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0101
  ‚Ä¢ Validation Loss: 0.4749
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4749, best: 0.4675)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 122/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0478
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4708, best: 0.4675)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 123/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0471
  ‚Ä¢ Validation Loss: 0.4735
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4735, best: 0.4675)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 124/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1040
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4712, best: 0.4675)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 125/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0768
  ‚Ä¢ Validation Loss: 0.4741
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4741, best: 0.4675)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 126/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0794
  ‚Ä¢ Validation Loss: 0.4746
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4746, best: 0.4675)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 127/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0875
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4714, best: 0.4675)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 128/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0527
  ‚Ä¢ Validation Loss: 0.4740
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4740, best: 0.4675)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 129/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0507
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4716, best: 0.4675)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 130/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0991
  ‚Ä¢ Validation Loss: 0.4681
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4681, best: 0.4675)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 131/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0854
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4723, best: 0.4675)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 132/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0384
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4692, best: 0.4675)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 133/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0442
  ‚Ä¢ Validation Loss: 0.4733
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4733, best: 0.4675)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 134/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0816
  ‚Ä¢ Validation Loss: 0.4708
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4708, best: 0.4675)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 135/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0260
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4700, best: 0.4675)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 136/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0669
  ‚Ä¢ Validation Loss: 0.4700
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4700, best: 0.4675)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 137/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0444
  ‚Ä¢ Validation Loss: 0.4725
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4725, best: 0.4675)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 138/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0650
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4710, best: 0.4675)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 139/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0615
  ‚Ä¢ Validation Loss: 0.4730
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4730, best: 0.4675)
    ‚ö† No improvement for 38 epochs (patience: 150, remaining: 112)

EPOCH 140/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0607
  ‚Ä¢ Validation Loss: 0.4714
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4714, best: 0.4675)
    ‚ö† No improvement for 39 epochs (patience: 150, remaining: 111)

EPOCH 141/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0894
  ‚Ä¢ Validation Loss: 0.4689
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4689, best: 0.4675)
    ‚ö† No improvement for 40 epochs (patience: 150, remaining: 110)

EPOCH 142/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0350
  ‚Ä¢ Validation Loss: 0.4704
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4704, best: 0.4675)
    ‚ö† No improvement for 41 epochs (patience: 150, remaining: 109)

EPOCH 143/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0775
  ‚Ä¢ Validation Loss: 0.4723
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4723, best: 0.4675)
    ‚ö† No improvement for 42 epochs (patience: 150, remaining: 108)

EPOCH 144/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0560
  ‚Ä¢ Validation Loss: 0.4712
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4712, best: 0.4675)
    ‚ö† No improvement for 43 epochs (patience: 150, remaining: 107)

EPOCH 145/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0183
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4710, best: 0.4675)
    ‚ö† No improvement for 44 epochs (patience: 150, remaining: 106)

EPOCH 146/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0470
  ‚Ä¢ Validation Loss: 0.4707
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4707, best: 0.4675)
    ‚ö† No improvement for 45 epochs (patience: 150, remaining: 105)

EPOCH 147/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0494
  ‚Ä¢ Validation Loss: 0.4691
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4691, best: 0.4675)
    ‚ö† No improvement for 46 epochs (patience: 150, remaining: 104)

EPOCH 148/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0640
  ‚Ä¢ Validation Loss: 0.4710
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4710, best: 0.4675)
    ‚ö† No improvement for 47 epochs (patience: 150, remaining: 103)

EPOCH 149/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0991
  ‚Ä¢ Validation Loss: 0.4695
  ‚Ä¢ Learning Rate: 0.000000 (Encoder: 0.000000)
    No improvement (current: 0.4695, best: 0.4675)
    ‚ö† No improvement for 48 epochs (patience: 150, remaining: 102)

EPOCH 150/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0243
  ‚Ä¢ Validation Loss: 0.4722
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4722, best: 0.4675)
    ‚ö† No improvement for 49 epochs (patience: 150, remaining: 101)

EPOCH 151/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0798
  ‚Ä¢ Validation Loss: 0.4737
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4737, best: 0.4675)
    ‚ö† No improvement for 50 epochs (patience: 150, remaining: 100)

EPOCH 152/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0256
  ‚Ä¢ Validation Loss: 0.4692
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4692, best: 0.4675)
    ‚ö† No improvement for 51 epochs (patience: 150, remaining: 99)

EPOCH 153/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0781
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4739, best: 0.4675)
    ‚ö† No improvement for 52 epochs (patience: 150, remaining: 98)

EPOCH 154/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0327
  ‚Ä¢ Validation Loss: 0.4770
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4770, best: 0.4675)
    ‚ö† No improvement for 53 epochs (patience: 150, remaining: 97)

EPOCH 155/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0462
  ‚Ä¢ Validation Loss: 0.4738
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4738, best: 0.4675)
    ‚ö† No improvement for 54 epochs (patience: 150, remaining: 96)

EPOCH 156/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0450
  ‚Ä¢ Validation Loss: 0.4766
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4766, best: 0.4675)
    ‚ö† No improvement for 55 epochs (patience: 150, remaining: 95)

EPOCH 157/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1213
  ‚Ä¢ Validation Loss: 0.4797
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4797, best: 0.4675)
    ‚ö† No improvement for 56 epochs (patience: 150, remaining: 94)

EPOCH 158/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1015
  ‚Ä¢ Validation Loss: 0.4795
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4795, best: 0.4675)
    ‚ö† No improvement for 57 epochs (patience: 150, remaining: 93)

EPOCH 159/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0321
  ‚Ä¢ Validation Loss: 0.4731
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4731, best: 0.4675)
    ‚ö† No improvement for 58 epochs (patience: 150, remaining: 92)

EPOCH 160/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 30 batches (0 NaN/Inf loss, 30 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1375
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4721, best: 0.4675)
    ‚ö† No improvement for 59 epochs (patience: 150, remaining: 91)

EPOCH 161/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0525
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4715, best: 0.4675)
    ‚ö† No improvement for 60 epochs (patience: 150, remaining: 90)

EPOCH 162/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1002
  ‚Ä¢ Validation Loss: 0.4743
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4743, best: 0.4675)
    ‚ö† No improvement for 61 epochs (patience: 150, remaining: 89)

EPOCH 163/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0968
  ‚Ä¢ Validation Loss: 0.4732
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4732, best: 0.4675)
    ‚ö† No improvement for 62 epochs (patience: 150, remaining: 88)

EPOCH 164/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0876
  ‚Ä¢ Validation Loss: 0.4701
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4701, best: 0.4675)
    ‚ö† No improvement for 63 epochs (patience: 150, remaining: 87)

EPOCH 165/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4690, best: 0.4675)
    ‚ö† No improvement for 64 epochs (patience: 150, remaining: 86)

EPOCH 166/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1208
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4773, best: 0.4675)
    ‚ö† No improvement for 65 epochs (patience: 150, remaining: 85)

EPOCH 167/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0599
  ‚Ä¢ Validation Loss: 0.4721
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4721, best: 0.4675)
    ‚ö† No improvement for 66 epochs (patience: 150, remaining: 84)

EPOCH 168/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0443
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4653
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 169/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1326
  ‚Ä¢ Validation Loss: 0.4773
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4773, best: 0.4653)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 170/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0525
  ‚Ä¢ Validation Loss: 0.4739
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4739, best: 0.4653)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 171/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1059
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4652
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 172/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1079
  ‚Ä¢ Validation Loss: 0.4690
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4690, best: 0.4652)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 173/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 31 batches (0 NaN/Inf loss, 31 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1189
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4698, best: 0.4652)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 174/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0787
  ‚Ä¢ Validation Loss: 0.4680
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4680, best: 0.4652)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 175/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1237
  ‚Ä¢ Validation Loss: 0.4667
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4667, best: 0.4652)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 176/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0395
  ‚Ä¢ Validation Loss: 0.4758
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4758, best: 0.4652)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 177/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0777
  ‚Ä¢ Validation Loss: 0.4641
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4641
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 178/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0437
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4757, best: 0.4641)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 179/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1190
  ‚Ä¢ Validation Loss: 0.4788
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4788, best: 0.4641)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 180/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0858
  ‚Ä¢ Validation Loss: 0.4705
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4705, best: 0.4641)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 181/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 33 batches (0 NaN/Inf loss, 33 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0966
  ‚Ä¢ Validation Loss: 0.4757
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4757, best: 0.4641)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 182/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0820
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4651, best: 0.4641)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 183/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0940
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4659, best: 0.4641)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 184/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0871
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4652, best: 0.4641)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 185/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0928
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4634
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 186/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0707
  ‚Ä¢ Validation Loss: 0.4659
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4659, best: 0.4634)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 187/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0855
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4640, best: 0.4634)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 188/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0793
  ‚Ä¢ Validation Loss: 0.4610
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4610
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 189/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0533
  ‚Ä¢ Validation Loss: 0.4644
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4644, best: 0.4610)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 190/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0324
  ‚Ä¢ Validation Loss: 0.4698
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    No improvement (current: 0.4698, best: 0.4610)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 191/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0594
  ‚Ä¢ Validation Loss: 0.4609
  ‚Ä¢ Learning Rate: 0.000005 (Encoder: 0.000005)
    ‚úì New best checkpoint saved! Val loss: 0.4609
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 192/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0513
  ‚Ä¢ Validation Loss: 0.4633
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4633, best: 0.4609)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 193/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0607
  ‚Ä¢ Validation Loss: 0.4643
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4643, best: 0.4609)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 194/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0532
  ‚Ä¢ Validation Loss: 0.4789
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4789, best: 0.4609)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 195/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0157
  ‚Ä¢ Validation Loss: 0.4742
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4742, best: 0.4609)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 196/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0199
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4697, best: 0.4609)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 197/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0456
  ‚Ä¢ Validation Loss: 0.4615
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4615, best: 0.4609)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 198/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0254
  ‚Ä¢ Validation Loss: 0.4586
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4586
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 199/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0257
  ‚Ä¢ Validation Loss: 0.4602
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4602, best: 0.4586)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 200/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0284
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
   üíæ Periodic checkpoint: epoch_200.pth
    No improvement (current: 0.4652, best: 0.4586)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 201/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0351
  ‚Ä¢ Validation Loss: 0.4652
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4652, best: 0.4586)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 202/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0437
  ‚Ä¢ Validation Loss: 0.4634
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4634, best: 0.4586)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 203/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0421
  ‚Ä¢ Validation Loss: 0.4622
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4622, best: 0.4586)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 204/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.4591
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4591, best: 0.4586)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 205/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 36 batches (0 NaN/Inf loss, 36 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0872
  ‚Ä¢ Validation Loss: 0.4653
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4653, best: 0.4586)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 206/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0533
  ‚Ä¢ Validation Loss: 0.4647
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4647, best: 0.4586)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 207/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0322
  ‚Ä¢ Validation Loss: 0.4608
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4608, best: 0.4586)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 208/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0490
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4672, best: 0.4586)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 209/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0542
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4592, best: 0.4586)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 210/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0370
  ‚Ä¢ Validation Loss: 0.4615
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4615, best: 0.4586)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 211/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0427
  ‚Ä¢ Validation Loss: 0.4697
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4697, best: 0.4586)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 212/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0724
  ‚Ä¢ Validation Loss: 0.4612
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4612, best: 0.4586)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 213/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0862
  ‚Ä¢ Validation Loss: 0.4726
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4726, best: 0.4586)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 214/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 32 batches (0 NaN/Inf loss, 32 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.1181
  ‚Ä¢ Validation Loss: 0.4617
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4617, best: 0.4586)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 215/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0516
  ‚Ä¢ Validation Loss: 0.4684
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4684, best: 0.4586)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 216/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0642
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4599, best: 0.4586)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 217/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0521
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4588, best: 0.4586)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 218/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0332
  ‚Ä¢ Validation Loss: 0.4599
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4599, best: 0.4586)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 219/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0184
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    ‚úì New best checkpoint saved! Val loss: 0.4581
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 220/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0266
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4597, best: 0.4581)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 221/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0169
  ‚Ä¢ Validation Loss: 0.4625
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4625, best: 0.4581)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 222/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0395
  ‚Ä¢ Validation Loss: 0.4589
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4589, best: 0.4581)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 223/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0422
  ‚Ä¢ Validation Loss: 0.4600
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4600, best: 0.4581)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 224/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0589
  ‚Ä¢ Validation Loss: 0.4715
  ‚Ä¢ Learning Rate: 0.000004 (Encoder: 0.000004)
    No improvement (current: 0.4715, best: 0.4581)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 225/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0267
  ‚Ä¢ Validation Loss: 0.4672
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4672, best: 0.4581)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 226/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0248
  ‚Ä¢ Validation Loss: 0.4628
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4628, best: 0.4581)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 227/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0085
  ‚Ä¢ Validation Loss: 0.4611
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4611, best: 0.4581)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 228/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0194
  ‚Ä¢ Validation Loss: 0.4618
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4618, best: 0.4581)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 229/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0164
  ‚Ä¢ Validation Loss: 0.4640
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4640, best: 0.4581)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 230/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0183
  ‚Ä¢ Validation Loss: 0.4614
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4614, best: 0.4581)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 231/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0412
  ‚Ä¢ Validation Loss: 0.4676
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4676, best: 0.4581)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 232/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0506
  ‚Ä¢ Validation Loss: 0.4793
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4793, best: 0.4581)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 233/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0824
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4573
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 234/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0321
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4577, best: 0.4573)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 235/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0863
  ‚Ä¢ Validation Loss: 0.4620
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4620, best: 0.4573)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 236/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0506
  ‚Ä¢ Validation Loss: 0.4657
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4657, best: 0.4573)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 237/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0647
  ‚Ä¢ Validation Loss: 0.4590
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4590, best: 0.4573)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 238/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 34 batches (0 NaN/Inf loss, 34 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0982
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4594, best: 0.4573)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 239/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.4669
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4669, best: 0.4573)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 240/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0678
  ‚Ä¢ Validation Loss: 0.4615
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4615, best: 0.4573)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 241/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0674
  ‚Ä¢ Validation Loss: 0.4609
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4609, best: 0.4573)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 242/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 35 batches (0 NaN/Inf loss, 35 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0848
  ‚Ä¢ Validation Loss: 0.4621
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4621, best: 0.4573)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 243/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0632
  ‚Ä¢ Validation Loss: 0.4584
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4584, best: 0.4573)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 244/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0519
  ‚Ä¢ Validation Loss: 0.4601
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4601, best: 0.4573)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 245/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0334
  ‚Ä¢ Validation Loss: 0.4661
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4661, best: 0.4573)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 246/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0478
  ‚Ä¢ Validation Loss: 0.4606
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4606, best: 0.4573)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 247/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0634
  ‚Ä¢ Validation Loss: 0.4588
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4588, best: 0.4573)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 248/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0463
  ‚Ä¢ Validation Loss: 0.4574
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4574, best: 0.4573)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 249/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0197
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    ‚úì New best checkpoint saved! Val loss: 0.4569
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 250/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0262
  ‚Ä¢ Validation Loss: 0.4596
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4596, best: 0.4569)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 251/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0538
  ‚Ä¢ Validation Loss: 0.4623
  ‚Ä¢ Learning Rate: 0.000003 (Encoder: 0.000003)
    No improvement (current: 0.4623, best: 0.4569)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 252/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0425
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4580, best: 0.4569)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 253/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0512
  ‚Ä¢ Validation Loss: 0.4658
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4658, best: 0.4569)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 254/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0201
  ‚Ä¢ Validation Loss: 0.4651
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4651, best: 0.4569)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 255/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0088
  ‚Ä¢ Validation Loss: 0.4716
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4716, best: 0.4569)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 256/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0535
  ‚Ä¢ Validation Loss: 0.4662
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4662, best: 0.4569)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 257/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0519
  ‚Ä¢ Validation Loss: 0.4549
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    ‚úì New best checkpoint saved! Val loss: 0.4549
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 258/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0462
  ‚Ä¢ Validation Loss: 0.4585
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4585, best: 0.4549)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 259/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0072
  ‚Ä¢ Validation Loss: 0.4592
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4592, best: 0.4549)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 260/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0162
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4595, best: 0.4549)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 261/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0426
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4573, best: 0.4549)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 262/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0164
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4573, best: 0.4549)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

EPOCH 263/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0550
  ‚Ä¢ Validation Loss: 0.4620
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4620, best: 0.4549)
    ‚ö† No improvement for 6 epochs (patience: 150, remaining: 144)

EPOCH 264/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0627
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4581, best: 0.4549)
    ‚ö† No improvement for 7 epochs (patience: 150, remaining: 143)

EPOCH 265/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0188
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4561, best: 0.4549)
    ‚ö† No improvement for 8 epochs (patience: 150, remaining: 142)

EPOCH 266/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0165
  ‚Ä¢ Validation Loss: 0.4578
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4578, best: 0.4549)
    ‚ö† No improvement for 9 epochs (patience: 150, remaining: 141)

EPOCH 267/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0430
  ‚Ä¢ Validation Loss: 0.4569
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4569, best: 0.4549)
    ‚ö† No improvement for 10 epochs (patience: 150, remaining: 140)

EPOCH 268/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0439
  ‚Ä¢ Validation Loss: 0.4556
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4556, best: 0.4549)
    ‚ö† No improvement for 11 epochs (patience: 150, remaining: 139)

EPOCH 269/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0533
  ‚Ä¢ Validation Loss: 0.4575
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4575, best: 0.4549)
    ‚ö† No improvement for 12 epochs (patience: 150, remaining: 138)

EPOCH 270/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 37 batches (0 NaN/Inf loss, 37 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0737
  ‚Ä¢ Validation Loss: 0.4594
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4594, best: 0.4549)
    ‚ö† No improvement for 13 epochs (patience: 150, remaining: 137)

EPOCH 271/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0234
  ‚Ä¢ Validation Loss: 0.4580
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4580, best: 0.4549)
    ‚ö† No improvement for 14 epochs (patience: 150, remaining: 136)

EPOCH 272/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0273
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4593, best: 0.4549)
    ‚ö† No improvement for 15 epochs (patience: 150, remaining: 135)

EPOCH 273/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 38 batches (0 NaN/Inf loss, 38 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0623
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4595, best: 0.4549)
    ‚ö† No improvement for 16 epochs (patience: 150, remaining: 134)

EPOCH 274/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0263
  ‚Ä¢ Validation Loss: 0.4587
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4587, best: 0.4549)
    ‚ö† No improvement for 17 epochs (patience: 150, remaining: 133)

EPOCH 275/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0426
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4577, best: 0.4549)
    ‚ö† No improvement for 18 epochs (patience: 150, remaining: 132)

EPOCH 276/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0328
  ‚Ä¢ Validation Loss: 0.4568
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4568, best: 0.4549)
    ‚ö† No improvement for 19 epochs (patience: 150, remaining: 131)

EPOCH 277/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0333
  ‚Ä¢ Validation Loss: 0.4572
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4572, best: 0.4549)
    ‚ö† No improvement for 20 epochs (patience: 150, remaining: 130)

EPOCH 278/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0075
  ‚Ä¢ Validation Loss: 0.4566
  ‚Ä¢ Learning Rate: 0.000002 (Encoder: 0.000002)
    No improvement (current: 0.4566, best: 0.4549)
    ‚ö† No improvement for 21 epochs (patience: 150, remaining: 129)

EPOCH 279/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0161
  ‚Ä¢ Validation Loss: 0.4573
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4573, best: 0.4549)
    ‚ö† No improvement for 22 epochs (patience: 150, remaining: 128)

EPOCH 280/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0299
  ‚Ä¢ Validation Loss: 0.4578
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4578, best: 0.4549)
    ‚ö† No improvement for 23 epochs (patience: 150, remaining: 127)

EPOCH 281/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0241
  ‚Ä¢ Validation Loss: 0.4593
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4593, best: 0.4549)
    ‚ö† No improvement for 24 epochs (patience: 150, remaining: 126)

EPOCH 282/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0499
  ‚Ä¢ Validation Loss: 0.4581
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4581, best: 0.4549)
    ‚ö† No improvement for 25 epochs (patience: 150, remaining: 125)

EPOCH 283/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0192
  ‚Ä¢ Validation Loss: 0.4571
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4571, best: 0.4549)
    ‚ö† No improvement for 26 epochs (patience: 150, remaining: 124)

EPOCH 284/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0318
  ‚Ä¢ Validation Loss: 0.4556
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4556, best: 0.4549)
    ‚ö† No improvement for 27 epochs (patience: 150, remaining: 123)

EPOCH 285/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0090
  ‚Ä¢ Validation Loss: 0.4567
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4567, best: 0.4549)
    ‚ö† No improvement for 28 epochs (patience: 150, remaining: 122)

EPOCH 286/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0266
  ‚Ä¢ Validation Loss: 0.4552
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4552, best: 0.4549)
    ‚ö† No improvement for 29 epochs (patience: 150, remaining: 121)

EPOCH 287/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0178
  ‚Ä¢ Validation Loss: 0.4572
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4572, best: 0.4549)
    ‚ö† No improvement for 30 epochs (patience: 150, remaining: 120)

EPOCH 288/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0509
  ‚Ä¢ Validation Loss: 0.4597
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4597, best: 0.4549)
    ‚ö† No improvement for 31 epochs (patience: 150, remaining: 119)

EPOCH 289/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 42 batches (0 NaN/Inf loss, 42 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0243
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4595, best: 0.4549)
    ‚ö† No improvement for 32 epochs (patience: 150, remaining: 118)

EPOCH 290/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0190
  ‚Ä¢ Validation Loss: 0.4595
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4595, best: 0.4549)
    ‚ö† No improvement for 33 epochs (patience: 150, remaining: 117)

EPOCH 291/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0177
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4577, best: 0.4549)
    ‚ö† No improvement for 34 epochs (patience: 150, remaining: 116)

EPOCH 292/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0336
  ‚Ä¢ Validation Loss: 0.4577
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4577, best: 0.4549)
    ‚ö† No improvement for 35 epochs (patience: 150, remaining: 115)

EPOCH 293/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0163
  ‚Ä¢ Validation Loss: 0.4551
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4551, best: 0.4549)
    ‚ö† No improvement for 36 epochs (patience: 150, remaining: 114)

EPOCH 294/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 43 batches (0 NaN/Inf loss, 43 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0186
  ‚Ä¢ Validation Loss: 0.4557
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4557, best: 0.4549)
    ‚ö† No improvement for 37 epochs (patience: 150, remaining: 113)

EPOCH 295/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 39 batches (0 NaN/Inf loss, 39 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0517
  ‚Ä¢ Validation Loss: 0.4537
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    ‚úì New best checkpoint saved! Val loss: 0.4537
    ‚úì Improvement detected! Resetting patience counter.

EPOCH 296/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 44 batches (0 NaN/Inf loss, 44 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0096
  ‚Ä¢ Validation Loss: 0.4541
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4541, best: 0.4537)
    ‚ö† No improvement for 1 epochs (patience: 150, remaining: 149)

EPOCH 297/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0346
  ‚Ä¢ Validation Loss: 0.4541
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4541, best: 0.4537)
    ‚ö† No improvement for 2 epochs (patience: 150, remaining: 148)

EPOCH 298/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 41 batches (0 NaN/Inf loss, 41 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0317
  ‚Ä¢ Validation Loss: 0.4561
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4561, best: 0.4537)
    ‚ö† No improvement for 3 epochs (patience: 150, remaining: 147)

EPOCH 299/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0414
  ‚Ä¢ Validation Loss: 0.4548
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
    No improvement (current: 0.4548, best: 0.4537)
    ‚ö† No improvement for 4 epochs (patience: 150, remaining: 146)

EPOCH 300/300
--------------------------------------------------
  ‚ö†Ô∏è  Skipped 40 batches (0 NaN/Inf loss, 40 NaN/Inf gradients)
Results:
  ‚Ä¢ Train Loss: 0.0407
  ‚Ä¢ Validation Loss: 0.4557
  ‚Ä¢ Learning Rate: 0.000001 (Encoder: 0.000001)
   üíæ Periodic checkpoint: epoch_300.pth
    No improvement (current: 0.4557, best: 0.4537)
    ‚ö† No improvement for 5 epochs (patience: 150, remaining: 145)

================================================================================
TRAINING COMPLETED
================================================================================
Best Val Loss:  0.4537
Total Epochs:   300
Models Saved:   ./Result/a2/Syr341
TensorBoard:    ./Result/a2/Syr341/tensorboard_logs
================================================================================

[02:32:58] Training completed. Best val loss: 0.4537

=== TRAINING COMPLETED SUCCESSFULLY ===
Training Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TRAINING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Proceeding to testing...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  TESTING CNN-TRANSFORMER BASE MODEL + FOURIER FUSION: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Test Configuration:
  ‚úì Test-Time Augmentation (TTA): ENABLED
  ‚úó CRF Post-processing: DISABLED
  - Batch Size: 1 (reduced for TTA memory efficiency)

WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.
=== Historical Document Segmentation Testing ===

================================================================================
üöÄ Loading CNN-Transformer Model for Testing
================================================================================
Model Configuration:
  ‚úì EfficientNet-B4 Encoder
  ‚úì Bottleneck: Enabled
  ‚úì Swin Transformer Decoder
  ‚úì Fusion Method: fourier
  ‚úì Adapter Mode: streaming
  ‚úì Deep Supervision: Disabled
  ‚úì Multi-Scale Aggregation: Disabled
  ‚úì Normalization: GroupNorm
================================================================================
CNN-Transformer U-Net initialized:
  - Image size: 224
  - Number of classes: 5
  - Encoder: EfficientNet-B4
  - EfficientNet variant: tf_efficientnet_b4_ns
  - Embed dimension: 96
  - Deep Supervision: DISABLED
  - Fusion Method: FOURIER
  - Bottleneck (2 Swin blocks): ENABLED
  - Adapter Mode: STREAMING
  - ‚úÖ GroupNorm: ENABLED (in adapters)
CNN-Transformer model uses EfficientNet pretrained weights automatically.
No additional pretrained weights loading required.
‚úì Checkpoint architecture matches model - loading with strict=True
Loaded checkpoint: best_model_latest.pth

=== Starting Testing ===
Dataset: UDIADS_BIB | Manuscript: Syr341
--------------------------------------------------------------------------------
TEST-TIME AUGMENTATION (TTA): ‚úì ENABLED
  ‚Üí Using 4 augmentations: original, horizontal flip, vertical flip, rotation 90¬∞
  ‚Üí Averaging predictions across all augmentations
CRF POST-PROCESSING: ‚úó DISABLED
--------------------------------------------------------------------------------


Found 30 original images to process
Processing: 031 (54 patches)
‚úì Ground truth found for 031
‚úì Completed: 031
Processing: 053 (54 patches)
‚úì Ground truth found for 053
‚úì Completed: 053
Processing: 054 (54 patches)
‚úì Ground truth found for 054
‚úì Completed: 054
Processing: 071 (54 patches)
‚úì Ground truth found for 071
‚úì Completed: 071
Processing: 073 (54 patches)
‚úì Ground truth found for 073
‚úì Completed: 073
Processing: 075 (54 patches)
‚úì Ground truth found for 075
‚úì Completed: 075
Processing: 100 (54 patches)
‚úì Ground truth found for 100
‚úì Completed: 100
Processing: 137 (54 patches)
‚úì Ground truth found for 137
‚úì Completed: 137
Processing: 150 (54 patches)
‚úì Ground truth found for 150
‚úì Completed: 150
Processing: 160 (54 patches)
‚úì Ground truth found for 160
‚úì Completed: 160
Processing: 167 (54 patches)
‚úì Ground truth found for 167
‚úì Completed: 167
Processing: 184 (54 patches)
‚úì Ground truth found for 184
‚úì Completed: 184
Processing: 190 (54 patches)
‚úì Ground truth found for 190
‚úì Completed: 190
Processing: 201 (54 patches)
‚úì Ground truth found for 201
‚úì Completed: 201
Processing: 210 (54 patches)
‚úì Ground truth found for 210
‚úì Completed: 210
Processing: 222 (54 patches)
‚úì Ground truth found for 222
‚úì Completed: 222
Processing: 224 (54 patches)
‚úì Ground truth found for 224
‚úì Completed: 224
Processing: 231 (54 patches)
‚úì Ground truth found for 231
‚úì Completed: 231
Processing: 241 (54 patches)
‚úì Ground truth found for 241
‚úì Completed: 241
Processing: 249 (54 patches)
‚úì Ground truth found for 249
‚úì Completed: 249
Processing: 252 (54 patches)
‚úì Ground truth found for 252
‚úì Completed: 252
Processing: 267 (54 patches)
‚úì Ground truth found for 267
‚úì Completed: 267
Processing: 281 (54 patches)
‚úì Ground truth found for 281
‚úì Completed: 281
Processing: 286 (54 patches)
‚úì Ground truth found for 286
‚úì Completed: 286
Processing: 290 (54 patches)
‚úì Ground truth found for 290
‚úì Completed: 290
Processing: 313 (54 patches)
‚úì Ground truth found for 313
‚úì Completed: 313
Processing: 362 (54 patches)
‚úì Ground truth found for 362
‚úì Completed: 362
Processing: 368 (54 patches)
‚úì Ground truth found for 368
‚úì Completed: 368
Processing: 376 (54 patches)
‚úì Ground truth found for 376
‚úì Completed: 376
Processing: 446 (54 patches)
‚úì Ground truth found for 446
‚úì Completed: 446

================================================================================
Testing Summary: Processed 30 images with ground truth
================================================================================


================================================================================
SEGMENTATION METRICS
================================================================================
Images Processed: 30

Per-class metrics:
--------------------------------------------------------------------------------
Background          : Precision=0.9579, Recall=0.9782, F1=0.9679, IoU=0.9379
Paratext            : Precision=0.2299, Recall=0.2290, F1=0.2294, IoU=0.1296
Decoration          : Precision=0.9233, Recall=0.5342, F1=0.6768, IoU=0.5115
Main Text           : Precision=0.8352, Recall=0.7872, F1=0.8105, IoU=0.6814
Title               : Precision=0.3228, Recall=0.0623, F1=0.1045, IoU=0.0551

Mean metrics:
--------------------------------------------------------------------------------
Mean Precision: 0.6538
Mean Recall:    0.5182
Mean F1-Score:  0.5578
Mean IoU:       0.4631
================================================================================

================================================================================
AVERAGE METRICS ACROSS ALL MANUSCRIPTS
================================================================================
Manuscripts: Latin2, Latin14396, Latin16746, Syr341
--------------------------------------------------------------------------------
Mean Precision: 0.6733
Mean Recall:    0.5694
Mean F1-Score:  0.5922
Mean IoU:       0.4915
================================================================================

=== TESTING COMPLETED SUCCESSFULLY ===
Testing Finished!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì TESTING COMPLETED: Syr341
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


============================================================================
ALL MANUSCRIPTS PROCESSED
============================================================================
Configuration Used: CNN-TRANSFORMER BASE MODEL + FOURIER FEATURE FUSION
Results Location: ./Result/a2/
============================================================================
=== JOB_STATISTICS ===
=== current date     : Tue Nov 18 02:39:26 AM CET 2025
= Job-ID             : 1392196 on tinygpu
= Job-Name           : 3rd
= Job-Command        : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network/run2.sh
= Initial workdir    : /home/hpc/iwi5/iwi5250h/DAS_Using_SwinUnet_Missformer/Models/Swin-Unet-main/models/network
= Queue/Partition    : work
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 22:00:00
= Elapsed runtime    : 05:02:28
= Total RAM usage    : 7.1 GiB of requested  GiB (%)   
= Node list          : tg082
= Subm/Elig/Start/End: 2025-11-17T21:02:29 / 2025-11-17T21:02:29 / 2025-11-17T21:36:58 / 2025-11-18T02:39:26
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc              53.1G   104.9G   209.7G        N/A     235K     500K   1,000K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 366382, 50 %, 32 %, 4594 MiB, 4104738 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 1058381, 12 %, 6 %, 920 MiB, 204059 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 1112042, 51 %, 33 %, 4594 MiB, 3711529 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 1732906, 13 %, 6 %, 920 MiB, 201071 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 1786289, 50 %, 33 %, 4594 MiB, 4086027 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 2483968, 12 %, 6 %, 920 MiB, 212138 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 2502771, 48 %, 31 %, 4594 MiB, 4218296 ms
NVIDIA GeForce RTX 3080, 00000000:DB:00.0, 2702481, 12 %, 6 %, 920 MiB, 217501 ms
